<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>安装ES前端图形化操作工具Kibana</title>
    <link href="/2020/08/04/%E5%AE%89%E8%A3%85ES%E5%89%8D%E7%AB%AF%E5%9B%BE%E5%BD%A2%E5%8C%96%E6%93%8D%E4%BD%9C%E5%B7%A5%E5%85%B7Kibana/"/>
    <url>/2020/08/04/%E5%AE%89%E8%A3%85ES%E5%89%8D%E7%AB%AF%E5%9B%BE%E5%BD%A2%E5%8C%96%E6%93%8D%E4%BD%9C%E5%B7%A5%E5%85%B7Kibana/</url>
    
    <content type="html"><![CDATA[<p>Kibana是一个针对Elasticsearch的开源分析及可视化平台，用来搜索、查看交互存储在Elasticsearch索引中的数据。使用Kibana，可以通过各种图表进行高级数据分析及展示。</p><p>Kibana让海量数据更容易理解。它操作简单，基于浏览器的用户界面可以快速创建仪表板（dashboard）实时显示Elasticsearch查询动态。</p><p><strong>1、下载安装包</strong></p><p>笔者选用的是跟ES同样的版本，7.6.1，下载地址：<a href="https://artifacts.elastic.co/downloads/kibana/kibana-7.6.1-linux-x86_64.tar.gz" target="_blank" rel="noopener">https://artifacts.elastic.co/downloads/kibana/kibana-7.6.1-linux-x86_64.tar.gz</a></p><p><strong>2、解压安装包</strong></p><pre><code class="hljs shell">[root@PaasMD01 software]# tar -zxvf kibana-7.6.1-linux-x86_64.tar.gz[root@PaasMD01 software]# mv kibana-7.6.1-linux-x86_64 kibana</code></pre><p><strong>3、修改kibana配置文件</strong></p><pre><code class="hljs shell">[root@PaasMD01 kibana]# cd config[root@PaasMD01 config]# vi kibana.yml<span class="hljs-meta">#</span><span class="bash"> 对外服务监听端口</span>server.port: 5601<span class="hljs-meta">#</span><span class="bash"> 0.0.0.0表示任何地址在没有防火墙限制的情况下都可以访问</span>server.host: "0.0.0.0"<span class="hljs-meta">#</span><span class="bash"> 默认值为主机名称，表示kibana实例绑定的主机，可以是IP地址或者主机名称.</span>server.name: "10.1.34.74"<span class="hljs-meta">#</span><span class="bash"> 用来处理ES请求的服务URL</span>elasticsearch.hosts:["http://10.1.34.74:9200","http://10.1.34.49:9200","http://10.1.34.50:9200"]<span class="hljs-meta">#</span><span class="bash"> 用来控制证书的认证，可选的值为full，none，certificate。此处由于没有证书，所以设置为null，否则启动会提示错误.</span>elasticsearch.ssl.verificationMode: none<span class="hljs-meta">#</span><span class="bash"> kibana搜索数据请求超时时间</span>elasticsearch.requestTimeout: 90000<span class="hljs-meta">#</span><span class="bash"> 汉化Kibana</span>i18n.locale: "zh-CN"</code></pre><p><strong>4、启动kibana</strong></p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 由于kibana不能由root用户启动，新建用户并授权</span>[root@PaasMD01 software]# chown -R calvin:calvin /usr/software/kibana/[root@PaasMD01 software]# su calvin[calvin@PaasMD01 software]$ cd kibana/bin[calvin@PaasMD01 bin]$ ./kibana</code></pre><p>如果想让kibana后台启动，则使用命令：</p><pre><code class="hljs shell">nohup ./kibana &amp;</code></pre><p>浏览器访问：<a href="http://10.1.34.74:5601/app/kibana" target="_blank" rel="noopener">http://10.1.34.74:5601/app/kibana</a></p><p><strong>5、测试es</strong></p><p>为了测试方便，笔者在elasticsearch-head新建了一条索引：test</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/es/test.png" srcset="/img/loading.gif" alt=""></p><p>通过kibana查询</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/es/kibana_query.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>kibana</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ElasticSearch集群搭建</title>
    <link href="/2020/08/04/ElasticSearch%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <url>/2020/08/04/ElasticSearch%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<p>Elaticsearch，简称为es， es是一个开源的高扩展的分布式全文检索引擎，它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别（大数据时代）的数据。es也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。</p><p>接下来，笔者将选用3台机器搭建ElasticSearch集群。</p><h3 id="一、集群机器规划"><a href="#一、集群机器规划" class="headerlink" title="一、集群机器规划"></a>一、集群机器规划</h3><h4 id="1、环境信息"><a href="#1、环境信息" class="headerlink" title="1、环境信息"></a>1、环境信息</h4><p>操作系统：Centos7</p><h4 id="2、机器规划"><a href="#2、机器规划" class="headerlink" title="2、机器规划"></a>2、机器规划</h4><table><thead><tr><th align="center">节点名称</th><th align="center">IP</th></tr></thead><tbody><tr><td align="center">paasmd01</td><td align="center">10.1.34.74</td></tr><tr><td align="center">paasredis01</td><td align="center">10.1.34.49</td></tr><tr><td align="center">paasredis02</td><td align="center">10.1.34.50</td></tr></tbody></table><h3 id="二、集群搭建"><a href="#二、集群搭建" class="headerlink" title="二、集群搭建"></a>二、集群搭建</h3><h4 id="1、下载ElasticSearch安装包"><a href="#1、下载ElasticSearch安装包" class="headerlink" title="1、下载ElasticSearch安装包"></a>1、下载ElasticSearch安装包</h4><p>官网：<a href="https://www.elastic.co/" target="_blank" rel="noopener">https://www.elastic.co/</a></p><p>笔者下载的是7.6.1 linux版本</p><h4 id="2、修改ES配置文件"><a href="#2、修改ES配置文件" class="headerlink" title="2、修改ES配置文件"></a>2、修改ES配置文件</h4><p>这里以10.1.34.74为例，列出elaticsearch.yml修改项</p><pre><code class="hljs yml"><span class="hljs-comment"># ======================== Elasticsearch Configuration =========================</span><span class="hljs-comment">#</span><span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> Elasticsearch comes with reasonable defaults for most settings.</span><span class="hljs-comment">#       Before you set out to tweak and tune the configuration, make sure you</span><span class="hljs-comment">#       understand what are you trying to accomplish and the consequences.</span><span class="hljs-comment">#</span><span class="hljs-comment"># The primary way of configuring a node is via this file. This template lists</span><span class="hljs-comment"># the most important settings you may want to configure for a production cluster.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Please consult the documentation for further information on configuration options:</span><span class="hljs-comment"># https://www.elastic.co/guide/en/elasticsearch/reference/index.html</span><span class="hljs-comment">#</span><span class="hljs-comment"># ---------------------------------- Cluster -----------------------------------</span><span class="hljs-comment">#</span><span class="hljs-comment"># Use a descriptive name for your cluster:</span><span class="hljs-comment">#</span><span class="hljs-attr">cluster.name:</span> <span class="hljs-string">你自定义的集群名称</span><span class="hljs-comment">#</span><span class="hljs-comment"># ------------------------------------ Node ------------------------------------</span><span class="hljs-comment">#</span><span class="hljs-comment"># Use a descriptive name for the node:</span><span class="hljs-comment">#</span><span class="hljs-attr">node.name:</span> <span class="hljs-string">master</span><span class="hljs-comment">#</span><span class="hljs-comment"># Add custom attributes to the node:</span><span class="hljs-comment">#</span><span class="hljs-comment">#node.attr.rack: r1</span><span class="hljs-comment">#</span><span class="hljs-comment"># ----------------------------------- Paths ------------------------------------</span><span class="hljs-comment">#</span><span class="hljs-comment"># Path to directory where to store the data (separate multiple locations by comma):</span><span class="hljs-comment">#</span><span class="hljs-attr">path.data:</span> <span class="hljs-string">/opt/elasticsearch/data</span><span class="hljs-comment">#</span><span class="hljs-comment"># Path to log files:</span><span class="hljs-comment">#</span><span class="hljs-attr">path.logs:</span> <span class="hljs-string">/opt/elasticsearch/logs</span><span class="hljs-comment">#</span><span class="hljs-comment"># ----------------------------------- Memory -----------------------------------</span><span class="hljs-comment">#</span><span class="hljs-comment"># Lock the memory on startup:</span><span class="hljs-comment">#</span><span class="hljs-comment">#bootstrap.memory_lock: true</span><span class="hljs-comment">#</span><span class="hljs-comment"># Make sure that the heap size is set to about half the memory available</span><span class="hljs-comment"># on the system and that the owner of the process is allowed to use this</span><span class="hljs-comment"># limit.</span><span class="hljs-comment">#</span><span class="hljs-comment"># Elasticsearch performs poorly when the system is swapping the memory.</span><span class="hljs-comment">#</span><span class="hljs-comment"># ---------------------------------- Network -----------------------------------</span><span class="hljs-comment">#</span><span class="hljs-comment"># Set the bind address to a specific IP (IPv4 or IPv6):</span><span class="hljs-comment">#</span><span class="hljs-attr">network.host:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-comment">#</span><span class="hljs-comment"># Set a custom port for HTTP:</span><span class="hljs-comment">#</span><span class="hljs-comment">#http.port: 9200</span><span class="hljs-comment">#</span><span class="hljs-comment"># For more information, consult the network module documentation.</span><span class="hljs-comment">#</span><span class="hljs-comment"># --------------------------------- Discovery ----------------------------------</span><span class="hljs-comment">#</span><span class="hljs-comment"># Pass an initial list of hosts to perform discovery when this node is started:</span><span class="hljs-comment"># The default list of hosts is ["127.0.0.1", "[::1]"]</span><span class="hljs-comment">#</span><span class="hljs-attr">discovery.seed_hosts:</span> <span class="hljs-string">["10.1.34.74",</span> <span class="hljs-string">"10.1.34.49"</span><span class="hljs-string">,</span> <span class="hljs-string">"10.1.34.50"</span><span class="hljs-string">]</span><span class="hljs-comment">#</span><span class="hljs-comment"># Bootstrap the cluster using an initial set of master-eligible nodes:</span><span class="hljs-comment">#</span><span class="hljs-attr">cluster.initial_master_nodes:</span> <span class="hljs-string">["master",</span> <span class="hljs-string">"node-1"</span><span class="hljs-string">,</span> <span class="hljs-string">"node-2"</span><span class="hljs-string">]</span><span class="hljs-comment">#</span><span class="hljs-comment"># For more information, consult the discovery and cluster formation module documentation.</span><span class="hljs-comment">#</span><span class="hljs-comment"># ---------------------------------- Gateway -----------------------------------</span><span class="hljs-comment">#</span><span class="hljs-comment"># Block initial recovery after a full cluster restart until N nodes are started:</span><span class="hljs-comment">#</span><span class="hljs-comment">#gateway.recover_after_nodes: 3</span><span class="hljs-comment">#</span><span class="hljs-comment"># For more information, consult the gateway module documentation.</span><span class="hljs-comment">#</span><span class="hljs-comment"># ---------------------------------- Various -----------------------------------</span><span class="hljs-comment">#</span><span class="hljs-comment"># Require explicit names when deleting indices:</span><span class="hljs-comment">#</span><span class="hljs-comment">#action.destructive_requires_name: true</span><span class="hljs-attr">http.cors.enabled:</span> <span class="hljs-literal">true</span><span class="hljs-attr">http.cors.allow-origin:</span> <span class="hljs-string">"*"</span></code></pre><p>其中：/opt/elasticsearch/data存储es产生的数据，/opt/elasticsearch/logs存储es产生的日志。</p><p>对应的文件夹都要设置777权限。</p><p>备注：<strong>10.1.34.49和10.1.34.50机器的node.name分别为node-1和node-2，其他配置同10.1.34.74</strong>。</p><h4 id="3、新建用户"><a href="#3、新建用户" class="headerlink" title="3、新建用户"></a>3、新建用户</h4><p>上面的es配置文件修改后，我们开始运行es。</p><pre><code class="hljs shell">[root@PaasMD01 /]# cd /usr/software/elasticsearch/bin/[root@PaasMD01 bin]# ./elasticsearch</code></pre><p>如果通过root用户启动es，会有如下的报错：</p><p><strong>java.lang.RuntimeException: can not run elasticsearch as root</strong></p><p>因为es不支持root用户启动，那么我们新建用户，并赋予用户权限执行es文件夹。</p><pre><code class="hljs shell">[root@PaasMD01 /]# groupadd calvin[root@PaasMD01 /]# useradd calvin -g calvin[root@PaasMD01 /]# chown -R calvin:calvin /usr/software/elasticsearch</code></pre><h4 id="4、修改参数配置"><a href="#4、修改参数配置" class="headerlink" title="4、修改参数配置"></a>4、修改参数配置</h4><p>以上配置都修改后，并且以新建的用户运行后，发现报如下的错误：</p><p><strong>max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]</strong></p><p>这里我们做如下的参数修改</p><p>（1）修改etc/security/limits.conf，在文件最后面加上</p><pre><code class="hljs shell">* soft nofile 65536* hard nofile 65536* soft nproc 4096* hard nproc 4096</code></pre><p>注：*后面有空格</p><p>（2）修改/etc/sysctl.conf，在文件最后面加上</p><pre><code class="hljs shell">vm.max_map_count=262144</code></pre><p>（3）配置重新生效</p><pre><code class="hljs shell">[root@PaasRedis02 elasticsearch]# sysctl -pvm.max_map_count = 262144</code></pre><p>最后，重新启动即可。如果想要es后台启动，使用命令：<strong>nohup ./elasticsearch &amp;</strong></p><p>启动后浏览器随便访问一个节点，笔者这里选择master节点访问：<a href="http://10.1.34.74:9200/" target="_blank" rel="noopener">http://10.1.34.74:9200/</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/es/cluster.png" srcset="/img/loading.gif" alt=""></p><h3 id="三、安装elasticsearch-head"><a href="#三、安装elasticsearch-head" class="headerlink" title="三、安装elasticsearch-head"></a>三、安装elasticsearch-head</h3><p>head插件是一个ES集群的web前端工具，它提供可视化的页面方便用户查看节点信息，对ES进行各种操作，如查询、删除、浏览索引等。<strong>这里笔者主要是想通过安装ElasticHD来查看ES集群是否成功</strong>。</p><h4 id="1、下载elasticsearch-head安装包"><a href="#1、下载elasticsearch-head安装包" class="headerlink" title="1、下载elasticsearch-head安装包"></a>1、下载elasticsearch-head安装包</h4><p>github地址：<a href="https://github.com/mobz/elasticsearch-head/" target="_blank" rel="noopener">https://github.com/mobz/elasticsearch-head/</a></p><h4 id="2、安装nodejs"><a href="#2、安装nodejs" class="headerlink" title="2、安装nodejs"></a>2、安装nodejs</h4><p>nodejs官网下载linux版本：<a href="http://nodejs.cn/download/" target="_blank" rel="noopener">http://nodejs.cn/download/</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/es/nodejs.png" srcset="/img/loading.gif" alt=""></p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> linux安装设置npm</span>[root@PaasRedis01 software]# tar -xvf node-v14.7.0-linux-x64.tar[root@PaasRedis01 software]# mv node-v14.7.0-linux-x64 nodejs<span class="hljs-meta">#</span><span class="bash"> linux安装设置npm</span>vi /etc/profile<span class="hljs-meta">#</span><span class="bash"> 在文件最后加上nodejs的bin路径</span>export PATH=$PATH:/usr/software/nodejs/bin<span class="hljs-meta">#</span><span class="bash"> 让修改配置生效</span>source /etc/profile</code></pre><p>查看nodejs是否安装成功</p><pre><code class="hljs shell">[root@PaasRedis01 software]# node --versionv14.7.0[root@PaasRedis01 software]# npm --version6.14.7</code></pre><h4 id="3、后台运行"><a href="#3、后台运行" class="headerlink" title="3、后台运行"></a>3、后台运行</h4><pre><code class="hljs shell">[root@PaasRedis01 elasticsearch-head]# nohup npm run start &amp;</code></pre><h4 id="4、查看集群情况"><a href="#4、查看集群情况" class="headerlink" title="4、查看集群情况"></a>4、查看集群情况</h4><p>浏览器访问：<a href="http://10.1.34.49:9100/" target="_blank" rel="noopener">http://10.1.34.49:9100/</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/es/clusterinfo.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>elasticsearch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k8s中搭建MinIO分布式存储集群</title>
    <link href="/2020/08/02/k8s%E4%B8%AD%E6%90%AD%E5%BB%BAMinIO%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E9%9B%86%E7%BE%A4/"/>
    <url>/2020/08/02/k8s%E4%B8%AD%E6%90%AD%E5%BB%BAMinIO%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<p>本篇笔者将通过编写yaml文件，执行kubectl命令，搭建MinIO集群。</p><p>备注：笔者以下操作均在k8s-master1机器上执行（选择任意一个Master节点执行均可）</p><h3 id="一、集群机器规划"><a href="#一、集群机器规划" class="headerlink" title="一、集群机器规划"></a>一、集群机器规划</h3><h4 id="1、环境信息"><a href="#1、环境信息" class="headerlink" title="1、环境信息"></a>1、环境信息</h4><p>操作系统：Centos7</p><h4 id="2、机器规划"><a href="#2、机器规划" class="headerlink" title="2、机器规划"></a>2、机器规划</h4><table><thead><tr><th align="center">节点名称</th><th align="center">IP</th></tr></thead><tbody><tr><td align="center">paasmin01</td><td align="center">10.1.34.12</td></tr><tr><td align="center">paasmin02</td><td align="center">10.1.34.13</td></tr><tr><td align="center">paasmin03</td><td align="center">10.1.34.14</td></tr><tr><td align="center">paasmin04</td><td align="center">10.1.34.15</td></tr></tbody></table><h4 id="3、k8s集群node节点分配标签"><a href="#3、k8s集群node节点分配标签" class="headerlink" title="3、k8s集群node节点分配标签"></a>3、k8s集群node节点分配标签</h4><pre><code class="hljs shell">[root@k8s-master1 ~]# kubectl label nodes paasmin01 minio-server=truenode/paasmin01 labeled[root@k8s-master1 ~]# kubectl label nodes paasmin02 minio-server=truenode/paasmin02 labeled[root@k8s-master1 ~]# kubectl label nodes paasmin03 minio-server=truenode/paasmin03 labeled[root@k8s-master1 ~]# kubectl label nodes paasmin04 minio-server=truenode/paasmin04 labeled</code></pre><p>给规划的机器设置标签，作为k8s的调度规则。</p><p>查看机器的标签：</p><pre><code class="hljs moonscript">[root@k8s-master1 ~]# kubectl get nodes <span class="hljs-comment">--show-labels</span>NAME          STATUS   ROLES    AGE     VERSION   LABELSpaasmin01     Ready    &lt;none&gt;   <span class="hljs-number">2</span>m55s   v1<span class="hljs-number">.18</span><span class="hljs-number">.6</span>   beta.kubernetes.<span class="hljs-built_in">io</span>/arch=amd64,beta.kubernetes.<span class="hljs-built_in">io</span>/<span class="hljs-built_in">os</span>=linux,kubernetes.<span class="hljs-built_in">io</span>/arch=amd64,kubernetes.<span class="hljs-built_in">io</span>/hostname=paasmin01,kubernetes.<span class="hljs-built_in">io</span>/<span class="hljs-built_in">os</span>=linux,minio-server=<span class="hljs-literal">true</span>paasmin02     Ready    &lt;none&gt;   <span class="hljs-number">3</span>m31s   v1<span class="hljs-number">.18</span><span class="hljs-number">.6</span>   beta.kubernetes.<span class="hljs-built_in">io</span>/arch=amd64,beta.kubernetes.<span class="hljs-built_in">io</span>/<span class="hljs-built_in">os</span>=linux,kubernetes.<span class="hljs-built_in">io</span>/arch=amd64,kubernetes.<span class="hljs-built_in">io</span>/hostname=paasmin02,kubernetes.<span class="hljs-built_in">io</span>/<span class="hljs-built_in">os</span>=linux,minio-server=<span class="hljs-literal">true</span>paasmin03     Ready    &lt;none&gt;   <span class="hljs-number">2</span>m39s   v1<span class="hljs-number">.18</span><span class="hljs-number">.6</span>   beta.kubernetes.<span class="hljs-built_in">io</span>/arch=amd64,beta.kubernetes.<span class="hljs-built_in">io</span>/<span class="hljs-built_in">os</span>=linux,kubernetes.<span class="hljs-built_in">io</span>/arch=amd64,kubernetes.<span class="hljs-built_in">io</span>/hostname=paasmin03,kubernetes.<span class="hljs-built_in">io</span>/<span class="hljs-built_in">os</span>=linux,minio-server=<span class="hljs-literal">true</span>paasmin04     Ready    &lt;none&gt;   <span class="hljs-number">4</span>m21s   v1<span class="hljs-number">.18</span><span class="hljs-number">.6</span>   beta.kubernetes.<span class="hljs-built_in">io</span>/arch=amd64,beta.kubernetes.<span class="hljs-built_in">io</span>/<span class="hljs-built_in">os</span>=linux,kubernetes.<span class="hljs-built_in">io</span>/arch=amd64,kubernetes.<span class="hljs-built_in">io</span>/hostname=paasmin04,kubernetes.<span class="hljs-built_in">io</span>/<span class="hljs-built_in">os</span>=linux,minio-server=<span class="hljs-literal">true</span></code></pre><p>可以看到4台机器：<strong>minio-server=true</strong></p><h3 id="二、YAML文件"><a href="#二、YAML文件" class="headerlink" title="二、YAML文件"></a>二、YAML文件</h3><h4 id="1、下载yaml文件"><a href="#1、下载yaml文件" class="headerlink" title="1、下载yaml文件"></a>1、下载yaml文件</h4><pre><code class="hljs shell">[root@k8s-master1 deploy]# wget https://download.osichina.net/tools/k8s/yaml/minio/minio-distributed-headless-service.yaml[root@k8s-master1 deploy]# wget https://download.osichina.net/tools/k8s/yaml/minio/minio-distributed-daemonset.yaml [root@k8s-master1 deploy]# wget https://download.osichina.net/tools/k8s/yaml/minio/minio-distributed-service.yaml</code></pre><h4 id="2、修改yaml文件"><a href="#2、修改yaml文件" class="headerlink" title="2、修改yaml文件"></a>2、修改yaml文件</h4><p><strong>（1）修改headless-service</strong></p><pre><code class="hljs shell">[root@k8s-master1 minio]# vi minio-distributed-headless-service.yaml apiVersion: v1kind: Servicemetadata:  name: minio  namespace: calvin # 修改为你自己的命名空间名  labels:    app: miniospec:  publishNotReadyAddresses: true  ports:    - port: 9000      name: minio  selector:    app: minio</code></pre><p><strong>（2）修改service</strong></p><pre><code class="hljs shell">[root@k8s-master1 minio]# vi minio-distributed-service.yaml apiVersion: v1kind: Servicemetadata:  name: minio  namespace: calvin  # 修改为你自己的命名空间名spec:  type: NodePort  ports:    - port: 9000      targetPort: 9000      protocol: TCP      nodePort: 30900  # 设置固定端口  selector:    app: minio</code></pre><p><strong>（3）修改daemonset</strong></p><pre><code class="hljs shell">[root@k8s-master1 minio]# vi minio-distributed-daemonset.yaml apiVersion: apps/v1kind: DaemonSetmetadata:  name: minio  namespace: calvin  # 修改为你自己的命名空间名  labels:    app: miniospec:  selector:    matchLabels:      app: minio  template:    metadata:      labels:        app: minio    spec:      # We only deploy minio to the specified nodes. select your nodes by using `kubectl label node hostname1 -l minio-server=true`      nodeSelector:        minio-server: "true"  # 指定含有特定标签的机器运行      # This is to maximize network performance, the headless service can be used to connect to a random host.      hostNetwork: true      # We're just using a hostpath. This path must be the same on all servers, and should be the largest, fastest block device you can fit.      volumes:      - name: storage        hostPath:          path: /tmp/export/      containers:      - name: minio        env:        - name: MINIO_ACCESS_KEY          value: "admin"  # 修改用户名        - name: MINIO_SECRET_KEY          value: "admin123" # 修改密码        image: minio/minio:RELEASE.2020-05-08T02-40-49Z        # Unfortunately you must manually define each server. Perhaps autodiscovery via DNS can be implemented in the future.        args:        - server        - http://10.1.34.1&#123;2...5&#125;/data/minio  # 修改server机器        ports:        - containerPort: 9000        volumeMounts:        - name: storage          mountPath: /data/minio/</code></pre><h4 id="3、执行yaml文件"><a href="#3、执行yaml文件" class="headerlink" title="3、执行yaml文件"></a>3、执行yaml文件</h4><pre><code class="hljs shell">[root@k8s-master1 deploy]# kubectl apply -f minio/ -n calvindaemonset.apps/minio createdservice/minio createdservice/minio configured</code></pre><h3 id="三、集群验证"><a href="#三、集群验证" class="headerlink" title="三、集群验证"></a>三、集群验证</h3><h4 id="1、查看k8s-dashboard"><a href="#1、查看k8s-dashboard" class="headerlink" title="1、查看k8s dashboard"></a>1、查看k8s dashboard</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/minio/pods.png" srcset="/img/loading.gif" alt=""></p><h4 id="2、访问MinIO-UI"><a href="#2、访问MinIO-UI" class="headerlink" title="2、访问MinIO UI"></a>2、访问MinIO UI</h4><p>浏览器访问：<a href="http://10.1.34.12:30900/minio/" target="_blank" rel="noopener">http://10.1.34.12:30900/minio/</a></p><p>用户名/密码：admin/admin123</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/minio/login.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/minio/ui.png" srcset="/img/loading.gif" alt=""></p><h4 id="3、Java客户端验证"><a href="#3、Java客户端验证" class="headerlink" title="3、Java客户端验证"></a>3、Java客户端验证</h4><p>代码已经上传到码云：<a href="https://gitee.com/calvin1993/storage-service" target="_blank" rel="noopener">https://gitee.com/calvin1993/storage-service</a></p><p><strong>（关于MinIO的Java客户端使用的文章，笔者后续会单独出文章补充）</strong></p><p>主要修改初始化MinIOClient，给bucket设置Policy，否则直接通过图片地址无法访问，需要权限。</p><p>通过运行代码，可以看到图片已经上传到bucket为calvin，文件夹为test的文件夹下。</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> com.calvin.controller;<span class="hljs-keyword">import</span> com.calvin.service.OssUploadService;<span class="hljs-keyword">import</span> org.springframework.beans.factory.annotation.Autowired;<span class="hljs-keyword">import</span> org.springframework.web.bind.annotation.RequestMapping;<span class="hljs-keyword">import</span> org.springframework.web.bind.annotation.RestController;<span class="hljs-comment">/**</span><span class="hljs-comment"> * <span class="hljs-doctag">@Author</span> calvin</span><span class="hljs-comment"> * <span class="hljs-doctag">@Date</span> 2020/8/1 19:48</span><span class="hljs-comment"> * <span class="hljs-doctag">@Description</span></span><span class="hljs-comment"> * <span class="hljs-doctag">@Version</span> 1.0</span><span class="hljs-comment"> */</span><span class="hljs-meta">@RestController</span><span class="hljs-meta">@RequestMapping</span>(<span class="hljs-string">"/oss"</span>)<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">OssController</span> </span>&#123;    <span class="hljs-meta">@Autowired</span>    <span class="hljs-keyword">private</span> OssUploadService ossUploadService;    <span class="hljs-comment">/**</span><span class="hljs-comment">     * 上传文件</span><span class="hljs-comment">     */</span>    <span class="hljs-meta">@RequestMapping</span>(<span class="hljs-string">"/upload"</span>)    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">upload</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-comment">//上传文件</span>        <span class="hljs-keyword">return</span> ossUploadService.putObject(<span class="hljs-string">"calvin"</span>, <span class="hljs-string">"test/ccc.jpg"</span>, <span class="hljs-string">"D:\\picture\\ccc.jpg"</span>);    &#125;&#125;</code></pre><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/minio/bucket.png" srcset="/img/loading.gif" alt=""></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/minio/ccc.png" srcset="/img/loading.gif" alt=""></p><p><strong>浏览器访问</strong>：<a href="http://10.1.34.12:30900/calvin/test/ccc.jpg" target="_blank" rel="noopener">http://10.1.34.12:30900/calvin/test/ccc.jpg</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/minio/12.png" srcset="/img/loading.gif" alt=""></p><p>由于是集群，切换另外一个集群测试是否也能访问</p><p><strong>浏览器访问</strong>：<a href="http://10.1.34.15:30900/calvin/test/ccc.jpg" target="_blank" rel="noopener">http://10.1.34.15:30900/calvin/test/ccc.jpg</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/minio/15.png" srcset="/img/loading.gif" alt=""></p><p>至此，MinIO分布式存储集群已经搭建并验证完毕。</p>]]></content>
    
    
    
    <tags>
      
      <tag>minio</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k8s搭建Nacos集群</title>
    <link href="/2020/08/01/k8s%E6%90%AD%E5%BB%BANacos%E9%9B%86%E7%BE%A4/"/>
    <url>/2020/08/01/k8s%E6%90%AD%E5%BB%BANacos%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<p>Nacos 是阿里巴巴的新开源项目，其核心定位是“一个更易于帮助构建云原生应用的动态服务发现、配置和服务管理平台”。</p><p>本篇笔者将通过k8s搭建Nacos集群。</p><h3 id="一、集群机器规划"><a href="#一、集群机器规划" class="headerlink" title="一、集群机器规划"></a>一、集群机器规划</h3><h4 id="1、环境信息"><a href="#1、环境信息" class="headerlink" title="1、环境信息"></a>1、环境信息</h4><p>操作系统：Centos7</p><p>JDK：1.8</p><h4 id="2、机器规划"><a href="#2、机器规划" class="headerlink" title="2、机器规划"></a>2、机器规划</h4><table><thead><tr><th align="center">节点名称</th><th align="center">IP</th></tr></thead><tbody><tr><td align="center">paasmd01</td><td align="center">10.1.34.74</td></tr><tr><td align="center">paasredis01</td><td align="center">10.1.34.49</td></tr><tr><td align="center">paasredis02</td><td align="center">10.1.34.50</td></tr></tbody></table><h4 id="3、k8s集群node节点分配标签"><a href="#3、k8s集群node节点分配标签" class="headerlink" title="3、k8s集群node节点分配标签"></a>3、k8s集群node节点分配标签</h4><pre><code class="hljs shell">[root@k8s-master1 deploy]# kubectl label nodes paasmd01 cloudnil.com/role=nacosnode/paasmd01 labeled[root@k8s-master1 deploy]# kubectl label nodes paasredis01 cloudnil.com/role=nacosnode/paasredis01 labeled[root@k8s-master1 deploy]# kubectl label nodes paasredis02 cloudnil.com/role=nacosnode/paasredis02 labeled</code></pre><p>给规划的机器设置标签，作为k8s的调度规则。</p><p>查看机器的标签：</p><pre><code class="hljs shell">[root@k8s-master1 deploy]# kubectl get nodes paasmd01 --show-labelsNAME       STATUS   ROLES    AGE    VERSION   LABELSpaasmd01   Ready    &lt;none&gt;   144m   v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cloudnil.com/role=nacos,kubernetes.io/arch=amd64,kubernetes.io/hostname=paasmd01,kubernetes.io/os=linux[root@k8s-master1 deploy]# kubectl get nodes paasredis01 --show-labelsNAME          STATUS   ROLES    AGE    VERSION   LABELSpaasredis01   Ready    &lt;none&gt;   144m   v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cloudnil.com/role=nacos,kubernetes.io/arch=amd64,kubernetes.io/hostname=paasredis01,kubernetes.io/os=linux[root@k8s-master1 deploy]# kubectl get nodes paasredis02 --show-labelsNAME          STATUS   ROLES    AGE    VERSION   LABELSpaasredis02   Ready    &lt;none&gt;   145m   v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cloudnil.com/role=nacos,kubernetes.io/arch=amd64,kubernetes.io/hostname=paasredis02,kubernetes.io/os=linux</code></pre><p>可以看到3台机器：<strong>cloudnil.com/role=nacos</strong></p><h3 id="二、搭建Nacos集群"><a href="#二、搭建Nacos集群" class="headerlink" title="二、搭建Nacos集群"></a>二、搭建Nacos集群</h3><h4 id="1、安装MySQL"><a href="#1、安装MySQL" class="headerlink" title="1、安装MySQL"></a>1、安装MySQL</h4><p>安装MySQL 5.7，可以参考笔者的博客《Docker安装MySQL》，博客地址：<a href="http://ecblog.cool/2020/08/01/Docker%E5%AE%89%E8%A3%85MySQL/" target="_blank" rel="noopener">http://ecblog.cool/2020/08/01/Docker%E5%AE%89%E8%A3%85MySQL/</a></p><p><strong>（1）新建nacos库</strong></p><pre><code class="hljs mysql">drop database if exists &#96;nacos&#96;;create database &#96;nacos&#96; DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;</code></pre><p><strong>（2）导入Nacos库相关SQL脚本</strong></p><p>官方Nacos库SQL脚本：<a href="https://github.com/alibaba/nacos/blob/master/distribution/conf/nacos-mysql.sql" target="_blank" rel="noopener">https://github.com/alibaba/nacos/blob/master/distribution/conf/nacos-mysql.sql</a></p><pre><code class="hljs mysql">&#x2F;* * Copyright 1999-2018 Alibaba Group Holding Ltd. * * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * *      http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an &quot;AS IS&quot; BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. *&#x2F;&#x2F;******************************************&#x2F;&#x2F;*   数据库全名 &#x3D; nacos_config   *&#x2F;&#x2F;*   表名称 &#x3D; config_info   *&#x2F;&#x2F;******************************************&#x2F;CREATE TABLE &#96;config_info&#96; (  &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;id&#39;,  &#96;data_id&#96; varchar(255) NOT NULL COMMENT &#39;data_id&#39;,  &#96;group_id&#96; varchar(255) DEFAULT NULL,  &#96;content&#96; longtext NOT NULL COMMENT &#39;content&#39;,  &#96;md5&#96; varchar(32) DEFAULT NULL COMMENT &#39;md5&#39;,  &#96;gmt_create&#96; datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;创建时间&#39;,  &#96;gmt_modified&#96; datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;修改时间&#39;,  &#96;src_user&#96; text COMMENT &#39;source user&#39;,  &#96;src_ip&#96; varchar(20) DEFAULT NULL COMMENT &#39;source ip&#39;,  &#96;app_name&#96; varchar(128) DEFAULT NULL,  &#96;tenant_id&#96; varchar(128) DEFAULT &#39;&#39; COMMENT &#39;租户字段&#39;,  &#96;c_desc&#96; varchar(256) DEFAULT NULL,  &#96;c_use&#96; varchar(64) DEFAULT NULL,  &#96;effect&#96; varchar(64) DEFAULT NULL,  &#96;type&#96; varchar(64) DEFAULT NULL,  &#96;c_schema&#96; text,  PRIMARY KEY (&#96;id&#96;),  UNIQUE KEY &#96;uk_configinfo_datagrouptenant&#96; (&#96;data_id&#96;,&#96;group_id&#96;,&#96;tenant_id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 COLLATE&#x3D;utf8_bin COMMENT&#x3D;&#39;config_info&#39;;&#x2F;******************************************&#x2F;&#x2F;*   数据库全名 &#x3D; nacos_config   *&#x2F;&#x2F;*   表名称 &#x3D; config_info_aggr   *&#x2F;&#x2F;******************************************&#x2F;CREATE TABLE &#96;config_info_aggr&#96; (  &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;id&#39;,  &#96;data_id&#96; varchar(255) NOT NULL COMMENT &#39;data_id&#39;,  &#96;group_id&#96; varchar(255) NOT NULL COMMENT &#39;group_id&#39;,  &#96;datum_id&#96; varchar(255) NOT NULL COMMENT &#39;datum_id&#39;,  &#96;content&#96; longtext NOT NULL COMMENT &#39;内容&#39;,  &#96;gmt_modified&#96; datetime NOT NULL COMMENT &#39;修改时间&#39;,  &#96;app_name&#96; varchar(128) DEFAULT NULL,  &#96;tenant_id&#96; varchar(128) DEFAULT &#39;&#39; COMMENT &#39;租户字段&#39;,  PRIMARY KEY (&#96;id&#96;),  UNIQUE KEY &#96;uk_configinfoaggr_datagrouptenantdatum&#96; (&#96;data_id&#96;,&#96;group_id&#96;,&#96;tenant_id&#96;,&#96;datum_id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 COLLATE&#x3D;utf8_bin COMMENT&#x3D;&#39;增加租户字段&#39;;&#x2F;******************************************&#x2F;&#x2F;*   数据库全名 &#x3D; nacos_config   *&#x2F;&#x2F;*   表名称 &#x3D; config_info_beta   *&#x2F;&#x2F;******************************************&#x2F;CREATE TABLE &#96;config_info_beta&#96; (  &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;id&#39;,  &#96;data_id&#96; varchar(255) NOT NULL COMMENT &#39;data_id&#39;,  &#96;group_id&#96; varchar(128) NOT NULL COMMENT &#39;group_id&#39;,  &#96;app_name&#96; varchar(128) DEFAULT NULL COMMENT &#39;app_name&#39;,  &#96;content&#96; longtext NOT NULL COMMENT &#39;content&#39;,  &#96;beta_ips&#96; varchar(1024) DEFAULT NULL COMMENT &#39;betaIps&#39;,  &#96;md5&#96; varchar(32) DEFAULT NULL COMMENT &#39;md5&#39;,  &#96;gmt_create&#96; datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;创建时间&#39;,  &#96;gmt_modified&#96; datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;修改时间&#39;,  &#96;src_user&#96; text COMMENT &#39;source user&#39;,  &#96;src_ip&#96; varchar(20) DEFAULT NULL COMMENT &#39;source ip&#39;,  &#96;tenant_id&#96; varchar(128) DEFAULT &#39;&#39; COMMENT &#39;租户字段&#39;,  PRIMARY KEY (&#96;id&#96;),  UNIQUE KEY &#96;uk_configinfobeta_datagrouptenant&#96; (&#96;data_id&#96;,&#96;group_id&#96;,&#96;tenant_id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 COLLATE&#x3D;utf8_bin COMMENT&#x3D;&#39;config_info_beta&#39;;&#x2F;******************************************&#x2F;&#x2F;*   数据库全名 &#x3D; nacos_config   *&#x2F;&#x2F;*   表名称 &#x3D; config_info_tag   *&#x2F;&#x2F;******************************************&#x2F;CREATE TABLE &#96;config_info_tag&#96; (  &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;id&#39;,  &#96;data_id&#96; varchar(255) NOT NULL COMMENT &#39;data_id&#39;,  &#96;group_id&#96; varchar(128) NOT NULL COMMENT &#39;group_id&#39;,  &#96;tenant_id&#96; varchar(128) DEFAULT &#39;&#39; COMMENT &#39;tenant_id&#39;,  &#96;tag_id&#96; varchar(128) NOT NULL COMMENT &#39;tag_id&#39;,  &#96;app_name&#96; varchar(128) DEFAULT NULL COMMENT &#39;app_name&#39;,  &#96;content&#96; longtext NOT NULL COMMENT &#39;content&#39;,  &#96;md5&#96; varchar(32) DEFAULT NULL COMMENT &#39;md5&#39;,  &#96;gmt_create&#96; datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;创建时间&#39;,  &#96;gmt_modified&#96; datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;修改时间&#39;,  &#96;src_user&#96; text COMMENT &#39;source user&#39;,  &#96;src_ip&#96; varchar(20) DEFAULT NULL COMMENT &#39;source ip&#39;,  PRIMARY KEY (&#96;id&#96;),  UNIQUE KEY &#96;uk_configinfotag_datagrouptenanttag&#96; (&#96;data_id&#96;,&#96;group_id&#96;,&#96;tenant_id&#96;,&#96;tag_id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 COLLATE&#x3D;utf8_bin COMMENT&#x3D;&#39;config_info_tag&#39;;&#x2F;******************************************&#x2F;&#x2F;*   数据库全名 &#x3D; nacos_config   *&#x2F;&#x2F;*   表名称 &#x3D; config_tags_relation   *&#x2F;&#x2F;******************************************&#x2F;CREATE TABLE &#96;config_tags_relation&#96; (  &#96;id&#96; bigint(20) NOT NULL COMMENT &#39;id&#39;,  &#96;tag_name&#96; varchar(128) NOT NULL COMMENT &#39;tag_name&#39;,  &#96;tag_type&#96; varchar(64) DEFAULT NULL COMMENT &#39;tag_type&#39;,  &#96;data_id&#96; varchar(255) NOT NULL COMMENT &#39;data_id&#39;,  &#96;group_id&#96; varchar(128) NOT NULL COMMENT &#39;group_id&#39;,  &#96;tenant_id&#96; varchar(128) DEFAULT &#39;&#39; COMMENT &#39;tenant_id&#39;,  &#96;nid&#96; bigint(20) NOT NULL AUTO_INCREMENT,  PRIMARY KEY (&#96;nid&#96;),  UNIQUE KEY &#96;uk_configtagrelation_configidtag&#96; (&#96;id&#96;,&#96;tag_name&#96;,&#96;tag_type&#96;),  KEY &#96;idx_tenant_id&#96; (&#96;tenant_id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 COLLATE&#x3D;utf8_bin COMMENT&#x3D;&#39;config_tag_relation&#39;;&#x2F;******************************************&#x2F;&#x2F;*   数据库全名 &#x3D; nacos_config   *&#x2F;&#x2F;*   表名称 &#x3D; group_capacity   *&#x2F;&#x2F;******************************************&#x2F;CREATE TABLE &#96;group_capacity&#96; (  &#96;id&#96; bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT &#39;主键ID&#39;,  &#96;group_id&#96; varchar(128) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;Group ID，空字符表示整个集群&#39;,  &#96;quota&#96; int(10) unsigned NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;配额，0表示使用默认值&#39;,  &#96;usage&#96; int(10) unsigned NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;使用量&#39;,  &#96;max_size&#96; int(10) unsigned NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;单个配置大小上限，单位为字节，0表示使用默认值&#39;,  &#96;max_aggr_count&#96; int(10) unsigned NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;聚合子配置最大个数，，0表示使用默认值&#39;,  &#96;max_aggr_size&#96; int(10) unsigned NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;单个聚合数据的子配置大小上限，单位为字节，0表示使用默认值&#39;,  &#96;max_history_count&#96; int(10) unsigned NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;最大变更历史数量&#39;,  &#96;gmt_create&#96; datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;创建时间&#39;,  &#96;gmt_modified&#96; datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;修改时间&#39;,  PRIMARY KEY (&#96;id&#96;),  UNIQUE KEY &#96;uk_group_id&#96; (&#96;group_id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 COLLATE&#x3D;utf8_bin COMMENT&#x3D;&#39;集群、各Group容量信息表&#39;;&#x2F;******************************************&#x2F;&#x2F;*   数据库全名 &#x3D; nacos_config   *&#x2F;&#x2F;*   表名称 &#x3D; his_config_info   *&#x2F;&#x2F;******************************************&#x2F;CREATE TABLE &#96;his_config_info&#96; (  &#96;id&#96; bigint(64) unsigned NOT NULL,  &#96;nid&#96; bigint(20) unsigned NOT NULL AUTO_INCREMENT,  &#96;data_id&#96; varchar(255) NOT NULL,  &#96;group_id&#96; varchar(128) NOT NULL,  &#96;app_name&#96; varchar(128) DEFAULT NULL COMMENT &#39;app_name&#39;,  &#96;content&#96; longtext NOT NULL,  &#96;md5&#96; varchar(32) DEFAULT NULL,  &#96;gmt_create&#96; datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,  &#96;gmt_modified&#96; datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,  &#96;src_user&#96; text,  &#96;src_ip&#96; varchar(20) DEFAULT NULL,  &#96;op_type&#96; char(10) DEFAULT NULL,  &#96;tenant_id&#96; varchar(128) DEFAULT &#39;&#39; COMMENT &#39;租户字段&#39;,  PRIMARY KEY (&#96;nid&#96;),  KEY &#96;idx_gmt_create&#96; (&#96;gmt_create&#96;),  KEY &#96;idx_gmt_modified&#96; (&#96;gmt_modified&#96;),  KEY &#96;idx_did&#96; (&#96;data_id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 COLLATE&#x3D;utf8_bin COMMENT&#x3D;&#39;多租户改造&#39;;&#x2F;******************************************&#x2F;&#x2F;*   数据库全名 &#x3D; nacos_config   *&#x2F;&#x2F;*   表名称 &#x3D; tenant_capacity   *&#x2F;&#x2F;******************************************&#x2F;CREATE TABLE &#96;tenant_capacity&#96; (  &#96;id&#96; bigint(20) unsigned NOT NULL AUTO_INCREMENT COMMENT &#39;主键ID&#39;,  &#96;tenant_id&#96; varchar(128) NOT NULL DEFAULT &#39;&#39; COMMENT &#39;Tenant ID&#39;,  &#96;quota&#96; int(10) unsigned NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;配额，0表示使用默认值&#39;,  &#96;usage&#96; int(10) unsigned NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;使用量&#39;,  &#96;max_size&#96; int(10) unsigned NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;单个配置大小上限，单位为字节，0表示使用默认值&#39;,  &#96;max_aggr_count&#96; int(10) unsigned NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;聚合子配置最大个数&#39;,  &#96;max_aggr_size&#96; int(10) unsigned NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;单个聚合数据的子配置大小上限，单位为字节，0表示使用默认值&#39;,  &#96;max_history_count&#96; int(10) unsigned NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;最大变更历史数量&#39;,  &#96;gmt_create&#96; datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;创建时间&#39;,  &#96;gmt_modified&#96; datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;修改时间&#39;,  PRIMARY KEY (&#96;id&#96;),  UNIQUE KEY &#96;uk_tenant_id&#96; (&#96;tenant_id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 COLLATE&#x3D;utf8_bin COMMENT&#x3D;&#39;租户容量信息表&#39;;CREATE TABLE &#96;tenant_info&#96; (  &#96;id&#96; bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#39;id&#39;,  &#96;kp&#96; varchar(128) NOT NULL COMMENT &#39;kp&#39;,  &#96;tenant_id&#96; varchar(128) default &#39;&#39; COMMENT &#39;tenant_id&#39;,  &#96;tenant_name&#96; varchar(128) default &#39;&#39; COMMENT &#39;tenant_name&#39;,  &#96;tenant_desc&#96; varchar(256) DEFAULT NULL COMMENT &#39;tenant_desc&#39;,  &#96;create_source&#96; varchar(32) DEFAULT NULL COMMENT &#39;create_source&#39;,  &#96;gmt_create&#96; bigint(20) NOT NULL COMMENT &#39;创建时间&#39;,  &#96;gmt_modified&#96; bigint(20) NOT NULL COMMENT &#39;修改时间&#39;,  PRIMARY KEY (&#96;id&#96;),  UNIQUE KEY &#96;uk_tenant_info_kptenantid&#96; (&#96;kp&#96;,&#96;tenant_id&#96;),  KEY &#96;idx_tenant_id&#96; (&#96;tenant_id&#96;)) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 COLLATE&#x3D;utf8_bin COMMENT&#x3D;&#39;tenant_info&#39;;CREATE TABLE &#96;users&#96; (&#96;username&#96; varchar(50) NOT NULL PRIMARY KEY,&#96;password&#96; varchar(500) NOT NULL,&#96;enabled&#96; boolean NOT NULL);CREATE TABLE &#96;roles&#96; (&#96;username&#96; varchar(50) NOT NULL,&#96;role&#96; varchar(50) NOT NULL,UNIQUE INDEX &#96;idx_user_role&#96; (&#96;username&#96; ASC, &#96;role&#96; ASC) USING BTREE);CREATE TABLE &#96;permissions&#96; (    &#96;role&#96; varchar(50) NOT NULL,    &#96;resource&#96; varchar(255) NOT NULL,    &#96;action&#96; varchar(8) NOT NULL,    UNIQUE INDEX &#96;uk_role_permission&#96; (&#96;role&#96;,&#96;resource&#96;,&#96;action&#96;) USING BTREE);INSERT INTO users (username, password, enabled) VALUES (&#39;nacos&#39;, &#39;$2a$10$EuWPZHzz32dJN7jexM34MOeYirDdFAZm2kuWj7VEOJhhZkDrxfvUu&#39;, TRUE);INSERT INTO roles (username, role) VALUES (&#39;nacos&#39;, &#39;ROLE_ADMIN&#39;);</code></pre><h4 id="2、下载官方k8s-nacos包"><a href="#2、下载官方k8s-nacos包" class="headerlink" title="2、下载官方k8s-nacos包"></a>2、下载官方k8s-nacos包</h4><pre><code class="hljs shell">git clone https://github.com/nacos-group/nacos-k8s.git</code></pre><h4 id="3、修改yaml文件"><a href="#3、修改yaml文件" class="headerlink" title="3、修改yaml文件"></a>3、修改yaml文件</h4><pre><code class="hljs shell">[root@k8s-master1 /]# cd nacos-k8s/deploy/nacos/[root@k8s-master1 /]# vim nacos-quick-start.yaml      ---apiVersion: v1kind: Namespacemetadata:  name: calvin   # 添加命名空间calvin---apiVersion: v1kind: Servicemetadata:  name: nacos-headless  namespace: calvin  labels:    app: nacos-headlessspec:  ports:    - port: 8848      name: server      targetPort: 8848      nodePort: 30112  # 设置固定端口  selector:    app: nacos  type: NodePort---apiVersion: v1kind: ConfigMapmetadata:  name: nacos-cm  namespace: calvindata:  mysql.db.host: "10.1.34.76"  # 添加mysql的host  mysql.db.name: "nacos"  mysql.port: "3306"  mysql.user: "root"  mysql.password: "123456"---apiVersion: apps/v1kind: StatefulSetmetadata:  name: nacos  namespace: calvinspec:  serviceName: nacos-headless  replicas: 3  template:    metadata:      labels:        app: nacos      annotations:        pod.alpha.kubernetes.io/initialized: "true"    spec:      affinity:        podAntiAffinity:          requiredDuringSchedulingIgnoredDuringExecution:            - labelSelector:                matchExpressions:                  - key: "app"                    operator: In                    values:                      - nacos-headless              topologyKey: "kubernetes.io/hostname"      containers:        - name: k8snacos          imagePullPolicy: Always          image: nacos/nacos-server:latest          resources:            requests:              memory: "2Gi"              cpu: "500m"          ports:            - containerPort: 8848              name: client          env:            - name: NACOS_REPLICAS              value: "3"            - name: MYSQL_SERVICE_DB_NAME              valueFrom:                configMapKeyRef:                  name: nacos-cm                  key: mysql.db.name            - name: MYSQL_SERVICE_HOST  # 添加mysql的host              valueFrom:                configMapKeyRef:                  name: nacos-cm                  key: mysql.db.host            - name: MYSQL_SERVICE_PORT              valueFrom:                configMapKeyRef:                  name: nacos-cm                  key: mysql.port            - name: MYSQL_SERVICE_USER              valueFrom:                configMapKeyRef:                  name: nacos-cm                  key: mysql.user            - name: MYSQL_SERVICE_PASSWORD              valueFrom:                configMapKeyRef:                  name: nacos-cm                  key: mysql.password            - name: NACOS_SERVER_PORT              value: "8848"            - name: NACOS_APPLICATION_PORT              value: "8848"            - name: PREFER_HOST_MODE              value: "hostname"            - name: NACOS_SERVERS              value: "nacos-0.nacos-headless.calvin.svc.cluster.local:8848 nacos-1.nacos-headless.calvin.svc.cluster.local:8848 nacos-2.nacos-headless.calvin.svc.cluster.local:8848"      nodeSelector:         cloudnil.com/role: nacos   # 添加k8s调度规则，nacos集群运行在标记该标签的机器上  selector:    matchLabels:      app: nacos</code></pre><h4 id="4、运行yaml文件"><a href="#4、运行yaml文件" class="headerlink" title="4、运行yaml文件"></a>4、运行yaml文件</h4><pre><code class="hljs shell">[root@k8s-master1 deploy]# kubectl apply -f nacos/ -n calvinservice/nacos-headless createdconfigmap/nacos-cm unchangedstatefulset.apps/nacos created</code></pre><h3 id="三、Nacos集群校验"><a href="#三、Nacos集群校验" class="headerlink" title="三、Nacos集群校验"></a>三、Nacos集群校验</h3><h4 id="1、查看k8s-dashboard"><a href="#1、查看k8s-dashboard" class="headerlink" title="1、查看k8s dashboard"></a>1、查看k8s dashboard</h4><p><strong>（1）Services</strong></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/nacos/nacos_services.png" srcset="/img/loading.gif" alt=""></p><p><strong>（2）Stateful Sets</strong></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/nacos/nacos_stateful.png" srcset="/img/loading.gif" alt=""></p><p><strong>（3）Pods</strong></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/nacos/nacos_pods.png" srcset="/img/loading.gif" alt=""></p><h4 id="2、访问Nacos-UI"><a href="#2、访问Nacos-UI" class="headerlink" title="2、访问Nacos UI"></a>2、访问Nacos UI</h4><p>随便选择nacos运行的机器，比如：paasmd01</p><p>浏览器访问：<a href="http://10.1.34.74:30112/" target="_blank" rel="noopener">http://10.1.34.74:30112/</a></p><p>默认用户名和密码都是admin。</p>]]></content>
    
    
    
    <tags>
      
      <tag>nacos</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker安装MySQL</title>
    <link href="/2020/08/01/Docker%E5%AE%89%E8%A3%85MySQL/"/>
    <url>/2020/08/01/Docker%E5%AE%89%E8%A3%85MySQL/</url>
    
    <content type="html"><![CDATA[<p>目标：安装版本：MySQL 5.7</p><p>机器：10.1.34.76（机器已安装Docker）</p><p><strong>1、拉取MySQL 5.7镜像</strong></p><pre><code class="hljs shell">docker pull docker.io/mysql:5.7</code></pre><p><strong>2、运行镜像</strong></p><pre><code class="hljs shell">docker run -itd --name my-mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 docker.io/mysql:5.7</code></pre><p><strong>3、进入MySQL容器</strong></p><pre><code class="hljs shell">docker exec -it my-mysql bash</code></pre><p><strong>4、设置远程连接权限</strong></p><pre><code class="hljs shell">[root@PaasDocker01 ~]# docker exec -it my-mysql bashroot@c2449f54463b:/# mysql -uroot -pEnter password: Welcome to the MySQL monitor.  Commands end with ; or \g.Your MySQL connection id is 17Server version: 5.7.31 MySQL Community Server (GPL)Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.<span class="hljs-meta">mysql&gt;</span><span class="bash"> ALTER USER <span class="hljs-string">'root'</span>@<span class="hljs-string">'localhost'</span> IDENTIFIED BY <span class="hljs-string">'123456'</span>;</span>Query OK, 0 rows affected (0.00 sec)<span class="hljs-meta">mysql&gt;</span><span class="bash"> flush privileges;</span>Query OK, 0 rows affected (0.00 sec)</code></pre><p>备注：如果需要新建用户，SQL如下：</p><pre><code class="hljs sql"><span class="hljs-comment">#添加远程登录用户</span><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">USER</span> <span class="hljs-string">'calvin'</span>@<span class="hljs-string">'%'</span> <span class="hljs-keyword">IDENTIFIED</span> <span class="hljs-keyword">WITH</span> mysql_native_password <span class="hljs-keyword">BY</span> <span class="hljs-string">'你的密码'</span>;<span class="hljs-keyword">GRANT</span> <span class="hljs-keyword">ALL</span> <span class="hljs-keyword">PRIVILEGES</span> <span class="hljs-keyword">ON</span> *.* <span class="hljs-keyword">TO</span> <span class="hljs-string">'calvin'</span>@<span class="hljs-string">'%'</span>;<span class="hljs-keyword">flush</span> <span class="hljs-keyword">privileges</span>;</code></pre><p><strong>5、MySQL客户端测试</strong></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/mysql.png" srcset="/img/loading.gif" alt="image-20200730225205337"></p>]]></content>
    
    
    
    <tags>
      
      <tag>mysql</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis Cluster集群搭建</title>
    <link href="/2020/07/30/Redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <url>/2020/07/30/Redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<p>Redis集群模式主要有2种：主从集群、分布式集群。</p><p>前者主要是为了高可用或是读写分离，后者为了更好的存储数据，负载均衡。</p><p><strong>redis集群提供了以下两个好处</strong></p><ul><li>将数据自动切分(split)到多个节点</li><li>当集群中的某一个节点故障时，redis还可以继续处理客户端的请求。</li></ul><p>一个 redis 集群包含 16384 个哈希槽（hash slot），数据库中的每个数据都属于这16384个哈希槽中的一个。集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽。集群中的每一个节点负责处理一部分哈希槽。</p><h3 id="一、集群机器规划"><a href="#一、集群机器规划" class="headerlink" title="一、集群机器规划"></a>一、集群机器规划</h3><h4 id="1、环境信息"><a href="#1、环境信息" class="headerlink" title="1、环境信息"></a>1、环境信息</h4><p>操作系统：Centos7</p><p>redis版本：5.0.8</p><h4 id="2、机器规划"><a href="#2、机器规划" class="headerlink" title="2、机器规划"></a>2、机器规划</h4><table><thead><tr><th align="center">节点名称</th><th align="center">IP</th></tr></thead><tbody><tr><td align="center">PaasDocker02</td><td align="center">10.1.34.48</td></tr><tr><td align="center">PaasMD02</td><td align="center">10.1.34.75</td></tr><tr><td align="center">PaasDocker01</td><td align="center">10.1.34.76</td></tr></tbody></table><p>每台机器分配2个redis节点：redis1，redis2</p><h3 id="二、搭建过程"><a href="#二、搭建过程" class="headerlink" title="二、搭建过程"></a>二、搭建过程</h3><h4 id="1、创建自定义redis配置文件"><a href="#1、创建自定义redis配置文件" class="headerlink" title="1、创建自定义redis配置文件"></a>1、创建自定义redis配置文件</h4><p>以10.1.34.48这台机器演示：</p><pre><code class="hljs shell">[root@PaasDocker02 /]# mkdir -p /usr/software/redis-cluster/[root@PaasDocker02 /]# cd /usr/software/redis-cluster/<span class="hljs-meta">#</span><span class="bash"> 解压传到redis-cluster文件夹下的redis-5.0.8.tar.gz</span>[root@PaasDocker02 redis-cluster]# tar -zxvf redis-5.0.8.tar.gz<span class="hljs-meta">#</span><span class="bash"> 将解压出来的redis-5.0.8文件夹重命名为redis1</span>[root@PaasDocker02 redis-cluster]# mv redis-5.0.8 redis1<span class="hljs-meta">#</span><span class="bash"> 同上再解压tar包并重命名为redis2</span>[root@PaasDocker02 redis-cluster]# mv redis-5.0.8 redis2<span class="hljs-meta">#</span><span class="bash"> 新增redis-data文件夹，存放redis数据</span>[root@PaasDocker02 redis-cluster]# mkdir redis-data<span class="hljs-meta">#</span><span class="bash"> 在redis-data文件夹下新建7001和7002文件夹</span>[root@PaasDocker02 redis-data]# mkdir 7001[root@PaasDocker02 redis-data]# mkdir 7002<span class="hljs-meta">#</span><span class="bash"> 新增redis-7001.conf配置文件</span>[root@PaasDocker02 redis-cluster]# touch redis-7001.conf[root@PaasDocker02 redis-cluster]# vi redis-7001.conf<span class="hljs-meta">#</span><span class="bash"> 在redis-7001.conf中添加以下内容</span>port 7001daemonize yespidfile /var/run/redis_7001.piddir /usr/software/redis-cluster/redis-data/7001cluster-enabled yescluster-config-file nodes_7001.confcluster-node-timeout 10100appendonly yes<span class="hljs-meta">#</span><span class="bash"> 新增redis-7001.conf配置文件</span>[root@PaasDocker02 redis-cluster]# touch redis-7002.conf[root@PaasDocker02 redis-cluster]# vi redis-7002.conf<span class="hljs-meta">#</span><span class="bash"> 在redis-7002.conf中添加以下内容</span>port 7002daemonize yespidfile /var/run/redis_7002.piddir /usr/software/redis-cluster/redis-data/7002cluster-enabled yescluster-config-file nodes_7002.confcluster-node-timeout 10100appendonly yes</code></pre><p>上面的步骤操作后，redis-cluster里面的内容如截图所示：</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/redis/redis%E6%9C%BA%E5%99%A8.png" srcset="/img/loading.gif" alt="image-20200729233011401"></p><p>同理，在10.1.34.75和10.1.34.76两台机器中分别操作10.1.34.48机器中的步骤。</p><h4 id="2、编译redis"><a href="#2、编译redis" class="headerlink" title="2、编译redis"></a>2、编译redis</h4><p>分别在redis1和redis2文件夹下执行make命令，执行结束后，可以看到redis1和redis2文件夹下可以看到生成的src文件夹。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/redis/redis_make.png" srcset="/img/loading.gif" alt="image-20200729234600485"></p><h4 id="3、创建启动集群的脚本"><a href="#3、创建启动集群的脚本" class="headerlink" title="3、创建启动集群的脚本"></a>3、创建启动集群的脚本</h4><pre><code class="hljs shell">[root@PaasDocker02 redis-cluster]# vi start-all.sh/usr/software/redis-cluster/redis1/src/redis-server /usr/software/redis-cluster/redis-7001.conf/usr/software/redis-cluster/redis2/src/redis-server /usr/software/redis-cluster/redis-7002.conf<span class="hljs-meta">#</span><span class="bash"> 添加可执行权限</span>[root@PaasDocker02 redis-cluster]# chmod +x start-all.sh</code></pre><p>为了后续停止集群方便，我们按照启动脚本的方式，也编写一个停止集群的脚本。</p><pre><code class="hljs shell">[root@PaasDocker02 redis-cluster]# vi stop-all.sh/usr/software/redis-cluster/redis1/src/redis-cli -p 7001 shutdown/usr/software/redis-cluster/redis2/src/redis-cli -p 7002 shutdown<span class="hljs-meta">#</span><span class="bash"> 添加可执行权限</span>[root@PaasDocker02 redis-cluster]# chmod +x stop-all.sh</code></pre><p>每台机器分别运行集群脚本</p><pre><code class="hljs shell">[root@PaasDocker02 redis-cluster]# ./start-all.sh49688:C 30 Jul 2020 14:12:23.752 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo49688:C 30 Jul 2020 14:12:23.752 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=49688, just started49688:C 30 Jul 2020 14:12:23.752 # Configuration loaded49690:C 30 Jul 2020 14:12:23.757 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo49690:C 30 Jul 2020 14:12:23.757 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=49690, just started49690:C 30 Jul 2020 14:12:23.757 # Configuration loaded[root@PaasDocker02 redis-cluster]# netstat -ntlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    tcp        0      0 0.0.0.0:17001           0.0.0.0:*               LISTEN      49689/redis-server  tcp        0      0 0.0.0.0:17002           0.0.0.0:*               LISTEN      49691/redis-server  tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN      864/rpcbind         tcp        0      0 192.168.122.1:53        0.0.0.0:*               LISTEN      1646/dnsmasq        tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1206/sshd           tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      1209/cupsd          tcp        0      0 0.0.0.0:7001            0.0.0.0:*               LISTEN      49689/redis-server  tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1378/master         tcp        0      0 0.0.0.0:7002            0.0.0.0:*               LISTEN      49691/redis-server  tcp        0      0 127.0.0.1:6010          0.0.0.0:*               LISTEN      45643/sshd: root@pt tcp6       0      0 :::17001                :::*                    LISTEN      49689/redis-server  tcp6       0      0 :::17002                :::*                    LISTEN      49691/redis-server  tcp6       0      0 :::111                  :::*                    LISTEN      864/rpcbind         tcp6       0      0 :::22                   :::*                    LISTEN      1206/sshd           tcp6       0      0 ::1:631                 :::*                    LISTEN      1209/cupsd          tcp6       0      0 :::7001                 :::*                    LISTEN      49689/redis-server  tcp6       0      0 ::1:25                  :::*                    LISTEN      1378/master         tcp6       0      0 :::7002                 :::*                    LISTEN      49691/redis-server  tcp6       0      0 ::1:6010                :::*                    LISTEN      45643/sshd: root@pt</code></pre><h4 id="4、配置redis-cluster集群"><a href="#4、配置redis-cluster集群" class="headerlink" title="4、配置redis cluster集群"></a>4、配置redis cluster集群</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 初始化创建集群</span>[root@PaasDocker02 redis-cluster]# cd redis1/src[root@PaasDocker02 src]# ./redis-cli --cluster create 10.1.34.48:7001 10.1.34.48:7002 10.1.34.75:7001 10.1.34.75:7002 10.1.34.76:7001 10.1.34.76:7002 --cluster-replicas 1<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Performing <span class="hljs-built_in">hash</span> slots allocation on 6 nodes...</span>Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 10.1.34.75:7002 to 10.1.34.48:7001Adding replica 10.1.34.76:7002 to 10.1.34.75:7001Adding replica 10.1.34.48:7002 to 10.1.34.76:7001M: 74b1797de0cadebbb799ead5f4bfbefee2b27c6c 10.1.34.48:7001   slots:[0-5460] (5461 slots) masterS: 77a38e94c940b3701158d413947c7f1343fc327b 10.1.34.48:7002   replicates b77c13d87ae67c54395a6292f01642284c695312M: 6c681b82c5b220e397f1fa939253e48b0ee39dde 10.1.34.75:7001   slots:[5461-10922] (5462 slots) masterS: 50ec08e3d47eb13a92d9a331a474d351ac4118e3 10.1.34.75:7002   replicates 74b1797de0cadebbb799ead5f4bfbefee2b27c6cM: b77c13d87ae67c54395a6292f01642284c695312 10.1.34.76:7001   slots:[10923-16383] (5461 slots) masterS: 7c359e52f2d73acf4fd55bf287b515ee280a5288 10.1.34.76:7002   replicates 6c681b82c5b220e397f1fa939253e48b0ee39ddeCan I set the above configuration? (type 'yes' to accept): yes<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Nodes configuration updated</span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Assign a different config epoch to each node</span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Sending CLUSTER MEET messages to join the cluster</span>Waiting for the cluster to join......<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Performing Cluster Check (using node 10.1.34.48:7001)</span>M: 74b1797de0cadebbb799ead5f4bfbefee2b27c6c 10.1.34.48:7001   slots:[0-5460] (5461 slots) master   1 additional replica(s)S: 77a38e94c940b3701158d413947c7f1343fc327b 10.1.34.48:7002   slots: (0 slots) slave   replicates b77c13d87ae67c54395a6292f01642284c695312S: 50ec08e3d47eb13a92d9a331a474d351ac4118e3 10.1.34.75:7002   slots: (0 slots) slave   replicates 74b1797de0cadebbb799ead5f4bfbefee2b27c6cM: b77c13d87ae67c54395a6292f01642284c695312 10.1.34.76:7001   slots:[10923-16383] (5461 slots) master   1 additional replica(s)M: 6c681b82c5b220e397f1fa939253e48b0ee39dde 10.1.34.75:7001   slots:[5461-10922] (5462 slots) master   1 additional replica(s)S: 7c359e52f2d73acf4fd55bf287b515ee280a5288 10.1.34.76:7002   slots: (0 slots) slave   replicates 6c681b82c5b220e397f1fa939253e48b0ee39dde[OK] All nodes agree about slots configuration.<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Check <span class="hljs-keyword">for</span> open slots...</span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Check slots coverage...</span>[OK] All 16384 slots covered.</code></pre><h3 id="三、集群验证"><a href="#三、集群验证" class="headerlink" title="三、集群验证"></a>三、集群验证</h3><h4 id="1、登录集群"><a href="#1、登录集群" class="headerlink" title="1、登录集群"></a>1、登录集群</h4><p>以10.1.34.48这台机器做测试，判断redis集群是否初始化成功。</p><pre><code class="hljs shell">[root@PaasDocker02 src]# ./redis-cli -c -h 127.0.0.1 -p 7001<span class="hljs-meta">#</span><span class="bash"> 查看集群情况</span>127.0.0.1:7001&gt; cluster infocluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:6cluster_my_epoch:1cluster_stats_messages_ping_sent:693cluster_stats_messages_pong_sent:730cluster_stats_messages_sent:1423cluster_stats_messages_ping_received:725cluster_stats_messages_pong_received:693cluster_stats_messages_meet_received:5cluster_stats_messages_received:1423<span class="hljs-meta">#</span><span class="bash"> 查看集群节点</span>127.0.0.1:7001&gt; cluster nodes77a38e94c940b3701158d413947c7f1343fc327b 10.1.34.48:7002@17002 slave b77c13d87ae67c54395a6292f01642284c695312 0 1596094774477 5 connected50ec08e3d47eb13a92d9a331a474d351ac4118e3 10.1.34.75:7002@17002 slave 74b1797de0cadebbb799ead5f4bfbefee2b27c6c 0 1596094773000 4 connected74b1797de0cadebbb799ead5f4bfbefee2b27c6c 10.1.34.48:7001@17001 myself,master - 0 1596094775000 1 connected 0-5460b77c13d87ae67c54395a6292f01642284c695312 10.1.34.76:7001@17001 master - 0 1596094775480 5 connected 10923-163836c681b82c5b220e397f1fa939253e48b0ee39dde 10.1.34.75:7001@17001 master - 0 1596094774000 3 connected 5461-109227c359e52f2d73acf4fd55bf287b515ee280a5288 10.1.34.76:7002@17002 slave 6c681b82c5b220e397f1fa939253e48b0ee39dde 0 1596094774000 6 connected</code></pre><h4 id="2、集群节点分析"><a href="#2、集群节点分析" class="headerlink" title="2、集群节点分析"></a>2、集群节点分析</h4><p>以下是做redis cluster初始化的时候，打印到控制台的日志。</p><pre><code class="hljs shell">M: 74b1797de0cadebbb799ead5f4bfbefee2b27c6c 10.1.34.48:7001   slots:[0-5460] (5461 slots) master   1 additional replica(s)S: 77a38e94c940b3701158d413947c7f1343fc327b 10.1.34.48:7002   slots: (0 slots) slave   replicates b77c13d87ae67c54395a6292f01642284c695312S: 50ec08e3d47eb13a92d9a331a474d351ac4118e3 10.1.34.75:7002   slots: (0 slots) slave   replicates 74b1797de0cadebbb799ead5f4bfbefee2b27c6cM: b77c13d87ae67c54395a6292f01642284c695312 10.1.34.76:7001   slots:[10923-16383] (5461 slots) master   1 additional replica(s)M: 6c681b82c5b220e397f1fa939253e48b0ee39dde 10.1.34.75:7001   slots:[5461-10922] (5462 slots) master   1 additional replica(s)S: 7c359e52f2d73acf4fd55bf287b515ee280a5288 10.1.34.76:7002   slots: (0 slots) slave   replicates 6c681b82c5b220e397f1fa939253e48b0ee39dde</code></pre><p>对日志打印出来的节点信息做列表分析（节点ID较长，取首字母往后8位字符）</p><table><thead><tr><th align="center">节点Node</th><th align="center">节点ID</th><th align="center">节点角色</th><th align="center">节点复制</th><th align="center">哈希槽</th></tr></thead><tbody><tr><td align="center">10.1.34.48:7001</td><td align="center">74b1797d</td><td align="center">Master</td><td align="center"></td><td align="center">0-5460</td></tr><tr><td align="center">10.1.34.48:7002</td><td align="center">77a38e94</td><td align="center">Slave</td><td align="center">b77c13d8</td><td align="center"></td></tr><tr><td align="center">10.1.34.75:7002</td><td align="center">50ec08e3</td><td align="center">Slave</td><td align="center">74b1797d</td><td align="center"></td></tr><tr><td align="center">10.1.34.76:7001</td><td align="center">b77c13d8</td><td align="center">Master</td><td align="center"></td><td align="center">10923-16383</td></tr><tr><td align="center">10.1.34.75:7001</td><td align="center">6c681b82</td><td align="center">Master</td><td align="center"></td><td align="center">5461-10922</td></tr><tr><td align="center">10.1.34.76:7002</td><td align="center">7c359e52</td><td align="center">Slave</td><td align="center">6c681b82</td><td align="center"></td></tr></tbody></table><p>可以得出，每个节点对应的节点角色，如果是Slave节点，对应的从哪个节点复制信息。</p><h4 id="3、检查集群节点存储"><a href="#3、检查集群节点存储" class="headerlink" title="3、检查集群节点存储"></a>3、检查集群节点存储</h4><p>笔者还是以Master节点10.1.34.48:7001为例，新建<strong>key=k1,value=v1</strong>。</p><pre><code class="hljs shell">[root@PaasDocker02 src]# ./redis-cli -c -h 127.0.0.1 -p 7001127.0.0.1:7001&gt; keys *(empty list or set)127.0.0.1:7001&gt; set k1 v1<span class="hljs-meta">-&gt;</span><span class="bash"> Redirected to slot [12706] located at 10.1.34.76:7001</span>OK10.1.34.76:7001&gt; get k1"v1"[root@PaasDocker02 redis-cluster]# cd redis1/src[root@PaasDocker02 src]# ./redis-cli -c -h 127.0.0.1 -p 7001127.0.0.1:7001&gt; keys *(empty list or set)127.0.0.1:7001&gt; set k1 v1<span class="hljs-meta">-&gt;</span><span class="bash"> Redirected to slot [12706] located at 10.1.34.76:7001</span>OK10.1.34.76:7001&gt; get k1"v1"</code></pre><p>可以看到<strong>k1</strong>落到12706这个哈希槽，对应于节点：10.1.34.76:7001。</p><p>那么我们到10.1.34.76这台机器，登录到redis 7001的客户端。</p><pre><code class="hljs shell">[root@PaasDocker01 redis-cluster]# cd redis1/src[root@PaasDocker01 src]# ./redis-cli -c -h 127.0.0.1 -p 7001127.0.0.1:7001&gt; keys *1) "k1"127.0.0.1:7001&gt; get k1"v1"</code></pre><p>根据集群节点分析的情况来看，10.1.34.76:7001是Master节点，那么对应的Slave节点是10.1.34.48:7002。</p><p>因此我们接着登录到10.1.34.48这台机器redis 7002的客户端，查看<strong>k1</strong>是否复制到这里。</p><pre><code class="hljs shell">[root@PaasDocker02 redis-cluster]# cd redis1/src[root@PaasDocker02 src]# ./redis-cli -c -h 127.0.0.1 -p 7002127.0.0.1:7002&gt; keys *1) "k1"127.0.0.1:7002&gt; get k1<span class="hljs-meta">-&gt;</span><span class="bash"> Redirected to slot [12706] located at 10.1.34.76:7001</span>"v1"</code></pre><p>我们也可以通过redis的PC客户端做进一步验证：</p><p><strong>（1）连接10.1.34.76:7001</strong></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/redis/redis1.png" srcset="/img/loading.gif" alt=""></p><p><strong>（2）连接10.1.34.48:7002</strong></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/redis/redis2.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>redis集群</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker私仓Harbor部署</title>
    <link href="/2020/07/29/Docker%E7%A7%81%E4%BB%93Harbor%E9%83%A8%E7%BD%B2/"/>
    <url>/2020/07/29/Docker%E7%A7%81%E4%BB%93Harbor%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<p>本篇笔者将为大家介绍的是一个企业级镜像仓库Harbor，将作为我们容器云平台的镜像仓库中心。</p><h3 id="一、Harbor介绍"><a href="#一、Harbor介绍" class="headerlink" title="一、Harbor介绍"></a>一、Harbor介绍</h3><h4 id="1、Harbor官网"><a href="#1、Harbor官网" class="headerlink" title="1、Harbor官网"></a>1、Harbor官网</h4><p>Habor是由VMWare公司开源的容器镜像仓库。事实上，Habor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制 ，AD/LDAP集成以及审计日志等，足以满足基本企业需求。</p><p><strong>github地址</strong>：<a href="https://github.com/goharbor/harbor" target="_blank" rel="noopener">https://github.com/goharbor/harbor</a></p><h4 id="2、Harbor主要功能"><a href="#2、Harbor主要功能" class="headerlink" title="2、Harbor主要功能"></a>2、Harbor主要功能</h4><ul><li><p><strong>基于角色访问控制（RBAC）</strong></p><p>在企业中，通常有不同的开发团队负责不同的项目，镜像像代码一样，每个人角色不同需求也不同，因此就需要访问权限控制，根据角色分配相应的权限。<br>例如，开发人员需要对项目构建这就用到读写权限（push/pull），测试人员只需要读权限（pull），运维一般管理镜像仓库，具备权限分配能力，项目经理具有所有权限。 </p></li><li><p><strong>镜像复制</strong></p><p>可以将仓库中的镜像同步到远程的Harbor，类似于MySQL主从同步功能。</p></li><li><p><strong>LDAP</strong></p><p>Harbor支持LDAP认证，可以很轻易接入已有的LDAP。</p></li><li><p><strong>镜像删除和空间回收</strong></p><p>Harbor支持在Web删除镜像，回收无用的镜像，释放磁盘空间。</p></li><li><p><strong>图形页面管理</strong></p><p>用户很方面搜索镜像及项目管理。</p></li><li><p><strong>审计</strong></p><p>对仓库的所有操作都有记录。</p></li><li><p><strong>REST API</strong></p><p>完整的API，方便与外部集成。</p></li></ul><h4 id="3、Harbor组件"><a href="#3、Harbor组件" class="headerlink" title="3、Harbor组件"></a>3、Harbor组件</h4><table><thead><tr><th align="center">组件</th><th align="center">功能</th></tr></thead><tbody><tr><td align="center">harbor-adminserver</td><td align="center">配置管理中心</td></tr><tr><td align="center">harbor-db</td><td align="center">Mysql数据库</td></tr><tr><td align="center">harbor-jobservice</td><td align="center">负责镜像复制</td></tr><tr><td align="center">harbor-log</td><td align="center">记录操作日志</td></tr><tr><td align="center">harbor-ui</td><td align="center">Web管理页面和API</td></tr><tr><td align="center">nginx</td><td align="center">前端代理，负责前端页面和镜像上传/下载转发</td></tr><tr><td align="center">redis</td><td align="center">会话</td></tr><tr><td align="center">registry</td><td align="center">镜像存储</td></tr></tbody></table><h3 id="二、Harbor部署"><a href="#二、Harbor部署" class="headerlink" title="二、Harbor部署"></a>二、Harbor部署</h3><h4 id="1、安装docker-compose"><a href="#1、安装docker-compose" class="headerlink" title="1、安装docker-compose"></a>1、安装docker-compose</h4><pre><code class="hljs shell">[root@PaasHarbor01 software]# sudo curl -L "https://github.com/docker/compose/releases/download/1.23.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose<span class="hljs-meta">  %</span><span class="bash"> Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span>                                 Dload  Upload   Total   Spent    Left  Speed100   638  100   638    0     0    160      0  0:00:03  0:00:03 --:--:--   160100 11.2M  100 11.2M    0     0   588k      0  0:00:19  0:00:19 --:--:-- 2112k[root@PaasHarbor01 software]# chmod +x /usr/local/bin/docker-compose[root@PaasHarbor01 software]# docker-compose --versiondocker-compose version 1.23.2, build 1110ad01</code></pre><p><strong>备注：机器上已经安装好Docker，版本：19.03.9</strong></p><h4 id="2、HTTP方式部署"><a href="#2、HTTP方式部署" class="headerlink" title="2、HTTP方式部署"></a>2、HTTP方式部署</h4><p>下载离线安装包：<a href="https://github.com/vmware/harbor/releases" target="_blank" rel="noopener">https://github.com/vmware/harbor/releases</a></p><pre><code class="hljs shell">[root@PaasHarbor01 software]# tar -zxvf harbor-offline-installer-v1.10.4.tgz[root@PaasHarbor01 software]# cd harbor[root@PaasHarbor01 harbor]# vi harbor.yml <span class="hljs-meta">#</span><span class="bash"> 1、将hostname改为机器ip或者ip对应的机器名</span>hostname: 10.1.34.7<span class="hljs-meta">#</span><span class="bash"> 2、将https下的内容注释，否则需要配置https证书</span><span class="hljs-meta">#</span><span class="bash"> https:</span><span class="hljs-meta">  #</span><span class="bash"> https port <span class="hljs-keyword">for</span> harbor, default is 443</span><span class="hljs-meta"> #</span><span class="bash"> port: 443</span><span class="hljs-meta">  #</span><span class="bash"> The path of cert and key files <span class="hljs-keyword">for</span> nginx</span><span class="hljs-meta">  #</span><span class="bash">certificate: /your/certificate/path</span><span class="hljs-meta">  #</span><span class="bash">private_key: /your/private/key/path</span><span class="hljs-meta">#</span><span class="bash"> 3、根据需要修改harbor初始密码，笔者暂时没有修改</span>harbor_admin_password: Harbor12345<span class="hljs-meta">#</span><span class="bash"> 准备配置文件</span>[root@PaasHarbor01 harbor]# ./prepare prepare base dir is set to /usr/software/harborWARNING:root:WARNING: HTTP protocol is insecure. Harbor will deprecate http protocol in the future. Please make sure to upgrade to httpsGenerated configuration file: /config/log/logrotate.confGenerated configuration file: /config/log/rsyslog_docker.confGenerated configuration file: /config/nginx/nginx.confGenerated configuration file: /config/core/envGenerated configuration file: /config/core/app.confGenerated configuration file: /config/registry/config.ymlGenerated configuration file: /config/registryctl/envGenerated configuration file: /config/db/envGenerated configuration file: /config/jobservice/envGenerated configuration file: /config/jobservice/config.ymlGenerated and saved secret to file: /secret/keys/secretkeyGenerated certificate, key file: /secret/core/private_key.pem, cert file: /secret/registry/root.crtGenerated configuration file: /compose_location/docker-compose.ymlClean up the input dir<span class="hljs-meta">#</span><span class="bash"> 安装并启动Harbor</span>[root@PaasHarbor01 harbor]# ./install.sh [Step 0]: checking if docker is installed ...Note: docker version: 19.03.9[Step 1]: checking docker-compose is installed ...Note: docker-compose version: 1.23.2[Step 2]: loading Harbor images ...7fd57902f2bf: Loading layer [==================================================&gt;]  8.465MB/8.465MB9f7a3727b327: Loading layer [==================================================&gt;]   67.5MB/67.5MBb165ecbfa6a0: Loading layer [==================================================&gt;]  3.072kB/3.072kB618609e47ff5: Loading layer [==================================================&gt;]  3.584kB/3.584kB4941a988de67: Loading layer [==================================================&gt;]  68.33MB/68.33MBLoaded image: goharbor/chartmuseum-photon:v1.10.4c249fd1745d2: Loading layer [==================================================&gt;]  12.24MB/12.24MB6f099dcc4dab: Loading layer [==================================================&gt;]  42.51MB/42.51MBeb32b6d20d4b: Loading layer [==================================================&gt;]  5.632kB/5.632kB5acd92618fef: Loading layer [==================================================&gt;]  40.45kB/40.45kB62b57401b9ca: Loading layer [==================================================&gt;]  42.51MB/42.51MBd7b6ded42cfb: Loading layer [==================================================&gt;]   2.56kB/2.56kBLoaded image: goharbor/harbor-core:v1.10.431b3ca7fa226: Loading layer [==================================================&gt;]   63.6MB/63.6MBb9972bab1402: Loading layer [==================================================&gt;]  66.73MB/66.73MB56b3ba4b4a66: Loading layer [==================================================&gt;]  5.632kB/5.632kB1654024d89fe: Loading layer [==================================================&gt;]   2.56kB/2.56kB040ec6bf5851: Loading layer [==================================================&gt;]   2.56kB/2.56kBe93cd0c30c28: Loading layer [==================================================&gt;]   2.56kB/2.56kBaed062c3be21: Loading layer [==================================================&gt;]   2.56kB/2.56kB820d1a1df842: Loading layer [==================================================&gt;]  10.75kB/10.75kBLoaded image: goharbor/harbor-db:v1.10.4ce217f401320: Loading layer [==================================================&gt;]  8.466MB/8.466MBb324500c7da3: Loading layer [==================================================&gt;]  3.584kB/3.584kB042b5242fe78: Loading layer [==================================================&gt;]  20.94MB/20.94MB87dd45007ea3: Loading layer [==================================================&gt;]  3.072kB/3.072kB651d502d735c: Loading layer [==================================================&gt;]  8.662MB/8.662MBfe72a4614aa1: Loading layer [==================================================&gt;]  30.42MB/30.42MBLoaded image: goharbor/harbor-registryctl:v1.10.45de330f38841: Loading layer [==================================================&gt;]   8.46MB/8.46MB0af0ddd91395: Loading layer [==================================================&gt;]  6.239MB/6.239MB3685afd2d128: Loading layer [==================================================&gt;]  16.04MB/16.04MBd8057fcd0a39: Loading layer [==================================================&gt;]  28.25MB/28.25MB0340225731b6: Loading layer [==================================================&gt;]  22.02kB/22.02kB06d8d803f0eb: Loading layer [==================================================&gt;]  50.52MB/50.52MBLoaded image: goharbor/notary-server-photon:v1.10.476eab6dc7bf5: Loading layer [==================================================&gt;]  332.6MB/332.6MBc96d1ad1968e: Loading layer [==================================================&gt;]  135.2kB/135.2kBLoaded image: goharbor/harbor-migrator:v1.10.47426785037a5: Loading layer [==================================================&gt;]  10.31MB/10.31MBb9a0601e3558: Loading layer [==================================================&gt;]  7.698MB/7.698MBaac781885802: Loading layer [==================================================&gt;]  223.2kB/223.2kB8af4d736a2ab: Loading layer [==================================================&gt;]  195.1kB/195.1kB5fef45ce538d: Loading layer [==================================================&gt;]  15.36kB/15.36kB5f98131a71d5: Loading layer [==================================================&gt;]  3.584kB/3.584kBLoaded image: goharbor/harbor-portal:v1.10.4528ae1964423: Loading layer [==================================================&gt;]  12.24MB/12.24MBb03ff000935f: Loading layer [==================================================&gt;]  49.37MB/49.37MBLoaded image: goharbor/harbor-jobservice:v1.10.46e2646825500: Loading layer [==================================================&gt;]  89.65MB/89.65MBfb20b8d71cf1: Loading layer [==================================================&gt;]  3.072kB/3.072kBd566c1cc124d: Loading layer [==================================================&gt;]   59.9kB/59.9kBc427dc7cb315: Loading layer [==================================================&gt;]  61.95kB/61.95kBLoaded image: goharbor/redis-photon:v1.10.4Loaded image: goharbor/prepare:v1.10.49fd7cf078b16: Loading layer [==================================================&gt;]  49.93MB/49.93MBbffa9c13b070: Loading layer [==================================================&gt;]  3.584kB/3.584kB5bc5a2da3367: Loading layer [==================================================&gt;]  3.072kB/3.072kBd207162a345a: Loading layer [==================================================&gt;]   2.56kB/2.56kB3f5fa111d1ff: Loading layer [==================================================&gt;]  3.072kB/3.072kB6fac1f97e0a4: Loading layer [==================================================&gt;]  3.584kB/3.584kB39089450a8d3: Loading layer [==================================================&gt;]  12.29kB/12.29kBc43cc9ac71a3: Loading layer [==================================================&gt;]  5.632kB/5.632kBLoaded image: goharbor/harbor-log:v1.10.493dfe2d38dda: Loading layer [==================================================&gt;]  115.3MB/115.3MBa2d6890966ca: Loading layer [==================================================&gt;]  12.15MB/12.15MB008d8a39ac95: Loading layer [==================================================&gt;]  3.072kB/3.072kBa06e99290956: Loading layer [==================================================&gt;]  49.15kB/49.15kB6d0c609a7ea0: Loading layer [==================================================&gt;]  3.584kB/3.584kBcc7d9f19817b: Loading layer [==================================================&gt;]  13.03MB/13.03MBLoaded image: goharbor/clair-photon:v1.10.40c8c48462931: Loading layer [==================================================&gt;]  8.466MB/8.466MB7c096b7a5806: Loading layer [==================================================&gt;]   9.71MB/9.71MBf18d35335b53: Loading layer [==================================================&gt;]   9.71MB/9.71MBLoaded image: goharbor/clair-adapter-photon:v1.10.4f55180240dc6: Loading layer [==================================================&gt;]  10.31MB/10.31MBLoaded image: goharbor/nginx-photon:v1.10.44a575c1c2167: Loading layer [==================================================&gt;]  8.466MB/8.466MBd0e9899aeeb5: Loading layer [==================================================&gt;]  3.584kB/3.584kBdb6d9646f0e0: Loading layer [==================================================&gt;]  3.072kB/3.072kB478d5f29f1a6: Loading layer [==================================================&gt;]  20.94MB/20.94MB1fbbee6ba37e: Loading layer [==================================================&gt;]  21.76MB/21.76MBLoaded image: goharbor/registry-photon:v1.10.410bbb8d426b9: Loading layer [==================================================&gt;]  14.61MB/14.61MB91b66eb6b6b0: Loading layer [==================================================&gt;]  28.25MB/28.25MB58956c7bbf02: Loading layer [==================================================&gt;]  22.02kB/22.02kB1c86ba20384f: Loading layer [==================================================&gt;]  49.09MB/49.09MBLoaded image: goharbor/notary-signer-photon:v1.10.4[Step 3]: preparing environment ...[Step 4]: preparing harbor configs ...prepare base dir is set to /usr/software/harborWARNING:root:WARNING: HTTP protocol is insecure. Harbor will deprecate http protocol in the future. Please make sure to upgrade to httpsClearing the configuration file: /config/log/logrotate.confClearing the configuration file: /config/log/rsyslog_docker.confClearing the configuration file: /config/nginx/nginx.confClearing the configuration file: /config/core/envClearing the configuration file: /config/core/app.confClearing the configuration file: /config/registry/config.ymlClearing the configuration file: /config/registryctl/envClearing the configuration file: /config/registryctl/config.ymlClearing the configuration file: /config/db/envClearing the configuration file: /config/jobservice/envClearing the configuration file: /config/jobservice/config.ymlGenerated configuration file: /config/log/logrotate.confGenerated configuration file: /config/log/rsyslog_docker.confGenerated configuration file: /config/nginx/nginx.confGenerated configuration file: /config/core/envGenerated configuration file: /config/core/app.confGenerated configuration file: /config/registry/config.ymlGenerated configuration file: /config/registryctl/envGenerated configuration file: /config/db/envGenerated configuration file: /config/jobservice/envGenerated configuration file: /config/jobservice/config.ymlloaded secret from file: /secret/keys/secretkeyGenerated configuration file: /compose_location/docker-compose.ymlClean up the input dir[Step 5]: starting Harbor ...Creating network "harbor_harbor" with the default driverCreating harbor-log ... doneCreating harbor-db     ... doneCreating harbor-portal ... doneCreating registryctl   ... doneCreating registry      ... doneCreating redis         ... doneCreating harbor-core   ... doneCreating harbor-jobservice ... doneCreating nginx             ... done✔ ----Harbor has been installed and started successfully.----</code></pre><p>接着，我们使用命令查看harbor相关docker启动情况</p><pre><code class="hljs shell">[root@PaasHarbor01 harbor]# docker-compose ps      Name                     Command                  State                 Ports          ---------------------------------------------------------------------------------------------harbor-core         /harbor/harbor_core              Up (healthy)                            harbor-db           /docker-entrypoint.sh            Up (healthy)   5432/tcp                 harbor-jobservice   /harbor/harbor_jobservice  ...   Up (healthy)                            harbor-log          /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514-&gt;10514/tcpharbor-portal       nginx -g daemon off;             Up (healthy)   8080/tcp                 nginx               nginx -g daemon off;             Up (healthy)   0.0.0.0:80-&gt;8080/tcp     redis               redis-server /etc/redis.conf     Up (healthy)   6379/tcp                 registry            /home/harbor/entrypoint.sh       Up (healthy)   5000/tcp                 registryctl         /home/harbor/start.sh            Up (healthy)</code></pre><h4 id="3、访问Harbor-UI"><a href="#3、访问Harbor-UI" class="headerlink" title="3、访问Harbor UI"></a>3、访问Harbor UI</h4><p>默认端口：80</p><p>浏览器访问：<a href="http://10.1.34.7" target="_blank" rel="noopener">http://10.1.34.7</a> 进入Harbor登录页</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/harbor_ui.png" srcset="/img/loading.gif" alt=""></p><p><strong>账号：admin</strong><br><strong>密码：Harbor12345</strong></p><p>登录后，查看Harbor页面</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/harbor_project.png" srcset="/img/loading.gif" alt=""></p><h4 id="4、Harbor的使用"><a href="#4、Harbor的使用" class="headerlink" title="4、Harbor的使用"></a>4、Harbor的使用</h4><p><strong>（1）新建项目</strong></p><p>从上图可以看到，library是默认自带的项目，通常用这个存储一些公共的镜像，这个项目下镜像谁都可以pull，但不能push，push需要先登录。</p><p>笔者新建项目registry，访问级别设置为私有。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/new_registry.png" srcset="/img/loading.gif" alt=""></p><p><strong>（2）为新建的项目赋予新用户push权限</strong></p><p>先创建一个用户：</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/new_person.png" srcset="/img/loading.gif" alt=""></p><p>进入registry项目，将用户加入这个成员：</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/add_person_project.png" srcset="/img/loading.gif" alt=""></p><p><strong>（3）推送镜像到仓库</strong></p><p>笔者这里为了演示方便，从DockerHub仓库中随便拉取一个镜像hello-world，然后将这个镜像推送到我们新建的项目registry。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/docker_hub.png" srcset="/img/loading.gif" alt=""></p><p>拉取镜像，并查看</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/hello-world.png" srcset="/img/loading.gif" alt=""></p><p>推送镜像的格式为：<strong>镜像中心域名  项目名称  名称  版本</strong></p><p>因此如果我们要推送到指定的镜像仓库就必须指定仓库中心地址</p><p>接下来，先给pull下来的hello-world镜像重新打tag</p><pre><code class="hljs shell">[root@PaasHarbor01 ~]# docker image tag hello-world 10.1.34.7/registry/hello-world:1.0[root@PaasHarbor01 ~]# docker imagesREPOSITORY                             TAG                 IMAGE ID            CREATED             SIZEgoharbor/chartmuseum-photon            v1.10.4             4d6611b3b6a9        2 weeks ago         178MBgoharbor/harbor-migrator               v1.10.4             c6ba18cc92c0        2 weeks ago         357MBgoharbor/redis-photon                  v1.10.4             1733199a8380        2 weeks ago         122MBgoharbor/clair-adapter-photon          v1.10.4             4d7fec33eb52        2 weeks ago         61.2MBgoharbor/clair-photon                  v1.10.4             48f8d69c3f63        2 weeks ago         171MBgoharbor/notary-server-photon          v1.10.4             3cc30fe05041        2 weeks ago         143MBgoharbor/notary-signer-photon          v1.10.4             46ecb328c811        2 weeks ago         140MBgoharbor/harbor-registryctl            v1.10.4             503dda3f193e        2 weeks ago         102MBgoharbor/registry-photon               v1.10.4             96183605aaeb        2 weeks ago         84.5MBgoharbor/nginx-photon                  v1.10.4             f8f638056eee        2 weeks ago         43.6MBgoharbor/harbor-log                    v1.10.4             b0de11e1ba03        2 weeks ago         82.1MBgoharbor/harbor-jobservice             v1.10.4             91c262f629d2        2 weeks ago         143MBgoharbor/harbor-core                   v1.10.4             cc013d5caa80        2 weeks ago         129MBgoharbor/harbor-portal                 v1.10.4             fec0c21d0a67        2 weeks ago         51.7MBgoharbor/harbor-db                     v1.10.4             2f077a558a2c        2 weeks ago         161MBgoharbor/prepare                       v1.10.4             85d07a7c81cd        2 weeks ago         168MBhello-world                            latest              bf756fb1ae65        6 months ago        13.3kB10.1.34.7/registry/hello-world         1.0                 bf756fb1ae65        6 months ago        13.3kBkubernetesui/dashboard                 v2.0.0-beta4        6802d83967b9        11 months ago       84MBkubernetesui/metrics-scraper           v1.0.1              709901356c11        12 months ago       40.1MBlizhenliang/flannel                    v0.11.0-amd64       ff281650a721        18 months ago       52.6MBlizhenliang/nginx-ingress-controller   0.20.0              a3f21ec4bd11        22 months ago       513MBlizhenliang/coredns                    1.2.2               367cdc8433a4        23 months ago       39.2MBlizhenliang/pause-amd64                3.0                 99e59f495ffa        4 years ago         747kB</code></pre><p>可以看到，已经存在重新打tag的镜像 ：<strong>10.1.34.7/registry/hello-world:1.0</strong></p><p>我们接着将镜像推送到我们自己的项目仓库。</p><pre><code class="hljs shell">[root@PaasHarbor01 harbor]# docker push 10.1.34.7/registry/hello-world:1.0Error response from daemon: Get https://10.1.34.7/v2/: dial tcp 10.1.34.7:443: connect: connection refused</code></pre><p>这个即Docker登录远程仓库https的问题，我们需要修改Docker的配置文件daemon.json，在insecure-registries中加上10.1.34.7。</p><pre><code class="hljs shell">[root@PaasHarbor01 /]# vi /etc/docker/daemon.json&#123;  "registry-mirrors": ["https://b9pmyelo.mirror.aliyuncs.com"],  "insecure-registries": ["192.168.31.70","10.1.34.7"],  "log-driver": "json-file",  "log-opts": &#123;    "max-size": "100m"  &#125;,  "storage-driver": "overlay2"&#125;</code></pre><p>然后我们需要刷新配置并且重启Docker</p><pre><code class="hljs shell">[root@PaasHarbor01 /]# systemctl daemon-reload[root@PaasHarbor01 /]# systemctl restart docker</code></pre><p>接着我们再次尝试推送镜像到Harbor</p><pre><code class="hljs shell">[root@PaasHarbor01 harbor]# docker push 10.1.34.7/registry/hello-world:1.0The push refers to repository [10.1.34.7/registry/hello-world]9c27e219663c: Preparing denied: requested access to the resource is denied</code></pre><p>可以看到，推送被拒绝，因此我们需要先登录到Harbor。</p><pre><code class="hljs shell">[root@PaasHarbor01 harbor]# docker login 10.1.34.7Username: calvinPassword: WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded</code></pre><p>我们再次推送镜像到私仓</p><pre><code class="hljs shell">[root@PaasHarbor01 harbor]# docker push 10.1.34.7/registry/hello-world:1.0The push refers to repository [10.1.34.7/registry/hello-world]9c27e219663c: Pushed 1.0: digest: sha256:90659bf80b44ce6be8234e6ff90a1ac34acbeb826903b02cfa0da11c82cbc042 size: 525</code></pre><p>发现镜像已经推送到Harbor中，我们查看Harbor的UI页。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/harbor_image.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>harbor</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k8s中搭建Nexus私仓</title>
    <link href="/2020/07/28/k8s%E4%B8%AD%E6%90%AD%E5%BB%BANexus%E7%A7%81%E4%BB%93/"/>
    <url>/2020/07/28/k8s%E4%B8%AD%E6%90%AD%E5%BB%BANexus%E7%A7%81%E4%BB%93/</url>
    
    <content type="html"><![CDATA[<p>本篇笔者将通过编写yaml文件，执行kubectl命令，搭建Maven私服Nexus3。</p><p>备注：笔者以下操作均在k8s-master1机器上执行（选择任意一个Master节点执行均可）</p><h3 id="一、编写yaml文件"><a href="#一、编写yaml文件" class="headerlink" title="一、编写yaml文件"></a>一、编写yaml文件</h3><h4 id="1、创建自定义的命名空间Namespace"><a href="#1、创建自定义的命名空间Namespace" class="headerlink" title="1、创建自定义的命名空间Namespace"></a>1、创建自定义的命名空间Namespace</h4><p>在实际的项目开发中，我们需要将容器运行在自定义的命名空间中，因此笔者在本篇也新建一个Namespace。</p><pre><code class="hljs shell">[root@k8s-master1 ~]# mkdir -p deploy/namespace[root@k8s-master1 ~]# cd deploy/namespace/[root@k8s-master1 namespace]# touch k8s-namespace.yaml[root@k8s-master1 namespace]# vi k8s-namespace.yamlapiVersion: v1kind: Namespacemetadata:  name: 你自己定义的命名空间名</code></pre><p>接着运行kubectl命令：</p><pre><code class="hljs shell">[root@k8s-master1 namespace]# kubectl apply -f k8s-namespace.yaml</code></pre><h4 id="2、编写nexus-pv-pvc-yaml"><a href="#2、编写nexus-pv-pvc-yaml" class="headerlink" title="2、编写nexus-pv-pvc.yaml"></a>2、编写nexus-pv-pvc.yaml</h4><pre><code class="hljs shell">apiVersion: v1kind: PersistentVolumemetadata:  name: nexus3-data-pv  labels:    app: nexus3-data-pvspec:  capacity:    storage: 100Gi  accessModes:    - ReadWriteOnce  persistentVolumeReclaimPolicy: Recycle  hostPath:    path: /data/maven-nexus---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: nexus3-data-pvc  labels:    app: nexus3-data-pvcspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 100Gi  selector:    matchLabels:      app: nexus3-data-pv</code></pre><h4 id="3、编写nexus-service-yaml"><a href="#3、编写nexus-service-yaml" class="headerlink" title="3、编写nexus-service.yaml"></a>3、编写nexus-service.yaml</h4><pre><code class="hljs shell">apiVersion: v1kind: Servicemetadata:  labels:    app: nexus3  name: nexus3spec:  type: NodePort  ports:  - port: 8081    targetPort: 8081    nodePort: 30020    name: web-ui  - port: 5000    targetPort: 5000    nodePort: 30050    name: docker-group  - port: 8889    targetPort: 8889    nodePort: 30080    name: docker-push  selector:    app: nexus3</code></pre><h4 id="4、编写nexus-deployment-yaml"><a href="#4、编写nexus-deployment-yaml" class="headerlink" title="4、编写nexus-deployment.yaml"></a>4、编写nexus-deployment.yaml</h4><pre><code class="hljs shell">kind: DeploymentapiVersion: apps/v1metadata:  labels:    app: nexus3  name: nexus3spec:  replicas: 1  selector:    matchLabels:      app: nexus3  template:    metadata:      labels:        app: nexus3    spec:      containers:        - name: nexus3          image: sonatype/nexus3:latest          imagePullPolicy: IfNotPresent          ports:          - containerPort: 8081            protocol: TCP          volumeMounts:          - name: nexus-data            mountPath: /nexus-data       volumes:        - name: nexus-data          persistentVolumeClaim:            claimName: nexus3-data-pvc      nodeSelector:        kubernetes.io/hostname: paasnexus01</code></pre><p>以上yaml文件中指定了Nexus镜像的pull地址，容器的默认端口以及宿主机的固定端口映射。另外，指定了容器运行所在的k8s节点。</p><p>查看nodes的hostname</p><pre><code class="hljs shell">[root@k8s-master1 ~]# kubectl get node --show-labelsNAME           STATUS   ROLES    AGE     VERSION   LABELSk8s-master1    Ready    &lt;none&gt;   32h     v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master1,kubernetes.io/os=linuxk8s-master2    Ready    &lt;none&gt;   32h     v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master2,kubernetes.io/os=linuxk8s-master3    Ready    &lt;none&gt;   32h     v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master3,kubernetes.io/os=linuxk8s-node1      Ready    &lt;none&gt;   32h     v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node1,kubernetes.io/os=linuxk8s-node2      Ready    &lt;none&gt;   32h     v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node2,kubernetes.io/os=linuxk8s-node3      Ready    &lt;none&gt;   32h     v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node3,kubernetes.io/os=linuxpaasharbor01   Ready    &lt;none&gt;   5h15m   v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=paasharbor01,kubernetes.io/os=linuxpaasnexus01    Ready    &lt;none&gt;   5h14m   v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=paasnexus01,kubernetes.io/os=linux</code></pre><p>设置指定机器运行</p><pre><code class="hljs shell">nodeSelector:  kubernetes.io/hostname: paasnexus01</code></pre><h3 id="二、执行yaml文件"><a href="#二、执行yaml文件" class="headerlink" title="二、执行yaml文件"></a>二、执行yaml文件</h3><h4 id="1、创建存放yaml文件的文件夹"><a href="#1、创建存放yaml文件的文件夹" class="headerlink" title="1、创建存放yaml文件的文件夹"></a>1、创建存放yaml文件的文件夹</h4><pre><code class="hljs shell">[root@k8s-master1 ~]# mkdir -p deploy/nexus[root@k8s-master1 ~]# cd deploy/nexus/<span class="hljs-meta">#</span><span class="bash"> 将nexus-pv-pvc.yaml，nexus-service.yaml和nexus-deployment.yaml上传到nexus文件夹中。</span></code></pre><p>另外，还要在nexus容器运行所在的宿主机上<strong>设置挂载目录的访问权限</strong>。</p><pre><code class="hljs shell">[root@PaasNexus01 ~]# mkdir -p /data/maven-nexus[root@PaasNexus01 ~]# chmod 777 /data/maven-nexus/</code></pre><h4 id="2、kubectl命令"><a href="#2、kubectl命令" class="headerlink" title="2、kubectl命令"></a>2、kubectl命令</h4><pre><code class="hljs shell">[root@k8s-master1 ~]# kubectl apply -f nexus/ -n 你自己定义的命名空间名</code></pre><h3 id="三、验证Nexus"><a href="#三、验证Nexus" class="headerlink" title="三、验证Nexus"></a>三、验证Nexus</h3><h4 id="1、切换命名空间，查看运行Pods"><a href="#1、切换命名空间，查看运行Pods" class="headerlink" title="1、切换命名空间，查看运行Pods"></a>1、切换命名空间，查看运行Pods</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/nexus/namespace.png" srcset="/img/loading.gif" alt=""></p><h4 id="2、访问Nexus-UI"><a href="#2、访问Nexus-UI" class="headerlink" title="2、访问Nexus UI"></a>2、访问Nexus UI</h4><p>从上图中可以看到Nexus3容器运行在passnexus01节点，对应的机器IP：10.1.34.20</p><p>浏览器访问：<a href="http://10.1.34.20:30020/" target="_blank" rel="noopener">http://10.1.34.20:30020/</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/nexus/nexus.png" srcset="/img/loading.gif" alt=""></p><h4 id="3、修改admin密码"><a href="#3、修改admin密码" class="headerlink" title="3、修改admin密码"></a>3、修改admin密码</h4><p>进入nexus3容器运行所在的宿主机，查看初始的admin密码</p><pre><code class="hljs shell">[root@PaasNexus01 ~]# cd /data/maven-nexus/[root@PaasNexus01 maven-nexus]# cat admin.password</code></pre><p>再次访问Nexus UI，修改admin用户的密码，登录后查看</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/k8s-nexus-ui.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>nexus</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用yaml文件创建一个Kubernetes应用</title>
    <link href="/2020/07/28/%E4%BD%BF%E7%94%A8yaml%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAKubernetes%E5%BA%94%E7%94%A8/"/>
    <url>/2020/07/28/%E4%BD%BF%E7%94%A8yaml%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAKubernetes%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>本篇笔者将通过编写yaml文件，执行kubectl命令，实现Kubernetes应用的运行。为了比较方便的验证应用创建是否成功，笔者以微服务组件Eureka Server作为演示（有UI页面，方便验证）。</p><p>备注：笔者以下操作均在k8s-master1机器上执行（选择任意一个Master节点执行均可）</p><h3 id="一、编写yaml文件"><a href="#一、编写yaml文件" class="headerlink" title="一、编写yaml文件"></a>一、编写yaml文件</h3><h4 id="1、yaml语法"><a href="#1、yaml语法" class="headerlink" title="1、yaml语法"></a>1、yaml语法</h4><p>如果大家对k8s-yaml的语法还是很熟悉，可以参考博客：<a href="https://www.cnblogs.com/fuyuteng/p/9460534.html" target="_blank" rel="noopener">https://www.cnblogs.com/fuyuteng/p/9460534.html</a></p><h4 id="2、eureka-yaml"><a href="#2、eureka-yaml" class="headerlink" title="2、eureka.yaml"></a>2、eureka.yaml</h4><p>针对Eureka应用，笔者的yaml文件如下，大家可以直接复制使用，已验证。</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">eureka</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">eureka</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span>  <span class="hljs-attr">ports:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">8761</span>    <span class="hljs-attr">nodePort:</span> <span class="hljs-number">30090</span> <span class="hljs-comment">#service对外开放端口</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">eureka</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">eureka</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">eureka</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span> <span class="hljs-comment">#运行容器的副本数，修改这里可以快速修改分布式节点数量</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">app:</span> <span class="hljs-string">eureka</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">eureka</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span> <span class="hljs-comment">#docker容器的配置</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">eureka</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">taskbeez/eureka-server:master</span> <span class="hljs-comment"># pull镜像的地址</span><span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">8761</span> <span class="hljs-comment"># 容器对外开放端口</span></code></pre><p>设置image pull的地址，可以设置从Docker私仓拉取，这里笔者默认从DockerHub仓库拉取Eureka镜像。设置好容器的默认对外接口接口和需要运行容器的副本数。如果需要设置宿主机运行的对外端口，需要注意查看k8s的默认端口范围，可以通过如下的方式查看：</p><pre><code class="hljs shell">[root@k8s-master1 /]# cd /opt/kubernetes/cfg[root@k8s-master1 cfg]# cat kube-apiserver.conf KUBE_APISERVER_OPTS="--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--etcd-servers=https://10.1.34.69:2379,https://10.1.34.71:2379,https://10.1.34.73:2379 \--bind-address=10.1.34.68 \--secure-port=6443 \--advertise-address=10.1.34.68 \--allow-privileged=true \--service-cluster-ip-range=10.0.0.0/24 \--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \--authorization-mode=RBAC,Node \--enable-bootstrap-token-auth=true \--token-auth-file=/opt/kubernetes/cfg/token.csv \--service-node-port-range=30000-32767 \--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \--kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \--tls-cert-file=/opt/kubernetes/ssl/server.pem  \--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \--client-ca-file=/opt/kubernetes/ssl/ca.pem \--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \--etcd-cafile=/opt/kubernetes/ssl/etcd/ca.pem \--etcd-certfile=/opt/kubernetes/ssl/etcd/server.pem \--etcd-keyfile=/opt/kubernetes/ssl/etcd/server-key.pem \--audit-log-maxage=30 \--audit-log-maxbackup=3 \--audit-log-maxsize=100 \--audit-log-path=/opt/kubernetes/logs/k8s-audit.log"</code></pre><p>如果需要修改k8s默认的端口范围，可以参考博客：<a href="https://blog.csdn.net/qq1445654576/article/details/104581296/" target="_blank" rel="noopener">https://blog.csdn.net/qq1445654576/article/details/104581296/</a> </p><h3 id="二、执行yaml文件"><a href="#二、执行yaml文件" class="headerlink" title="二、执行yaml文件"></a>二、执行yaml文件</h3><h4 id="1、创建存放yaml文件的文件夹"><a href="#1、创建存放yaml文件的文件夹" class="headerlink" title="1、创建存放yaml文件的文件夹"></a>1、创建存放yaml文件的文件夹</h4><pre><code class="hljs shell">[root@k8s-master1 ~]# mkdir -p deploy/eureka[root@k8s-master1 ~]# cd deploy/eureka/</code></pre><p>将上文中写好的eureka.yaml文件上传到eureka文件夹下。</p><h4 id="2、kubectl命令"><a href="#2、kubectl命令" class="headerlink" title="2、kubectl命令"></a>2、kubectl命令</h4><p>由于在windows电脑上编辑，空格与unix不一样，执行会导致校验yaml格式不通过。</p><pre><code class="hljs shell">[root@k8s-master1 deploy]# kubectl apply -f eureka/service/eureka unchangederror: error parsing eureka/eureka.yaml: error converting YAML to JSON: yaml: line 20: found character that cannot start any token</code></pre><p>检查到对应的位置，处理好空格，再次执行</p><pre><code class="hljs shell">[root@k8s-master1 deploy]# kubectl apply -f eureka/service/eureka createddeployment.apps/eureka created</code></pre><h3 id="三、验证k8s应用部署情况"><a href="#三、验证k8s应用部署情况" class="headerlink" title="三、验证k8s应用部署情况"></a>三、验证k8s应用部署情况</h3><h4 id="1、查看Services"><a href="#1、查看Services" class="headerlink" title="1、查看Services"></a>1、查看Services</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s_eureka_services.png" srcset="/img/loading.gif" alt=""></p><h4 id="2、查看Deployments"><a href="#2、查看Deployments" class="headerlink" title="2、查看Deployments"></a>2、查看Deployments</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s_eureka_deployments.png" srcset="/img/loading.gif" alt=""></p><h4 id="3、查看运行Pods"><a href="#3、查看运行Pods" class="headerlink" title="3、查看运行Pods"></a>3、查看运行Pods</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s_eureka_pods.png" srcset="/img/loading.gif" alt=""></p><h4 id="4、访问Eureka-Server地址验证应用部署"><a href="#4、访问Eureka-Server地址验证应用部署" class="headerlink" title="4、访问Eureka Server地址验证应用部署"></a>4、访问Eureka Server地址验证应用部署</h4><p>通过查看Pods，可以看到Eureka的容器运行的3个节点分别为：k8s-node2，k8s-master1和k8s-master3，固定端口在yaml中设置的是：30090。</p><p>k8s会根据集群中的机器做动态的调度，运行在节点上。如果需要指定容器运行所在的宿主机，也是在yaml中做相应的配置修改。</p><table><thead><tr><th align="center">节点名称</th><th align="center">IP</th></tr></thead><tbody><tr><td align="center">k8s-master1</td><td align="center">10.1.34.68</td></tr><tr><td align="center">k8s-master3</td><td align="center">10.1.34.70</td></tr><tr><td align="center">k8s-node2</td><td align="center">10.1.34.72</td></tr></tbody></table><p>浏览器访问：<a href="http://10.1.34.70:30090/" target="_blank" rel="noopener">http://10.1.34.70:30090/</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/70_30090_eureka.png" srcset="/img/loading.gif" alt=""></p><p>浏览器访问：<a href="http://10.1.34.72:30090/" target="_blank" rel="noopener">http://10.1.34.72:30090/</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/72_30090_eureka.png" srcset="/img/loading.gif" alt=""></p><p>浏览器访问：<a href="http://10.1.34.68:30090/" target="_blank" rel="noopener">http://10.1.34.68:30090/</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/68_30090_eureka.png" srcset="/img/loading.gif" alt=""></p><p>至此，通过k8s运行docker集群的示例已经验证，搭建成功。</p>]]></content>
    
    
    
    <tags>
      
      <tag>yaml</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于Ansible脚本部署k8s多Master集群</title>
    <link href="/2020/07/28/%E5%9F%BA%E4%BA%8EAnsible%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2k8s%E5%A4%9AMaster%E9%9B%86%E7%BE%A4/"/>
    <url>/2020/07/28/%E5%9F%BA%E4%BA%8EAnsible%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2k8s%E5%A4%9AMaster%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h3 id="一、安装要求"><a href="#一、安装要求" class="headerlink" title="一、安装要求"></a>一、安装要求</h3><h4 id="1、机器要求"><a href="#1、机器要求" class="headerlink" title="1、机器要求"></a>1、机器要求</h4><p>在开始之前，部署Kubernetes集群机器需要满足以下几个条件：</p><ul><li>一台或多台机器，操作系统 CentOS7.x-86_x64</li><li>硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多</li><li>集群中所有机器之间网络互通</li><li>可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点</li><li>禁止swap分区</li></ul><p>确保所有节点系统时间一致</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install ntpdate -y</span><span class="hljs-meta">#</span><span class="bash"> ntpdate time.windows.com</span></code></pre><h4 id="2、多Master架构图"><a href="#2、多Master架构图" class="headerlink" title="2、多Master架构图"></a>2、多Master架构图</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/multi-master.jpg" srcset="/img/loading.gif" alt=""></p><h4 id="3、多Master服务器规划"><a href="#3、多Master服务器规划" class="headerlink" title="3、多Master服务器规划"></a>3、多Master服务器规划</h4><table><thead><tr><th align="center">节点名称</th><th align="center">IP</th></tr></thead><tbody><tr><td align="center">ansible</td><td align="center">10.1.96.48</td></tr><tr><td align="center">lb-master</td><td align="center">10.1.34.74</td></tr><tr><td align="center">lb-backup</td><td align="center">10.1.34.75</td></tr><tr><td align="center">k8s-master1</td><td align="center">10.1.34.68</td></tr><tr><td align="center">k8s-master2</td><td align="center">10.1.34.69</td></tr><tr><td align="center">k8s-master3</td><td align="center">10.1.34.70</td></tr><tr><td align="center">k8s-node1</td><td align="center">10.1.34.71</td></tr><tr><td align="center">k8s-node2</td><td align="center">10.1.34.72</td></tr><tr><td align="center">k8s-node3</td><td align="center">10.1.34.73</td></tr></tbody></table><h3 id="二、下载所需文件并修改配置"><a href="#二、下载所需文件并修改配置" class="headerlink" title="二、下载所需文件并修改配置"></a>二、下载所需文件并修改配置</h3><h4 id="1、下载Ansible部署文件"><a href="#1、下载Ansible部署文件" class="headerlink" title="1、下载Ansible部署文件"></a>1、下载Ansible部署文件</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://github.com/calvinGithub/ansible-install-k8s</span><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span> ansible-install-k8s</span></code></pre><p>下载软件包并上传到<strong>上面列举的机器</strong>root目录下，并解压压缩包</p><p>链接:<a href="https://pan.baidu.com/s/11-c6ZEwwKS2YsnZqlcMIyw" target="_blank" rel="noopener">https://pan.baidu.com/s/11-c6ZEwwKS2YsnZqlcMIyw</a>  密码:gsep</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> tar zxf binary_pkg.tar.gz</span></code></pre><h4 id="2、修改hosts文件"><a href="#2、修改hosts文件" class="headerlink" title="2、修改hosts文件"></a>2、修改hosts文件</h4><p>修改hosts文件，根据规划修改对应IP和名称。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vi hosts</span>[master]10.1.34.68 node_name=k8s-master110.1.34.69 node_name=k8s-master210.1.34.70 node_name=k8s-master3[node]10.1.34.71 node_name=k8s-node110.1.34.72 node_name=k8s-node210.1.34.73 node_name=k8s-node3[etcd]10.1.34.69 etcd_name=etcd-110.1.34.71 etcd_name=etcd-210.1.34.73 etcd_name=etcd-3[lb]10.1.34.74 lb_name=lb-master10.1.34.75 lb_name=lb-backup[k8s:children]masternode[newnode]<span class="hljs-meta">#</span><span class="bash">192.168.31.91 node_name=k8s-node4</span></code></pre><h4 id="3、修改group-vars-all-yml文件"><a href="#3、修改group-vars-all-yml文件" class="headerlink" title="3、修改group_vars/all.yml文件"></a>3、修改group_vars/all.yml文件</h4><p>修改软件包目录和证书可信任IP。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim group_vars/all.yml</span><span class="hljs-meta">#</span><span class="bash"> 安装目录 </span>software_dir: '/root/binary_pkg'k8s_work_dir: '/opt/kubernetes'etcd_work_dir: '/opt/etcd'tmp_dir: '/tmp/k8s'<span class="hljs-meta">#</span><span class="bash"> 集群网络</span>service_cidr: '10.0.0.0/24'cluster_dns: '10.0.0.2'   # 与roles/addons/files/coredns.yaml中IP一致，并且是service_cidr中的IPpod_cidr: '10.244.0.0/16' # 与roles/addons/files/kube-flannel.yaml中网段一致service_nodeport_range: '30000-32767'cluster_domain: 'cluster.local'<span class="hljs-meta">#</span><span class="bash"> 高可用，如果部署单Master，该项忽略</span>vip: '10.1.34.74'nic: 'ens192' # ifconfig查看自己机器的网卡名称<span class="hljs-meta">#</span><span class="bash"> 自签证书可信任IP列表，为方便扩展，可添加多个预留IP</span>cert_hosts:<span class="hljs-meta">  #</span><span class="bash"> 包含所有LB、VIP、Master IP和service_cidr的第一个IP</span>  k8s:    - 10.0.0.1    - 10.1.96.48    - 10.1.34.74    - 10.1.34.75    - 10.1.34.68    - 10.1.34.69    - 10.1.34.70    - 10.1.34.71    - 10.1.34.72    - 10.1.34.73<span class="hljs-meta">  #</span><span class="bash"> 包含所有etcd节点IP</span>  etcd:    - 10.1.34.69    - 10.1.34.71    - 10.1.34.73</code></pre><h3 id="三、一键部署"><a href="#三、一键部署" class="headerlink" title="三、一键部署"></a>三、一键部署</h3><h4 id="1、centos7安装Ansible"><a href="#1、centos7安装Ansible" class="headerlink" title="1、centos7安装Ansible"></a>1、centos7安装Ansible</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install -y epel-release</span><span class="hljs-meta">#</span><span class="bash"> yum install ansible -y</span></code></pre><h4 id="2、多Master版启动命令"><a href="#2、多Master版启动命令" class="headerlink" title="2、多Master版启动命令"></a>2、多Master版启动命令</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ansible-playbook -i hosts multi-master-deploy.yml -uroot -k</span></code></pre><p>命令结束后，可以看到生成了访问令牌</p><pre><code class="hljs shell">TASK [addons : Kubernetes Dashboard登录信息] ******************************************************************************************************************************************************ok: [10.1.34.68] =&gt; &#123;    "ui.stdout_lines": [        "访问地址---&gt;https://NodeIP:30001",         "令牌内容---&gt;eyJhbGciOiJSUzI1NiIsImtpZCI6IjNqVXMyekhlT2Vha3QwS0kwWTktaUNFYXg1TS1pSnZMajNnRE0zQllKYm8ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tdndnOTkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiODY4YTlmYTMtYmY2Mi00ZjkxLWFmMDMtMzlhNGMzYmFhYjhiIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.fAvxmkRuevgQ3SyhCs3xGGaQdY_YnJUzXArA2-V3UdEZQT66u4BIGi_jJGuzFZ7IIFKIZtkNlFrJRukDSyxJtgXFimmz38Aq1P3Vys5ZwryKGfSZ3KvfkNxVHcamBRihIOU4ePrHQBKOVOg6F4uENt636ZlyUj6433JtMSLYqKLtcL1ctpPEtnmUujebr8uLFaWyfMpdiSLCXHcRuoFo9EMFgLKWvhMpUbhLUhOYT_kp8H5e35i2sK7rC6ty6r-F8imClQCyKKebXr_H5hYDghwjocFN6RhN6zyJzniGF8EYW2i7qa6QECIzvUaQbQ7yM0J4D-eMPCoNHGcQdgvFuA"    ]&#125;ok: [10.1.34.70] =&gt; &#123;    "ui.stdout_lines": [        "访问地址---&gt;https://NodeIP:30001",         "令牌内容---&gt;eyJhbGciOiJSUzI1NiIsImtpZCI6IjNqVXMyekhlT2Vha3QwS0kwWTktaUNFYXg1TS1pSnZMajNnRE0zQllKYm8ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tdndnOTkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiODY4YTlmYTMtYmY2Mi00ZjkxLWFmMDMtMzlhNGMzYmFhYjhiIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.fAvxmkRuevgQ3SyhCs3xGGaQdY_YnJUzXArA2-V3UdEZQT66u4BIGi_jJGuzFZ7IIFKIZtkNlFrJRukDSyxJtgXFimmz38Aq1P3Vys5ZwryKGfSZ3KvfkNxVHcamBRihIOU4ePrHQBKOVOg6F4uENt636ZlyUj6433JtMSLYqKLtcL1ctpPEtnmUujebr8uLFaWyfMpdiSLCXHcRuoFo9EMFgLKWvhMpUbhLUhOYT_kp8H5e35i2sK7rC6ty6r-F8imClQCyKKebXr_H5hYDghwjocFN6RhN6zyJzniGF8EYW2i7qa6QECIzvUaQbQ7yM0J4D-eMPCoNHGcQdgvFuA"    ]&#125;ok: [10.1.34.69] =&gt; &#123;    "ui.stdout_lines": [        "访问地址---&gt;https://NodeIP:30001",         "令牌内容---&gt;eyJhbGciOiJSUzI1NiIsImtpZCI6IjNqVXMyekhlT2Vha3QwS0kwWTktaUNFYXg1TS1pSnZMajNnRE0zQllKYm8ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tdndnOTkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiODY4YTlmYTMtYmY2Mi00ZjkxLWFmMDMtMzlhNGMzYmFhYjhiIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.fAvxmkRuevgQ3SyhCs3xGGaQdY_YnJUzXArA2-V3UdEZQT66u4BIGi_jJGuzFZ7IIFKIZtkNlFrJRukDSyxJtgXFimmz38Aq1P3Vys5ZwryKGfSZ3KvfkNxVHcamBRihIOU4ePrHQBKOVOg6F4uENt636ZlyUj6433JtMSLYqKLtcL1ctpPEtnmUujebr8uLFaWyfMpdiSLCXHcRuoFo9EMFgLKWvhMpUbhLUhOYT_kp8H5e35i2sK7rC6ty6r-F8imClQCyKKebXr_H5hYDghwjocFN6RhN6zyJzniGF8EYW2i7qa6QECIzvUaQbQ7yM0J4D-eMPCoNHGcQdgvFuA"    ]&#125;</code></pre><h3 id="四、测试部署情况"><a href="#四、测试部署情况" class="headerlink" title="四、测试部署情况"></a>四、测试部署情况</h3><h4 id="1、访问Dashboard"><a href="#1、访问Dashboard" class="headerlink" title="1、访问Dashboard"></a>1、访问Dashboard</h4><p>随便访问任何一个节点，<a href="https://Node:30001" target="_blank" rel="noopener">https://Node:30001</a> ，并输入上面输出的token。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s_login.png" srcset="/img/loading.gif" alt=""></p><p>登录后，点击Nodes，可以看到加入到k8s集群的所有节点，默认命名空间为default。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/%E5%A4%9AMaster%E9%9B%86%E7%BE%A4Node.png" srcset="/img/loading.gif" alt="image-20200718231233862"></p><p>进入页面后，点击左侧导航栏Nodes，可以看到目前加入k8s的服务器节点。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718231355987.png" srcset="/img/loading.gif" alt="image-20200718231355987"></p><h4 id="2、使用kubectl命令查看容器"><a href="#2、使用kubectl命令查看容器" class="headerlink" title="2、使用kubectl命令查看容器"></a>2、使用kubectl命令查看容器</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 在任意一个master节点使用命令查看nodes</span>[root@k8s-master1 ~]# kubectl get nodesNAME           STATUS   ROLES    AGE    VERSIONk8s-master1    Ready    &lt;none&gt;   29h    v1.18.6k8s-master2    Ready    &lt;none&gt;   29h    v1.18.6k8s-master3    Ready    &lt;none&gt;   29h    v1.18.6k8s-node1      Ready    &lt;none&gt;   29h    v1.18.6k8s-node2      Ready    &lt;none&gt;   29h    v1.18.6k8s-node3      Ready    &lt;none&gt;   29h    v1.18.6</code></pre><p>至此，多Master的k8s集群已经搭建成功。</p><h3 id="五、增加集群节点"><a href="#五、增加集群节点" class="headerlink" title="五、增加集群节点"></a>五、增加集群节点</h3><h4 id="1、修改hosts文件添加新节点ip"><a href="#1、修改hosts文件添加新节点ip" class="headerlink" title="1、修改hosts文件添加新节点ip"></a>1、修改hosts文件添加新节点ip</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vi hosts</span>[newnode]10.1.34.20 node_name=PaasNexus0110.1.34.7  node_name=PaasHarbor01</code></pre><h4 id="2、执行部署命令"><a href="#2、执行部署命令" class="headerlink" title="2、执行部署命令"></a>2、执行部署命令</h4><pre><code class="hljs shell">ansible-playbook -i hosts add-node.yml -uroot -k</code></pre><h4 id="3、在任一Master节点允许颁发证书并加入集群"><a href="#3、在任一Master节点允许颁发证书并加入集群" class="headerlink" title="3、在任一Master节点允许颁发证书并加入集群"></a>3、在任一Master节点允许颁发证书并加入集群</h4><pre><code class="hljs shell">[root@k8s-master1 ~]# kubectl get csrNAME                                                   AGE    SIGNERNAME               REQUESTOR           CONDITIONnode-csr-lGmQlYI-ByUQGy0F7Deox0vfkKJuWZX0gh3rPodClOA   117m   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending[root@k8s-master1 ~]# kubectl certificate approve node-csr-lGmQlYI-ByUQGy0F7Deox0vfkKJuWZX0gh3rPodClOAcertificatesigningrequest.certificates.k8s.io/node-csr-lGmQlYI-ByUQGy0F7Deox0vfkKJuWZX0gh3rPodClOA approved</code></pre><p>接着用命令查看节点</p><pre><code class="hljs shell">[root@k8s-master1 ~]# kubectl get nodesNAME           STATUS   ROLES    AGE    VERSIONk8s-master1    Ready    &lt;none&gt;   29h    v1.18.6k8s-master2    Ready    &lt;none&gt;   29h    v1.18.6k8s-master3    Ready    &lt;none&gt;   29h    v1.18.6k8s-node1      Ready    &lt;none&gt;   29h    v1.18.6k8s-node2      Ready    &lt;none&gt;   29h    v1.18.6k8s-node3      Ready    &lt;none&gt;   29h    v1.18.6paasharbor01   Ready    &lt;none&gt;   152m   v1.18.6paasnexus01    Ready    &lt;none&gt;   152m   v1.18.6</code></pre><p>k8s Dashboard页面查看Nodes</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/%E6%96%B0%E5%A2%9Ek8s%E8%8A%82%E7%82%B9.png" srcset="/img/loading.gif" alt="image-20200728160021747"></p><p>最后还需要注意一点，为了新加入的节点能够彼此通信，还需要在master节点和新增加的机器下增加hosts映射。</p><p>其中10.1.34.20 PaasNexus01和10.1.34.7  PaasHarbor01是新增的机器，需要补充的。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vi /etc/hosts</span>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain610.1.34.68 k8s-master110.1.34.69 k8s-master210.1.34.70 k8s-master310.1.34.71 k8s-node110.1.34.72 k8s-node210.1.34.73 k8s-node310.1.34.20 PaasNexus0110.1.34.7  PaasHarbor01</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>k8s多Master</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于Ansible脚本部署k8s单Master集群</title>
    <link href="/2020/07/28/%E5%9F%BA%E4%BA%8EAnsible%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2k8s%E5%8D%95Master%E9%9B%86%E7%BE%A4/"/>
    <url>/2020/07/28/%E5%9F%BA%E4%BA%8EAnsible%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2k8s%E5%8D%95Master%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h3 id="一、安装要求"><a href="#一、安装要求" class="headerlink" title="一、安装要求"></a>一、安装要求</h3><h4 id="1、机器要求"><a href="#1、机器要求" class="headerlink" title="1、机器要求"></a>1、机器要求</h4><p>在开始之前，部署Kubernetes集群机器需要满足以下几个条件：</p><ul><li>一台或多台机器，操作系统 CentOS7.x-86_x64</li><li>硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多</li><li>集群中所有机器之间网络互通</li><li>可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点</li><li>禁止swap分区</li></ul><p>确保所有节点系统时间一致</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install ntpdate -y</span><span class="hljs-meta">#</span><span class="bash"> ntpdate time.windows.com</span></code></pre><h4 id="2、单Master架构图"><a href="#2、单Master架构图" class="headerlink" title="2、单Master架构图"></a>2、单Master架构图</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/single-master.jpg" srcset="/img/loading.gif" alt="single-master"></p><h4 id="3、单Master服务器规划"><a href="#3、单Master服务器规划" class="headerlink" title="3、单Master服务器规划"></a>3、单Master服务器规划</h4><table><thead><tr><th align="center">节点名称</th><th align="center">IP</th></tr></thead><tbody><tr><td align="center">etcd-1</td><td align="center">10.10.10.8</td></tr><tr><td align="center">k8s-node01</td><td align="center">10.10.10.9</td></tr><tr><td align="center">k8s-node02</td><td align="center">10.10.10.10</td></tr></tbody></table><h3 id="二、下载所需文件并修改配置"><a href="#二、下载所需文件并修改配置" class="headerlink" title="二、下载所需文件并修改配置"></a>二、下载所需文件并修改配置</h3><h4 id="1、下载Ansible部署文件"><a href="#1、下载Ansible部署文件" class="headerlink" title="1、下载Ansible部署文件"></a>1、下载Ansible部署文件</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://github.com/calvinGithub/ansible-install-k8s</span><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span> ansible-install-k8s</span></code></pre><p>下载软件包并上传到<strong>上面列举的机器</strong>root目录下，并解压压缩包</p><p>链接:<a href="https://pan.baidu.com/s/11-c6ZEwwKS2YsnZqlcMIyw" target="_blank" rel="noopener">https://pan.baidu.com/s/11-c6ZEwwKS2YsnZqlcMIyw</a>  密码:gsep</p><pre><code class="hljs css"># <span class="hljs-selector-tag">tar</span> <span class="hljs-selector-tag">zxf</span> <span class="hljs-selector-tag">binary_pkg</span><span class="hljs-selector-class">.tar</span><span class="hljs-selector-class">.gz</span></code></pre><h4 id="2、修改hosts文件"><a href="#2、修改hosts文件" class="headerlink" title="2、修改hosts文件"></a>2、修改hosts文件</h4><p>修改hosts文件，根据规划修改对应IP和名称。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vi hosts</span>[master]<span class="hljs-meta">#</span><span class="bash"> 如果部署单Master，只保留一个Master节点</span><span class="hljs-meta">#</span><span class="bash"> 默认Naster节点也部署Node组件</span>10.10.10.8 node_name=k8s-master01[node]10.10.10.9 node_name=k8s-node0110.10.10.10 node_name=k8s-node02[etcd]10.10.10.8 etcd_name=etcd-110.10.10.9 etcd_name=etcd-210.10.10.10 etcd_name=etcd-3[lb]<span class="hljs-meta">#</span><span class="bash"> 如果部署单Master，该项忽略</span>192.168.31.63 lb_name=lb-master192.168.31.71 lb_name=lb-backup[k8s:children]masternode[newnode]<span class="hljs-meta">#</span><span class="bash">192.168.31.91 node_name=k8s-node3</span></code></pre><h4 id="3、修改group-vars-all-yml文件"><a href="#3、修改group-vars-all-yml文件" class="headerlink" title="3、修改group_vars/all.yml文件"></a>3、修改group_vars/all.yml文件</h4><p>修改软件包目录和证书可信任IP。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim group_vars/all.yml</span><span class="hljs-meta">#</span><span class="bash"> 安装目录 </span>software_dir: '/root/binary_pkg'k8s_work_dir: '/opt/kubernetes'etcd_work_dir: '/opt/etcd'tmp_dir: '/tmp/k8s'<span class="hljs-meta">#</span><span class="bash"> 集群网络</span>service_cidr: '10.0.0.0/24'cluster_dns: '10.0.0.2'   # 与roles/addons/files/coredns.yaml中IP一致，并且是service_cidr中的IPpod_cidr: '10.244.0.0/16' # 与roles/addons/files/kube-flannel.yaml中网段一致service_nodeport_range: '30000-32767'cluster_domain: 'cluster.local'<span class="hljs-meta">#</span><span class="bash"> 高可用，如果部署单Master，该项忽略</span>vip: '192.168.31.88'nic: 'ens33' # ifconfig查看自己机器的网卡名称<span class="hljs-meta">#</span><span class="bash"> 自签证书可信任IP列表，为方便扩展，可添加多个预留IP</span>cert_hosts:<span class="hljs-meta">  #</span><span class="bash"> 包含所有LB、VIP、Master IP和service_cidr的第一个IP</span>  k8s:    - 10.0.0.1    - 10.10.10.8    - 10.10.10.9    - 10.10.10.10<span class="hljs-meta">  #</span><span class="bash"> 包含所有etcd节点IP</span>  etcd:    - 10.10.10.8    - 10.10.10.9    - 10.10.10.10</code></pre><h3 id="三、一键部署"><a href="#三、一键部署" class="headerlink" title="三、一键部署"></a>三、一键部署</h3><h4 id="1、centos7安装Ansible"><a href="#1、centos7安装Ansible" class="headerlink" title="1、centos7安装Ansible"></a>1、centos7安装Ansible</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install -y epel-release</span><span class="hljs-meta">#</span><span class="bash"> yum install ansible -y</span></code></pre><h4 id="2、单Master版启动命令"><a href="#2、单Master版启动命令" class="headerlink" title="2、单Master版启动命令"></a>2、单Master版启动命令</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ansible-playbook -i hosts single-master-deploy.yml -uroot -k</span></code></pre><p>命令结束后，可以看到生成了访问令牌</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718230920066.png" srcset="/img/loading.gif" alt="image-20200718230920066"></p><pre><code class="hljs shell">eyJhbGciOiJSUzI1NiIsImtpZCI6IjFhMTN5LXdUVFN6TVVoNmZ1dUltVngyTmxvMXNKejFTYkZkbldaNWhWdXcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tcXRyNmwiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWY5YWIyNmMtN2MwZC00MTZkLWJhYmMtZTMwMWY5MDBmMjU0Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.4gRDjg8It_8h0LInLHXrchsUCcdH0UerDDkCdd2y_UnKwc6wAhReDQYc2NaYVda2704Hu7-n7_fXFATQ_bf6noBjoQNsCEJ1NwBl0DO4h3l_i80YIVT9R2yl1nEq6CY5Da7rR01pp6-Htih1mpEb-O_xsP_L6FrsVjKBubY63gK4O9LQLQSnoEE-24ggg2VgyS5rVHTTDlEpq3J0XeqXiBbaZJa8o65Yg08sKzeHAffgY2538p-9ZV5-tJAsL4wbud8TjSXCzWkxsVsmH7Au0ZwRrth1tVrVEqoPGOusDVu6sFxSO53lpwsvlcg1TpKIxNMEbDTQ6GSJbPA_dyeW9Q</code></pre><h3 id="四、测试部署情况"><a href="#四、测试部署情况" class="headerlink" title="四、测试部署情况"></a>四、测试部署情况</h3><h4 id="1、访问Dashboard"><a href="#1、访问Dashboard" class="headerlink" title="1、访问Dashboard"></a>1、访问Dashboard</h4><p>随便访问任何一个节点，<a href="https://Node:30001" target="_blank" rel="noopener">https://Node:30001</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718231233862.png" srcset="/img/loading.gif" alt="image-20200718231233862"></p><p>进入页面后，点击左侧导航栏Nodes，可以看到目前加入k8s的服务器节点。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718231355987.png" srcset="/img/loading.gif" alt="image-20200718231355987"></p><h4 id="2、使用kubectl命令查看容器"><a href="#2、使用kubectl命令查看容器" class="headerlink" title="2、使用kubectl命令查看容器"></a>2、使用kubectl命令查看容器</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 1.查看nodes</span>[root@k8s-master01 ansible-install-k8s]# kubectl get nodeNAME           STATUS   ROLES    AGE   VERSIONk8s-master01   Ready    &lt;none&gt;   15m   v1.18.6k8s-node01     Ready    &lt;none&gt;   15m   v1.18.6k8s-node02     Ready    &lt;none&gt;   15m   v1.18.6<span class="hljs-meta">#</span><span class="bash"> 2.创建应用nginx</span>[root@k8s-master01 ansible-install-k8s]# kubectl create deployment web --image=nginxdeployment.apps/web created<span class="hljs-meta">#</span><span class="bash"> 3.启动应用nginx</span>[root@k8s-master01 ansible-install-k8s]# kubectl expose deployment web --port=80 --target-port=80 --name=web --type=NodePortservice/web exposed<span class="hljs-meta">#</span><span class="bash"> 4.查看所有启动的应用(30794即为容器80端口对应的宿主机映射端口)</span>[root@k8s-master01 ansible-install-k8s]# kubectl get svcNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGEkubernetes   ClusterIP   10.0.0.1     &lt;none&gt;        443/TCP        32mweb          NodePort    10.0.0.95    &lt;none&gt;        80:30794/TCP   38s</code></pre><h4 id="3、Dashboard查看运行容器"><a href="#3、Dashboard查看运行容器" class="headerlink" title="3、Dashboard查看运行容器"></a>3、Dashboard查看运行容器</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718232547965.png" srcset="/img/loading.gif" alt="image-20200718232547965"></p><p>可以看到测试用的web容器已经启动，查看容器日志</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718232658162.png" srcset="/img/loading.gif" alt="image-20200718232658162"></p><p>浏览器访问：<a href="http://10.10.10.9:30794/" target="_blank" rel="noopener">http://10.10.10.9:30794/</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718232858065.png" srcset="/img/loading.gif" alt="image-20200718232858065"></p><p>至此，单Master的k8s集群及部署docker应用已经成功。</p>]]></content>
    
    
    
    <tags>
      
      <tag>k8s单Master</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Centos7安装Ansible异常问题排查</title>
    <link href="/2020/07/28/Centos7%E5%AE%89%E8%A3%85Ansible%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"/>
    <url>/2020/07/28/Centos7%E5%AE%89%E8%A3%85Ansible%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/</url>
    
    <content type="html"><![CDATA[<p>目标：Centos7系统的机器安装Ansible</p><p>yum源：阿里云</p><p>外网：虚拟机可以连接外网</p><p>出现的问题：直接用命令yum -y install ansible，发现安装不上。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/ansible.png" srcset="/img/loading.gif" alt="image-20200724112514837"></p><p><strong>问题排查</strong>：</p><p>（1）原来安装ansible，需要先安装epel源</p><p>（2）<strong>yum install epel-release</strong></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/epel.png" srcset="/img/loading.gif" alt="image-20200724112635120"></p><p>然后<strong>yum repolis</strong>t查看源，发现并没有epel。</p><p>（3）排查机器上是否已经存在epel源：<strong>yum list installed|grep epel</strong></p><p>发现确实存在，先移除：<strong>yum remove epel-release.noarch</strong></p><p>（4）重新下载安装epel源</p><pre><code class="hljs shell">wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmrpm -ivh epel-release-latest-7.noarch.rpm</code></pre><p>接着查看源：<strong>yum repolist</strong>，发现epel源已经存在</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/repolist.png" srcset="/img/loading.gif" alt="image-20200724113312221"></p><p>（5）最后重新用命令：<strong>yum install ansible -y</strong> 安装Ansible即可。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/ansible_version.png" srcset="/img/loading.gif" alt="image-20200724113350777"></p>]]></content>
    
    
    
    <tags>
      
      <tag>ansible</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
