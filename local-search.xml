<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Redis Cluster集群搭建</title>
    <link href="/2020/07/30/Redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <url>/2020/07/30/Redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</url>
    
    <content type="html"><![CDATA[<p>Redis集群模式主要有2种：主从集群、分布式集群。</p><p>前者主要是为了高可用或是读写分离，后者为了更好的存储数据，负载均衡。</p><p><strong>redis集群提供了以下两个好处</strong></p><ul><li>将数据自动切分(split)到多个节点</li><li>当集群中的某一个节点故障时，redis还可以继续处理客户端的请求。</li></ul><p>一个 redis 集群包含 16384 个哈希槽（hash slot），数据库中的每个数据都属于这16384个哈希槽中的一个。集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽。集群中的每一个节点负责处理一部分哈希槽。</p><h3 id="一、集群机器规划"><a href="#一、集群机器规划" class="headerlink" title="一、集群机器规划"></a>一、集群机器规划</h3><h4 id="1、环境信息"><a href="#1、环境信息" class="headerlink" title="1、环境信息"></a>1、环境信息</h4><p>操作系统：Centos7</p><p>redis版本：5.0.8</p><h4 id="2、机器规划"><a href="#2、机器规划" class="headerlink" title="2、机器规划"></a>2、机器规划</h4><table><thead><tr><th align="center">节点名称</th><th align="center">IP</th></tr></thead><tbody><tr><td align="center">PaasDocker02</td><td align="center">10.1.34.48</td></tr><tr><td align="center">PaasMD02</td><td align="center">10.1.34.75</td></tr><tr><td align="center">PaasDocker01</td><td align="center">10.1.34.76</td></tr></tbody></table><p>每台机器分配2个redis节点：redis1，redis2</p><h3 id="二、搭建过程"><a href="#二、搭建过程" class="headerlink" title="二、搭建过程"></a>二、搭建过程</h3><h4 id="1、创建自定义redis配置文件"><a href="#1、创建自定义redis配置文件" class="headerlink" title="1、创建自定义redis配置文件"></a>1、创建自定义redis配置文件</h4><p>以10.1.34.48这台机器演示：</p><pre><code class="hljs shell">[root@PaasDocker02 /]# mkdir -p /usr/software/redis-cluster/[root@PaasDocker02 /]# cd /usr/software/redis-cluster/<span class="hljs-meta">#</span><span class="bash"> 解压传到redis-cluster文件夹下的redis-5.0.8.tar.gz</span>[root@PaasDocker02 redis-cluster]# tar -zxvf redis-5.0.8.tar.gz<span class="hljs-meta">#</span><span class="bash"> 将解压出来的redis-5.0.8文件夹重命名为redis1</span>[root@PaasDocker02 redis-cluster]# mv redis-5.0.8 redis1<span class="hljs-meta">#</span><span class="bash"> 同上再解压tar包并重命名为redis2</span>[root@PaasDocker02 redis-cluster]# mv redis-5.0.8 redis2<span class="hljs-meta">#</span><span class="bash"> 新增redis-data文件夹，存放redis数据</span>[root@PaasDocker02 redis-cluster]# mkdir redis-data<span class="hljs-meta">#</span><span class="bash"> 在redis-data文件夹下新建7001和7002文件夹</span>[root@PaasDocker02 redis-data]# mkdir 7001[root@PaasDocker02 redis-data]# mkdir 7002<span class="hljs-meta">#</span><span class="bash"> 新增redis-7001.conf配置文件</span>[root@PaasDocker02 redis-cluster]# touch redis-7001.conf[root@PaasDocker02 redis-cluster]# vi redis-7001.conf<span class="hljs-meta">#</span><span class="bash"> 在redis-7001.conf中添加以下内容</span>port 7001daemonize yespidfile /var/run/redis_7001.piddir /usr/software/redis-cluster/redis-data/7001cluster-enabled yescluster-config-file nodes_7001.confcluster-node-timeout 10100appendonly yes<span class="hljs-meta">#</span><span class="bash"> 新增redis-7001.conf配置文件</span>[root@PaasDocker02 redis-cluster]# touch redis-7002.conf[root@PaasDocker02 redis-cluster]# vi redis-7002.conf<span class="hljs-meta">#</span><span class="bash"> 在redis-7002.conf中添加以下内容</span>port 7002daemonize yespidfile /var/run/redis_7002.piddir /usr/software/redis-cluster/redis-data/7002cluster-enabled yescluster-config-file nodes_7002.confcluster-node-timeout 10100appendonly yes</code></pre><p>上面的步骤操作后，redis-cluster里面的内容如截图所示：</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/redis/redis%E6%9C%BA%E5%99%A8.png" srcset="/img/loading.gif" alt="image-20200729233011401"></p><p>同理，在10.1.34.75和10.1.34.76两台机器中分别操作10.1.34.48机器中的步骤。</p><h4 id="2、编译redis"><a href="#2、编译redis" class="headerlink" title="2、编译redis"></a>2、编译redis</h4><p>分别在redis1和redis2文件夹下执行make命令，执行结束后，可以看到redis1和redis2文件夹下可以看到生成的src文件夹。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/redis/redis_make.png" srcset="/img/loading.gif" alt="image-20200729234600485"></p><h4 id="3、创建启动集群的脚本"><a href="#3、创建启动集群的脚本" class="headerlink" title="3、创建启动集群的脚本"></a>3、创建启动集群的脚本</h4><pre><code class="hljs shell">[root@PaasDocker02 redis-cluster]# vi start-all.sh/usr/software/redis-cluster/redis1/src/redis-server /usr/software/redis-cluster/redis-7001.conf/usr/software/redis-cluster/redis2/src/redis-server /usr/software/redis-cluster/redis-7002.conf<span class="hljs-meta">#</span><span class="bash"> 添加可执行权限</span>[root@PaasDocker02 redis-cluster]# chmod +x start-all.sh</code></pre><p>为了后续停止集群方便，我们按照启动脚本的方式，也编写一个停止集群的脚本。</p><pre><code class="hljs shell">[root@PaasDocker02 redis-cluster]# vi stop-all.sh/usr/software/redis-cluster/redis1/src/redis-cli -p 7001 shutdown/usr/software/redis-cluster/redis2/src/redis-cli -p 7002 shutdown<span class="hljs-meta">#</span><span class="bash"> 添加可执行权限</span>[root@PaasDocker02 redis-cluster]# chmod +x stop-all.sh</code></pre><p>每台机器分别运行集群脚本</p><pre><code class="hljs shell">[root@PaasDocker02 redis-cluster]# ./start-all.sh49688:C 30 Jul 2020 14:12:23.752 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo49688:C 30 Jul 2020 14:12:23.752 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=49688, just started49688:C 30 Jul 2020 14:12:23.752 # Configuration loaded49690:C 30 Jul 2020 14:12:23.757 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo49690:C 30 Jul 2020 14:12:23.757 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=49690, just started49690:C 30 Jul 2020 14:12:23.757 # Configuration loaded[root@PaasDocker02 redis-cluster]# netstat -ntlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    tcp        0      0 0.0.0.0:17001           0.0.0.0:*               LISTEN      49689/redis-server  tcp        0      0 0.0.0.0:17002           0.0.0.0:*               LISTEN      49691/redis-server  tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN      864/rpcbind         tcp        0      0 192.168.122.1:53        0.0.0.0:*               LISTEN      1646/dnsmasq        tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1206/sshd           tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      1209/cupsd          tcp        0      0 0.0.0.0:7001            0.0.0.0:*               LISTEN      49689/redis-server  tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1378/master         tcp        0      0 0.0.0.0:7002            0.0.0.0:*               LISTEN      49691/redis-server  tcp        0      0 127.0.0.1:6010          0.0.0.0:*               LISTEN      45643/sshd: root@pt tcp6       0      0 :::17001                :::*                    LISTEN      49689/redis-server  tcp6       0      0 :::17002                :::*                    LISTEN      49691/redis-server  tcp6       0      0 :::111                  :::*                    LISTEN      864/rpcbind         tcp6       0      0 :::22                   :::*                    LISTEN      1206/sshd           tcp6       0      0 ::1:631                 :::*                    LISTEN      1209/cupsd          tcp6       0      0 :::7001                 :::*                    LISTEN      49689/redis-server  tcp6       0      0 ::1:25                  :::*                    LISTEN      1378/master         tcp6       0      0 :::7002                 :::*                    LISTEN      49691/redis-server  tcp6       0      0 ::1:6010                :::*                    LISTEN      45643/sshd: root@pt</code></pre><h4 id="4、配置redis-cluster集群"><a href="#4、配置redis-cluster集群" class="headerlink" title="4、配置redis cluster集群"></a>4、配置redis cluster集群</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 初始化创建集群</span>[root@PaasDocker02 redis-cluster]# cd redis1/src[root@PaasDocker02 src]# ./redis-cli --cluster create 10.1.34.48:7001 10.1.34.48:7002 10.1.34.75:7001 10.1.34.75:7002 10.1.34.76:7001 10.1.34.76:7002 --cluster-replicas 1<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Performing <span class="hljs-built_in">hash</span> slots allocation on 6 nodes...</span>Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 10.1.34.75:7002 to 10.1.34.48:7001Adding replica 10.1.34.76:7002 to 10.1.34.75:7001Adding replica 10.1.34.48:7002 to 10.1.34.76:7001M: 74b1797de0cadebbb799ead5f4bfbefee2b27c6c 10.1.34.48:7001   slots:[0-5460] (5461 slots) masterS: 77a38e94c940b3701158d413947c7f1343fc327b 10.1.34.48:7002   replicates b77c13d87ae67c54395a6292f01642284c695312M: 6c681b82c5b220e397f1fa939253e48b0ee39dde 10.1.34.75:7001   slots:[5461-10922] (5462 slots) masterS: 50ec08e3d47eb13a92d9a331a474d351ac4118e3 10.1.34.75:7002   replicates 74b1797de0cadebbb799ead5f4bfbefee2b27c6cM: b77c13d87ae67c54395a6292f01642284c695312 10.1.34.76:7001   slots:[10923-16383] (5461 slots) masterS: 7c359e52f2d73acf4fd55bf287b515ee280a5288 10.1.34.76:7002   replicates 6c681b82c5b220e397f1fa939253e48b0ee39ddeCan I set the above configuration? (type 'yes' to accept): yes<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Nodes configuration updated</span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Assign a different config epoch to each node</span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Sending CLUSTER MEET messages to join the cluster</span>Waiting for the cluster to join......<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Performing Cluster Check (using node 10.1.34.48:7001)</span>M: 74b1797de0cadebbb799ead5f4bfbefee2b27c6c 10.1.34.48:7001   slots:[0-5460] (5461 slots) master   1 additional replica(s)S: 77a38e94c940b3701158d413947c7f1343fc327b 10.1.34.48:7002   slots: (0 slots) slave   replicates b77c13d87ae67c54395a6292f01642284c695312S: 50ec08e3d47eb13a92d9a331a474d351ac4118e3 10.1.34.75:7002   slots: (0 slots) slave   replicates 74b1797de0cadebbb799ead5f4bfbefee2b27c6cM: b77c13d87ae67c54395a6292f01642284c695312 10.1.34.76:7001   slots:[10923-16383] (5461 slots) master   1 additional replica(s)M: 6c681b82c5b220e397f1fa939253e48b0ee39dde 10.1.34.75:7001   slots:[5461-10922] (5462 slots) master   1 additional replica(s)S: 7c359e52f2d73acf4fd55bf287b515ee280a5288 10.1.34.76:7002   slots: (0 slots) slave   replicates 6c681b82c5b220e397f1fa939253e48b0ee39dde[OK] All nodes agree about slots configuration.<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Check <span class="hljs-keyword">for</span> open slots...</span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Check slots coverage...</span>[OK] All 16384 slots covered.</code></pre><h3 id="三、集群验证"><a href="#三、集群验证" class="headerlink" title="三、集群验证"></a>三、集群验证</h3><h4 id="1、登录集群"><a href="#1、登录集群" class="headerlink" title="1、登录集群"></a>1、登录集群</h4><p>以10.1.34.48这台机器做测试，判断redis集群是否初始化成功。</p><pre><code class="hljs shell">[root@PaasDocker02 src]# ./redis-cli -c -h 127.0.0.1 -p 7001<span class="hljs-meta">#</span><span class="bash"> 查看集群情况</span>127.0.0.1:7001&gt; cluster infocluster_state:okcluster_slots_assigned:16384cluster_slots_ok:16384cluster_slots_pfail:0cluster_slots_fail:0cluster_known_nodes:6cluster_size:3cluster_current_epoch:6cluster_my_epoch:1cluster_stats_messages_ping_sent:693cluster_stats_messages_pong_sent:730cluster_stats_messages_sent:1423cluster_stats_messages_ping_received:725cluster_stats_messages_pong_received:693cluster_stats_messages_meet_received:5cluster_stats_messages_received:1423<span class="hljs-meta">#</span><span class="bash"> 查看集群节点</span>127.0.0.1:7001&gt; cluster nodes77a38e94c940b3701158d413947c7f1343fc327b 10.1.34.48:7002@17002 slave b77c13d87ae67c54395a6292f01642284c695312 0 1596094774477 5 connected50ec08e3d47eb13a92d9a331a474d351ac4118e3 10.1.34.75:7002@17002 slave 74b1797de0cadebbb799ead5f4bfbefee2b27c6c 0 1596094773000 4 connected74b1797de0cadebbb799ead5f4bfbefee2b27c6c 10.1.34.48:7001@17001 myself,master - 0 1596094775000 1 connected 0-5460b77c13d87ae67c54395a6292f01642284c695312 10.1.34.76:7001@17001 master - 0 1596094775480 5 connected 10923-163836c681b82c5b220e397f1fa939253e48b0ee39dde 10.1.34.75:7001@17001 master - 0 1596094774000 3 connected 5461-109227c359e52f2d73acf4fd55bf287b515ee280a5288 10.1.34.76:7002@17002 slave 6c681b82c5b220e397f1fa939253e48b0ee39dde 0 1596094774000 6 connected</code></pre><h4 id="2、集群节点分析"><a href="#2、集群节点分析" class="headerlink" title="2、集群节点分析"></a>2、集群节点分析</h4><p>以下是做redis cluster初始化的时候，打印到控制台的日志。</p><pre><code class="hljs shell">M: 74b1797de0cadebbb799ead5f4bfbefee2b27c6c 10.1.34.48:7001   slots:[0-5460] (5461 slots) master   1 additional replica(s)S: 77a38e94c940b3701158d413947c7f1343fc327b 10.1.34.48:7002   slots: (0 slots) slave   replicates b77c13d87ae67c54395a6292f01642284c695312S: 50ec08e3d47eb13a92d9a331a474d351ac4118e3 10.1.34.75:7002   slots: (0 slots) slave   replicates 74b1797de0cadebbb799ead5f4bfbefee2b27c6cM: b77c13d87ae67c54395a6292f01642284c695312 10.1.34.76:7001   slots:[10923-16383] (5461 slots) master   1 additional replica(s)M: 6c681b82c5b220e397f1fa939253e48b0ee39dde 10.1.34.75:7001   slots:[5461-10922] (5462 slots) master   1 additional replica(s)S: 7c359e52f2d73acf4fd55bf287b515ee280a5288 10.1.34.76:7002   slots: (0 slots) slave   replicates 6c681b82c5b220e397f1fa939253e48b0ee39dde</code></pre><p>对日志打印出来的节点信息做列表分析（节点ID较长，取首字母往后8位字符）</p><table><thead><tr><th align="center">节点Node</th><th align="center">节点ID</th><th align="center">节点角色</th><th align="center">节点复制</th><th align="center">哈希槽</th></tr></thead><tbody><tr><td align="center">10.1.34.48:7001</td><td align="center">74b1797d</td><td align="center">Master</td><td align="center"></td><td align="center">0-5460</td></tr><tr><td align="center">10.1.34.48:7002</td><td align="center">77a38e94</td><td align="center">Slave</td><td align="center">b77c13d8</td><td align="center"></td></tr><tr><td align="center">10.1.34.75:7002</td><td align="center">50ec08e3</td><td align="center">Slave</td><td align="center">74b1797d</td><td align="center"></td></tr><tr><td align="center">10.1.34.76:7001</td><td align="center">b77c13d8</td><td align="center">Master</td><td align="center"></td><td align="center">10923-16383</td></tr><tr><td align="center">10.1.34.75:7001</td><td align="center">6c681b82</td><td align="center">Master</td><td align="center"></td><td align="center">5461-10922</td></tr><tr><td align="center">10.1.34.76:7002</td><td align="center">7c359e52</td><td align="center">Slave</td><td align="center">6c681b82</td><td align="center"></td></tr></tbody></table><p>可以得出，每个节点对应的节点角色，如果是Slave节点，对应的从哪个节点复制信息。</p><h4 id="3、检查集群节点存储"><a href="#3、检查集群节点存储" class="headerlink" title="3、检查集群节点存储"></a>3、检查集群节点存储</h4><p>笔者还是以Master节点10.1.34.48:7001为例，新建<strong>key=k1,value=v1</strong>。</p><pre><code class="hljs shell">[root@PaasDocker02 src]# ./redis-cli -c -h 127.0.0.1 -p 7001127.0.0.1:7001&gt; keys *(empty list or set)127.0.0.1:7001&gt; set k1 v1<span class="hljs-meta">-&gt;</span><span class="bash"> Redirected to slot [12706] located at 10.1.34.76:7001</span>OK10.1.34.76:7001&gt; get k1"v1"[root@PaasDocker02 redis-cluster]# cd redis1/src[root@PaasDocker02 src]# ./redis-cli -c -h 127.0.0.1 -p 7001127.0.0.1:7001&gt; keys *(empty list or set)127.0.0.1:7001&gt; set k1 v1<span class="hljs-meta">-&gt;</span><span class="bash"> Redirected to slot [12706] located at 10.1.34.76:7001</span>OK10.1.34.76:7001&gt; get k1"v1"</code></pre><p>可以看到<strong>k1</strong>落到12706这个哈希槽，对应于节点：10.1.34.76:7001。</p><p>那么我们到10.1.34.76这台机器，登录到redis 7001的客户端。</p><pre><code class="hljs shell">[root@PaasDocker01 redis-cluster]# cd redis1/src[root@PaasDocker01 src]# ./redis-cli -c -h 127.0.0.1 -p 7001127.0.0.1:7001&gt; keys *1) "k1"127.0.0.1:7001&gt; get k1"v1"</code></pre><p>根据集群节点分析的情况来看，10.1.34.76:7001是Master节点，那么对应的Slave节点是10.1.34.48:7002。</p><p>因此我们接着登录到10.1.34.48这台机器redis 7002的客户端，查看<strong>k1</strong>是否复制到这里。</p><pre><code class="hljs shell">[root@PaasDocker02 redis-cluster]# cd redis1/src[root@PaasDocker02 src]# ./redis-cli -c -h 127.0.0.1 -p 7002127.0.0.1:7002&gt; keys *1) "k1"127.0.0.1:7002&gt; get k1<span class="hljs-meta">-&gt;</span><span class="bash"> Redirected to slot [12706] located at 10.1.34.76:7001</span>"v1"</code></pre><p>我们也可以通过redis的PC客户端做进一步验证：</p><p><strong>（1）连接10.1.34.76:7001</strong></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/redis/redis1.png" srcset="/img/loading.gif" alt=""></p><p><strong>（2）连接10.1.34.48:7002</strong></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/redis/redis2.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>redis集群</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker私仓Harbor部署</title>
    <link href="/2020/07/29/Docker%E7%A7%81%E4%BB%93Harbor%E9%83%A8%E7%BD%B2/"/>
    <url>/2020/07/29/Docker%E7%A7%81%E4%BB%93Harbor%E9%83%A8%E7%BD%B2/</url>
    
    <content type="html"><![CDATA[<p>本篇笔者将为大家介绍的是一个企业级镜像仓库Harbor，将作为我们容器云平台的镜像仓库中心。</p><h3 id="一、Harbor介绍"><a href="#一、Harbor介绍" class="headerlink" title="一、Harbor介绍"></a>一、Harbor介绍</h3><h4 id="1、Harbor官网"><a href="#1、Harbor官网" class="headerlink" title="1、Harbor官网"></a>1、Harbor官网</h4><p>Habor是由VMWare公司开源的容器镜像仓库。事实上，Habor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制 ，AD/LDAP集成以及审计日志等，足以满足基本企业需求。</p><p><strong>github地址</strong>：<a href="https://github.com/goharbor/harbor" target="_blank" rel="noopener">https://github.com/goharbor/harbor</a></p><h4 id="2、Harbor主要功能"><a href="#2、Harbor主要功能" class="headerlink" title="2、Harbor主要功能"></a>2、Harbor主要功能</h4><ul><li><p><strong>基于角色访问控制（RBAC）</strong></p><p>在企业中，通常有不同的开发团队负责不同的项目，镜像像代码一样，每个人角色不同需求也不同，因此就需要访问权限控制，根据角色分配相应的权限。<br>例如，开发人员需要对项目构建这就用到读写权限（push/pull），测试人员只需要读权限（pull），运维一般管理镜像仓库，具备权限分配能力，项目经理具有所有权限。 </p></li><li><p><strong>镜像复制</strong></p><p>可以将仓库中的镜像同步到远程的Harbor，类似于MySQL主从同步功能。</p></li><li><p><strong>LDAP</strong></p><p>Harbor支持LDAP认证，可以很轻易接入已有的LDAP。</p></li><li><p><strong>镜像删除和空间回收</strong></p><p>Harbor支持在Web删除镜像，回收无用的镜像，释放磁盘空间。</p></li><li><p><strong>图形页面管理</strong></p><p>用户很方面搜索镜像及项目管理。</p></li><li><p><strong>审计</strong></p><p>对仓库的所有操作都有记录。</p></li><li><p><strong>REST API</strong></p><p>完整的API，方便与外部集成。</p></li></ul><h4 id="3、Harbor组件"><a href="#3、Harbor组件" class="headerlink" title="3、Harbor组件"></a>3、Harbor组件</h4><table><thead><tr><th align="center">组件</th><th align="center">功能</th></tr></thead><tbody><tr><td align="center">harbor-adminserver</td><td align="center">配置管理中心</td></tr><tr><td align="center">harbor-db</td><td align="center">Mysql数据库</td></tr><tr><td align="center">harbor-jobservice</td><td align="center">负责镜像复制</td></tr><tr><td align="center">harbor-log</td><td align="center">记录操作日志</td></tr><tr><td align="center">harbor-ui</td><td align="center">Web管理页面和API</td></tr><tr><td align="center">nginx</td><td align="center">前端代理，负责前端页面和镜像上传/下载转发</td></tr><tr><td align="center">redis</td><td align="center">会话</td></tr><tr><td align="center">registry</td><td align="center">镜像存储</td></tr></tbody></table><h3 id="二、Harbor部署"><a href="#二、Harbor部署" class="headerlink" title="二、Harbor部署"></a>二、Harbor部署</h3><h4 id="1、安装docker-compose"><a href="#1、安装docker-compose" class="headerlink" title="1、安装docker-compose"></a>1、安装docker-compose</h4><pre><code class="hljs shell">[root@PaasHarbor01 software]# sudo curl -L "https://github.com/docker/compose/releases/download/1.23.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose<span class="hljs-meta">  %</span><span class="bash"> Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span>                                 Dload  Upload   Total   Spent    Left  Speed100   638  100   638    0     0    160      0  0:00:03  0:00:03 --:--:--   160100 11.2M  100 11.2M    0     0   588k      0  0:00:19  0:00:19 --:--:-- 2112k[root@PaasHarbor01 software]# chmod +x /usr/local/bin/docker-compose[root@PaasHarbor01 software]# docker-compose --versiondocker-compose version 1.23.2, build 1110ad01</code></pre><p><strong>备注：机器上已经安装好Docker，版本：19.03.9</strong></p><h4 id="2、HTTP方式部署"><a href="#2、HTTP方式部署" class="headerlink" title="2、HTTP方式部署"></a>2、HTTP方式部署</h4><pre><code class="hljs shell">[root@PaasHarbor01 harbor]# vi harbor.yml <span class="hljs-meta">#</span><span class="bash"> 1、将hostname改为机器ip或者ip对应的机器名</span>hostname: 10.1.34.7<span class="hljs-meta">#</span><span class="bash"> 2、将https下的内容注释，否则需要配置https证书</span><span class="hljs-meta">#</span><span class="bash"> https:</span><span class="hljs-meta">  #</span><span class="bash"> https port <span class="hljs-keyword">for</span> harbor, default is 443</span><span class="hljs-meta"> #</span><span class="bash"> port: 443</span><span class="hljs-meta">  #</span><span class="bash"> The path of cert and key files <span class="hljs-keyword">for</span> nginx</span><span class="hljs-meta">  #</span><span class="bash">certificate: /your/certificate/path</span><span class="hljs-meta">  #</span><span class="bash">private_key: /your/private/key/path</span><span class="hljs-meta">#</span><span class="bash"> 3、根据需要修改harbor初始密码，笔者暂时没有修改</span>harbor_admin_password: Harbor12345<span class="hljs-meta">#</span><span class="bash"> 准备配置文件</span>[root@PaasHarbor01 harbor]# ./prepare prepare base dir is set to /usr/software/harborWARNING:root:WARNING: HTTP protocol is insecure. Harbor will deprecate http protocol in the future. Please make sure to upgrade to httpsGenerated configuration file: /config/log/logrotate.confGenerated configuration file: /config/log/rsyslog_docker.confGenerated configuration file: /config/nginx/nginx.confGenerated configuration file: /config/core/envGenerated configuration file: /config/core/app.confGenerated configuration file: /config/registry/config.ymlGenerated configuration file: /config/registryctl/envGenerated configuration file: /config/db/envGenerated configuration file: /config/jobservice/envGenerated configuration file: /config/jobservice/config.ymlGenerated and saved secret to file: /secret/keys/secretkeyGenerated certificate, key file: /secret/core/private_key.pem, cert file: /secret/registry/root.crtGenerated configuration file: /compose_location/docker-compose.ymlClean up the input dir<span class="hljs-meta">#</span><span class="bash"> 安装并启动Harbor</span>[root@PaasHarbor01 harbor]# ./install.sh [Step 0]: checking if docker is installed ...Note: docker version: 19.03.9[Step 1]: checking docker-compose is installed ...Note: docker-compose version: 1.23.2[Step 2]: loading Harbor images ...7fd57902f2bf: Loading layer [==================================================&gt;]  8.465MB/8.465MB9f7a3727b327: Loading layer [==================================================&gt;]   67.5MB/67.5MBb165ecbfa6a0: Loading layer [==================================================&gt;]  3.072kB/3.072kB618609e47ff5: Loading layer [==================================================&gt;]  3.584kB/3.584kB4941a988de67: Loading layer [==================================================&gt;]  68.33MB/68.33MBLoaded image: goharbor/chartmuseum-photon:v1.10.4c249fd1745d2: Loading layer [==================================================&gt;]  12.24MB/12.24MB6f099dcc4dab: Loading layer [==================================================&gt;]  42.51MB/42.51MBeb32b6d20d4b: Loading layer [==================================================&gt;]  5.632kB/5.632kB5acd92618fef: Loading layer [==================================================&gt;]  40.45kB/40.45kB62b57401b9ca: Loading layer [==================================================&gt;]  42.51MB/42.51MBd7b6ded42cfb: Loading layer [==================================================&gt;]   2.56kB/2.56kBLoaded image: goharbor/harbor-core:v1.10.431b3ca7fa226: Loading layer [==================================================&gt;]   63.6MB/63.6MBb9972bab1402: Loading layer [==================================================&gt;]  66.73MB/66.73MB56b3ba4b4a66: Loading layer [==================================================&gt;]  5.632kB/5.632kB1654024d89fe: Loading layer [==================================================&gt;]   2.56kB/2.56kB040ec6bf5851: Loading layer [==================================================&gt;]   2.56kB/2.56kBe93cd0c30c28: Loading layer [==================================================&gt;]   2.56kB/2.56kBaed062c3be21: Loading layer [==================================================&gt;]   2.56kB/2.56kB820d1a1df842: Loading layer [==================================================&gt;]  10.75kB/10.75kBLoaded image: goharbor/harbor-db:v1.10.4ce217f401320: Loading layer [==================================================&gt;]  8.466MB/8.466MBb324500c7da3: Loading layer [==================================================&gt;]  3.584kB/3.584kB042b5242fe78: Loading layer [==================================================&gt;]  20.94MB/20.94MB87dd45007ea3: Loading layer [==================================================&gt;]  3.072kB/3.072kB651d502d735c: Loading layer [==================================================&gt;]  8.662MB/8.662MBfe72a4614aa1: Loading layer [==================================================&gt;]  30.42MB/30.42MBLoaded image: goharbor/harbor-registryctl:v1.10.45de330f38841: Loading layer [==================================================&gt;]   8.46MB/8.46MB0af0ddd91395: Loading layer [==================================================&gt;]  6.239MB/6.239MB3685afd2d128: Loading layer [==================================================&gt;]  16.04MB/16.04MBd8057fcd0a39: Loading layer [==================================================&gt;]  28.25MB/28.25MB0340225731b6: Loading layer [==================================================&gt;]  22.02kB/22.02kB06d8d803f0eb: Loading layer [==================================================&gt;]  50.52MB/50.52MBLoaded image: goharbor/notary-server-photon:v1.10.476eab6dc7bf5: Loading layer [==================================================&gt;]  332.6MB/332.6MBc96d1ad1968e: Loading layer [==================================================&gt;]  135.2kB/135.2kBLoaded image: goharbor/harbor-migrator:v1.10.47426785037a5: Loading layer [==================================================&gt;]  10.31MB/10.31MBb9a0601e3558: Loading layer [==================================================&gt;]  7.698MB/7.698MBaac781885802: Loading layer [==================================================&gt;]  223.2kB/223.2kB8af4d736a2ab: Loading layer [==================================================&gt;]  195.1kB/195.1kB5fef45ce538d: Loading layer [==================================================&gt;]  15.36kB/15.36kB5f98131a71d5: Loading layer [==================================================&gt;]  3.584kB/3.584kBLoaded image: goharbor/harbor-portal:v1.10.4528ae1964423: Loading layer [==================================================&gt;]  12.24MB/12.24MBb03ff000935f: Loading layer [==================================================&gt;]  49.37MB/49.37MBLoaded image: goharbor/harbor-jobservice:v1.10.46e2646825500: Loading layer [==================================================&gt;]  89.65MB/89.65MBfb20b8d71cf1: Loading layer [==================================================&gt;]  3.072kB/3.072kBd566c1cc124d: Loading layer [==================================================&gt;]   59.9kB/59.9kBc427dc7cb315: Loading layer [==================================================&gt;]  61.95kB/61.95kBLoaded image: goharbor/redis-photon:v1.10.4Loaded image: goharbor/prepare:v1.10.49fd7cf078b16: Loading layer [==================================================&gt;]  49.93MB/49.93MBbffa9c13b070: Loading layer [==================================================&gt;]  3.584kB/3.584kB5bc5a2da3367: Loading layer [==================================================&gt;]  3.072kB/3.072kBd207162a345a: Loading layer [==================================================&gt;]   2.56kB/2.56kB3f5fa111d1ff: Loading layer [==================================================&gt;]  3.072kB/3.072kB6fac1f97e0a4: Loading layer [==================================================&gt;]  3.584kB/3.584kB39089450a8d3: Loading layer [==================================================&gt;]  12.29kB/12.29kBc43cc9ac71a3: Loading layer [==================================================&gt;]  5.632kB/5.632kBLoaded image: goharbor/harbor-log:v1.10.493dfe2d38dda: Loading layer [==================================================&gt;]  115.3MB/115.3MBa2d6890966ca: Loading layer [==================================================&gt;]  12.15MB/12.15MB008d8a39ac95: Loading layer [==================================================&gt;]  3.072kB/3.072kBa06e99290956: Loading layer [==================================================&gt;]  49.15kB/49.15kB6d0c609a7ea0: Loading layer [==================================================&gt;]  3.584kB/3.584kBcc7d9f19817b: Loading layer [==================================================&gt;]  13.03MB/13.03MBLoaded image: goharbor/clair-photon:v1.10.40c8c48462931: Loading layer [==================================================&gt;]  8.466MB/8.466MB7c096b7a5806: Loading layer [==================================================&gt;]   9.71MB/9.71MBf18d35335b53: Loading layer [==================================================&gt;]   9.71MB/9.71MBLoaded image: goharbor/clair-adapter-photon:v1.10.4f55180240dc6: Loading layer [==================================================&gt;]  10.31MB/10.31MBLoaded image: goharbor/nginx-photon:v1.10.44a575c1c2167: Loading layer [==================================================&gt;]  8.466MB/8.466MBd0e9899aeeb5: Loading layer [==================================================&gt;]  3.584kB/3.584kBdb6d9646f0e0: Loading layer [==================================================&gt;]  3.072kB/3.072kB478d5f29f1a6: Loading layer [==================================================&gt;]  20.94MB/20.94MB1fbbee6ba37e: Loading layer [==================================================&gt;]  21.76MB/21.76MBLoaded image: goharbor/registry-photon:v1.10.410bbb8d426b9: Loading layer [==================================================&gt;]  14.61MB/14.61MB91b66eb6b6b0: Loading layer [==================================================&gt;]  28.25MB/28.25MB58956c7bbf02: Loading layer [==================================================&gt;]  22.02kB/22.02kB1c86ba20384f: Loading layer [==================================================&gt;]  49.09MB/49.09MBLoaded image: goharbor/notary-signer-photon:v1.10.4[Step 3]: preparing environment ...[Step 4]: preparing harbor configs ...prepare base dir is set to /usr/software/harborWARNING:root:WARNING: HTTP protocol is insecure. Harbor will deprecate http protocol in the future. Please make sure to upgrade to httpsClearing the configuration file: /config/log/logrotate.confClearing the configuration file: /config/log/rsyslog_docker.confClearing the configuration file: /config/nginx/nginx.confClearing the configuration file: /config/core/envClearing the configuration file: /config/core/app.confClearing the configuration file: /config/registry/config.ymlClearing the configuration file: /config/registryctl/envClearing the configuration file: /config/registryctl/config.ymlClearing the configuration file: /config/db/envClearing the configuration file: /config/jobservice/envClearing the configuration file: /config/jobservice/config.ymlGenerated configuration file: /config/log/logrotate.confGenerated configuration file: /config/log/rsyslog_docker.confGenerated configuration file: /config/nginx/nginx.confGenerated configuration file: /config/core/envGenerated configuration file: /config/core/app.confGenerated configuration file: /config/registry/config.ymlGenerated configuration file: /config/registryctl/envGenerated configuration file: /config/db/envGenerated configuration file: /config/jobservice/envGenerated configuration file: /config/jobservice/config.ymlloaded secret from file: /secret/keys/secretkeyGenerated configuration file: /compose_location/docker-compose.ymlClean up the input dir[Step 5]: starting Harbor ...Creating network "harbor_harbor" with the default driverCreating harbor-log ... doneCreating harbor-db     ... doneCreating harbor-portal ... doneCreating registryctl   ... doneCreating registry      ... doneCreating redis         ... doneCreating harbor-core   ... doneCreating harbor-jobservice ... doneCreating nginx             ... done✔ ----Harbor has been installed and started successfully.----</code></pre><p>接着，我们使用命令查看harbor相关docker启动情况</p><pre><code class="hljs shell">[root@PaasHarbor01 harbor]# docker-compose ps      Name                     Command                  State                 Ports          ---------------------------------------------------------------------------------------------harbor-core         /harbor/harbor_core              Up (healthy)                            harbor-db           /docker-entrypoint.sh            Up (healthy)   5432/tcp                 harbor-jobservice   /harbor/harbor_jobservice  ...   Up (healthy)                            harbor-log          /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514-&gt;10514/tcpharbor-portal       nginx -g daemon off;             Up (healthy)   8080/tcp                 nginx               nginx -g daemon off;             Up (healthy)   0.0.0.0:80-&gt;8080/tcp     redis               redis-server /etc/redis.conf     Up (healthy)   6379/tcp                 registry            /home/harbor/entrypoint.sh       Up (healthy)   5000/tcp                 registryctl         /home/harbor/start.sh            Up (healthy)</code></pre><h4 id="3、访问Harbor-UI"><a href="#3、访问Harbor-UI" class="headerlink" title="3、访问Harbor UI"></a>3、访问Harbor UI</h4><p>默认端口：80</p><p>浏览器访问：<a href="http://10.1.34.7" target="_blank" rel="noopener">http://10.1.34.7</a> 进入Harbor登录页</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/harbor_ui.png" srcset="/img/loading.gif" alt=""></p><p><strong>账号：admin</strong><br><strong>密码：Harbor12345</strong></p><p>登录后，查看Harbor页面</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/harbor_project.png" srcset="/img/loading.gif" alt=""></p><h4 id="4、Harbor的使用"><a href="#4、Harbor的使用" class="headerlink" title="4、Harbor的使用"></a>4、Harbor的使用</h4><p><strong>（1）新建项目</strong></p><p>从上图可以看到，library是默认自带的项目，通常用这个存储一些公共的镜像，这个项目下镜像谁都可以pull，但不能push，push需要先登录。</p><p>笔者新建项目registry，访问级别设置为私有。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/new_registry.png" srcset="/img/loading.gif" alt=""></p><p><strong>（2）为新建的项目赋予新用户push权限</strong></p><p>先创建一个用户：</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/new_person.png" srcset="/img/loading.gif" alt=""></p><p>进入registry项目，将用户加入这个成员：</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/add_person_project.png" srcset="/img/loading.gif" alt=""></p><p><strong>（3）推送镜像到仓库</strong></p><p>笔者这里为了演示方便，从DockerHub仓库中随便拉取一个镜像hello-world，然后将这个镜像推送到我们新建的项目registry。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/docker_hub.png" srcset="/img/loading.gif" alt=""></p><p>拉取镜像，并查看</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/hello-world.png" srcset="/img/loading.gif" alt=""></p><p>推送镜像的格式为：<strong>镜像中心域名  项目名称  名称  版本</strong></p><p>因此如果我们要推送到指定的镜像仓库就必须指定仓库中心地址</p><p>接下来，先给pull下来的hello-world镜像重新打tag</p><pre><code class="hljs shell">[root@PaasHarbor01 ~]# docker image tag hello-world 10.1.34.7/registry/hello-world:1.0[root@PaasHarbor01 ~]# docker imagesREPOSITORY                             TAG                 IMAGE ID            CREATED             SIZEgoharbor/chartmuseum-photon            v1.10.4             4d6611b3b6a9        2 weeks ago         178MBgoharbor/harbor-migrator               v1.10.4             c6ba18cc92c0        2 weeks ago         357MBgoharbor/redis-photon                  v1.10.4             1733199a8380        2 weeks ago         122MBgoharbor/clair-adapter-photon          v1.10.4             4d7fec33eb52        2 weeks ago         61.2MBgoharbor/clair-photon                  v1.10.4             48f8d69c3f63        2 weeks ago         171MBgoharbor/notary-server-photon          v1.10.4             3cc30fe05041        2 weeks ago         143MBgoharbor/notary-signer-photon          v1.10.4             46ecb328c811        2 weeks ago         140MBgoharbor/harbor-registryctl            v1.10.4             503dda3f193e        2 weeks ago         102MBgoharbor/registry-photon               v1.10.4             96183605aaeb        2 weeks ago         84.5MBgoharbor/nginx-photon                  v1.10.4             f8f638056eee        2 weeks ago         43.6MBgoharbor/harbor-log                    v1.10.4             b0de11e1ba03        2 weeks ago         82.1MBgoharbor/harbor-jobservice             v1.10.4             91c262f629d2        2 weeks ago         143MBgoharbor/harbor-core                   v1.10.4             cc013d5caa80        2 weeks ago         129MBgoharbor/harbor-portal                 v1.10.4             fec0c21d0a67        2 weeks ago         51.7MBgoharbor/harbor-db                     v1.10.4             2f077a558a2c        2 weeks ago         161MBgoharbor/prepare                       v1.10.4             85d07a7c81cd        2 weeks ago         168MBhello-world                            latest              bf756fb1ae65        6 months ago        13.3kB10.1.34.7/registry/hello-world         1.0                 bf756fb1ae65        6 months ago        13.3kBkubernetesui/dashboard                 v2.0.0-beta4        6802d83967b9        11 months ago       84MBkubernetesui/metrics-scraper           v1.0.1              709901356c11        12 months ago       40.1MBlizhenliang/flannel                    v0.11.0-amd64       ff281650a721        18 months ago       52.6MBlizhenliang/nginx-ingress-controller   0.20.0              a3f21ec4bd11        22 months ago       513MBlizhenliang/coredns                    1.2.2               367cdc8433a4        23 months ago       39.2MBlizhenliang/pause-amd64                3.0                 99e59f495ffa        4 years ago         747kB</code></pre><p>可以看到，已经存在重新打tag的镜像 ：<strong>10.1.34.7/registry/hello-world:1.0</strong></p><p>我们接着将镜像推送到我们自己的项目仓库。</p><pre><code class="hljs shell">[root@PaasHarbor01 harbor]# docker push 10.1.34.7/registry/hello-world:1.0Error response from daemon: Get https://10.1.34.7/v2/: dial tcp 10.1.34.7:443: connect: connection refused</code></pre><p>这个即Docker登录远程仓库https的问题，我们需要修改Docker的配置文件daemon.json，在insecure-registries中加上10.1.34.7。</p><pre><code class="hljs shell">[root@PaasHarbor01 /]# vi /etc/docker/daemon.json&#123;  "registry-mirrors": ["https://b9pmyelo.mirror.aliyuncs.com"],  "insecure-registries": ["192.168.31.70","10.1.34.7"],  "log-driver": "json-file",  "log-opts": &#123;    "max-size": "100m"  &#125;,  "storage-driver": "overlay2"&#125;</code></pre><p>然后我们需要刷新配置并且重启Docker</p><pre><code class="hljs shell">[root@PaasHarbor01 /]# systemctl daemon-reload[root@PaasHarbor01 /]# systemctl restart docker</code></pre><p>接着我们再次尝试推送镜像到Harbor</p><pre><code class="hljs shell">[root@PaasHarbor01 harbor]# docker push 10.1.34.7/registry/hello-world:1.0The push refers to repository [10.1.34.7/registry/hello-world]9c27e219663c: Preparing denied: requested access to the resource is denied</code></pre><p>可以看到，推送被拒绝，因此我们需要先登录到Harbor。</p><pre><code class="hljs shell">[root@PaasHarbor01 harbor]# docker login 10.1.34.7Username: calvinPassword: WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded</code></pre><p>我们再次推送镜像到私仓</p><pre><code class="hljs shell">[root@PaasHarbor01 harbor]# docker push 10.1.34.7/registry/hello-world:1.0The push refers to repository [10.1.34.7/registry/hello-world]9c27e219663c: Pushed 1.0: digest: sha256:90659bf80b44ce6be8234e6ff90a1ac34acbeb826903b02cfa0da11c82cbc042 size: 525</code></pre><p>发现镜像已经推送到Harbor中，我们查看Harbor的UI页。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/harbor/harbor_image.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>harbor</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>k8s中搭建Nexus私仓</title>
    <link href="/2020/07/28/k8s%E4%B8%AD%E6%90%AD%E5%BB%BANexus%E7%A7%81%E4%BB%93/"/>
    <url>/2020/07/28/k8s%E4%B8%AD%E6%90%AD%E5%BB%BANexus%E7%A7%81%E4%BB%93/</url>
    
    <content type="html"><![CDATA[<p>本篇笔者将通过编写yaml文件，执行kubectl命令，搭建Maven私服Nexus3。</p><p>备注：笔者以下操作均在k8s-master1机器上执行（选择任意一个Master节点执行均可）</p><h3 id="一、编写yaml文件"><a href="#一、编写yaml文件" class="headerlink" title="一、编写yaml文件"></a>一、编写yaml文件</h3><h4 id="1、创建自定义的命名空间Namespace"><a href="#1、创建自定义的命名空间Namespace" class="headerlink" title="1、创建自定义的命名空间Namespace"></a>1、创建自定义的命名空间Namespace</h4><p>在实际的项目开发中，我们需要将容器运行在自定义的命名空间中，因此笔者在本篇也新建一个Namespace。</p><pre><code class="hljs shell">[root@k8s-master1 ~]# mkdir -p deploy/namespace[root@k8s-master1 ~]# cd deploy/namespace/[root@k8s-master1 namespace]# touch k8s-namespace.yaml[root@k8s-master1 namespace]# vi k8s-namespace.yamlapiVersion: v1kind: Namespacemetadata:  name: 你自己定义的命名空间名</code></pre><p>接着运行kubectl命令：</p><pre><code class="hljs shell">[root@k8s-master1 namespace]# kubectl apply -f k8s-namespace.yaml</code></pre><h4 id="2、编写nexus-pv-pvc-yaml"><a href="#2、编写nexus-pv-pvc-yaml" class="headerlink" title="2、编写nexus-pv-pvc.yaml"></a>2、编写nexus-pv-pvc.yaml</h4><pre><code class="hljs shell">apiVersion: v1kind: PersistentVolumemetadata:  name: nexus3-data-pv  labels:    app: nexus3-data-pvspec:  capacity:    storage: 100Gi  accessModes:    - ReadWriteOnce  persistentVolumeReclaimPolicy: Recycle  hostPath:    path: /data/maven-nexus---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: nexus3-data-pvc  labels:    app: nexus3-data-pvcspec:  accessModes:    - ReadWriteOnce  resources:    requests:      storage: 100Gi  selector:    matchLabels:      app: nexus3-data-pv</code></pre><h4 id="3、编写nexus-service-yaml"><a href="#3、编写nexus-service-yaml" class="headerlink" title="3、编写nexus-service.yaml"></a>3、编写nexus-service.yaml</h4><pre><code class="hljs shell">apiVersion: v1kind: Servicemetadata:  labels:    app: nexus3  name: nexus3spec:  type: NodePort  ports:  - port: 8081    targetPort: 8081    nodePort: 30020    name: web-ui  - port: 5000    targetPort: 5000    nodePort: 30050    name: docker-group  - port: 8889    targetPort: 8889    nodePort: 30080    name: docker-push  selector:    app: nexus3</code></pre><h4 id="4、编写nexus-deployment-yaml"><a href="#4、编写nexus-deployment-yaml" class="headerlink" title="4、编写nexus-deployment.yaml"></a>4、编写nexus-deployment.yaml</h4><pre><code class="hljs shell">kind: DeploymentapiVersion: apps/v1metadata:  labels:    app: nexus3  name: nexus3spec:  replicas: 1  selector:    matchLabels:      app: nexus3  template:    metadata:      labels:        app: nexus3    spec:      containers:        - name: nexus3          image: sonatype/nexus3:latest          imagePullPolicy: IfNotPresent          ports:          - containerPort: 8081            protocol: TCP          volumeMounts:          - name: nexus-data            mountPath: /nexus-data       volumes:        - name: nexus-data          persistentVolumeClaim:            claimName: nexus3-data-pvc      nodeSelector:        kubernetes.io/hostname: paasnexus01</code></pre><p>以上yaml文件中指定了Nexus镜像的pull地址，容器的默认端口以及宿主机的固定端口映射。另外，指定了容器运行所在的k8s节点。</p><p>查看nodes的hostname</p><pre><code class="hljs shell">[root@k8s-master1 ~]# kubectl get node --show-labelsNAME           STATUS   ROLES    AGE     VERSION   LABELSk8s-master1    Ready    &lt;none&gt;   32h     v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master1,kubernetes.io/os=linuxk8s-master2    Ready    &lt;none&gt;   32h     v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master2,kubernetes.io/os=linuxk8s-master3    Ready    &lt;none&gt;   32h     v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master3,kubernetes.io/os=linuxk8s-node1      Ready    &lt;none&gt;   32h     v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node1,kubernetes.io/os=linuxk8s-node2      Ready    &lt;none&gt;   32h     v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node2,kubernetes.io/os=linuxk8s-node3      Ready    &lt;none&gt;   32h     v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node3,kubernetes.io/os=linuxpaasharbor01   Ready    &lt;none&gt;   5h15m   v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=paasharbor01,kubernetes.io/os=linuxpaasnexus01    Ready    &lt;none&gt;   5h14m   v1.18.6   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=paasnexus01,kubernetes.io/os=linux</code></pre><p>设置指定机器运行</p><pre><code class="hljs shell">nodeSelector:  kubernetes.io/hostname: paasnexus01</code></pre><h3 id="二、执行yaml文件"><a href="#二、执行yaml文件" class="headerlink" title="二、执行yaml文件"></a>二、执行yaml文件</h3><h4 id="1、创建存放yaml文件的文件夹"><a href="#1、创建存放yaml文件的文件夹" class="headerlink" title="1、创建存放yaml文件的文件夹"></a>1、创建存放yaml文件的文件夹</h4><pre><code class="hljs shell">[root@k8s-master1 ~]# mkdir -p deploy/nexus[root@k8s-master1 ~]# cd deploy/nexus/<span class="hljs-meta">#</span><span class="bash"> 将nexus-pv-pvc.yaml，nexus-service.yaml和nexus-deployment.yaml上传到nexus文件夹中。</span></code></pre><p>另外，还要在nexus容器运行所在的宿主机上<strong>设置挂载目录的访问权限</strong>。</p><pre><code class="hljs shell">[root@PaasNexus01 ~]# mkdir -p /data/maven-nexus[root@PaasNexus01 ~]# chmod 777 /data/maven-nexus/</code></pre><h4 id="2、kubectl命令"><a href="#2、kubectl命令" class="headerlink" title="2、kubectl命令"></a>2、kubectl命令</h4><pre><code class="hljs shell">[root@k8s-master1 ~]# kubectl apply -f nexus/ -n 你自己定义的命名空间名</code></pre><h3 id="三、验证Nexus"><a href="#三、验证Nexus" class="headerlink" title="三、验证Nexus"></a>三、验证Nexus</h3><h4 id="1、切换命名空间，查看运行Pods"><a href="#1、切换命名空间，查看运行Pods" class="headerlink" title="1、切换命名空间，查看运行Pods"></a>1、切换命名空间，查看运行Pods</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/k8s-nexus3.png" srcset="/img/loading.gif" alt=""></p><h4 id="2、访问Nexus-UI"><a href="#2、访问Nexus-UI" class="headerlink" title="2、访问Nexus UI"></a>2、访问Nexus UI</h4><p>从上图中可以看到Nexus3容器运行在passnexus01节点，对应的机器IP：10.1.34.20</p><p>浏览器访问：<a href="http://10.1.34.20:30020/" target="_blank" rel="noopener">http://10.1.34.20:30020/</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/k8s-nexus3.png" srcset="/img/loading.gif" alt=""></p><h4 id="3、修改admin密码"><a href="#3、修改admin密码" class="headerlink" title="3、修改admin密码"></a>3、修改admin密码</h4><p>进入nexus3容器运行所在的宿主机，查看初始的admin密码</p><pre><code class="hljs shell">[root@PaasNexus01 ~]# cd /data/maven-nexus/[root@PaasNexus01 maven-nexus]# cat admin.password</code></pre><p>再次访问Nexus UI，修改admin用户的密码，登录后查看</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/k8s-nexus-ui.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    
    <tags>
      
      <tag>nexus</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用yaml文件创建一个Kubernetes应用</title>
    <link href="/2020/07/28/%E4%BD%BF%E7%94%A8yaml%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAKubernetes%E5%BA%94%E7%94%A8/"/>
    <url>/2020/07/28/%E4%BD%BF%E7%94%A8yaml%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAKubernetes%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>本篇笔者将通过编写yaml文件，执行kubectl命令，实现Kubernetes应用的运行。为了比较方便的验证应用创建是否成功，笔者以微服务组件Eureka Server作为演示（有UI页面，方便验证）。</p><p>备注：笔者以下操作均在k8s-master1机器上执行（选择任意一个Master节点执行均可）</p><h3 id="一、编写yaml文件"><a href="#一、编写yaml文件" class="headerlink" title="一、编写yaml文件"></a>一、编写yaml文件</h3><h4 id="1、yaml语法"><a href="#1、yaml语法" class="headerlink" title="1、yaml语法"></a>1、yaml语法</h4><p>如果大家对k8s-yaml的语法还是很熟悉，可以参考博客：<a href="https://www.cnblogs.com/fuyuteng/p/9460534.html" target="_blank" rel="noopener">https://www.cnblogs.com/fuyuteng/p/9460534.html</a></p><h4 id="2、eureka-yaml"><a href="#2、eureka-yaml" class="headerlink" title="2、eureka.yaml"></a>2、eureka.yaml</h4><p>针对Eureka应用，笔者的yaml文件如下，大家可以直接复制使用，已验证。</p><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">eureka</span>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">eureka</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span>  <span class="hljs-attr">ports:</span>  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">8761</span>    <span class="hljs-attr">nodePort:</span> <span class="hljs-number">30090</span> <span class="hljs-comment">#service对外开放端口</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">eureka</span><span class="hljs-meta">---</span><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><span class="hljs-attr">metadata:</span>  <span class="hljs-attr">labels:</span>    <span class="hljs-attr">app:</span> <span class="hljs-string">eureka</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">eureka</span><span class="hljs-attr">spec:</span>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span> <span class="hljs-comment">#运行容器的副本数，修改这里可以快速修改分布式节点数量</span>  <span class="hljs-attr">selector:</span>    <span class="hljs-attr">matchLabels:</span>      <span class="hljs-attr">app:</span> <span class="hljs-string">eureka</span>  <span class="hljs-attr">template:</span>    <span class="hljs-attr">metadata:</span>      <span class="hljs-attr">labels:</span>        <span class="hljs-attr">app:</span> <span class="hljs-string">eureka</span>    <span class="hljs-attr">spec:</span>      <span class="hljs-attr">containers:</span> <span class="hljs-comment">#docker容器的配置</span>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">eureka</span>        <span class="hljs-attr">image:</span> <span class="hljs-string">taskbeez/eureka-server:master</span> <span class="hljs-comment"># pull镜像的地址</span><span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span>        <span class="hljs-attr">ports:</span>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">8761</span> <span class="hljs-comment"># 容器对外开放端口</span></code></pre><p>设置image pull的地址，可以设置从Docker私仓拉取，这里笔者默认从DockerHub仓库拉取Eureka镜像。设置好容器的默认对外接口接口和需要运行容器的副本数。如果需要设置宿主机运行的对外端口，需要注意查看k8s的默认端口范围，可以通过如下的方式查看：</p><pre><code class="hljs shell">[root@k8s-master1 /]# cd /opt/kubernetes/cfg[root@k8s-master1 cfg]# cat kube-apiserver.conf KUBE_APISERVER_OPTS="--logtostderr=false \--v=2 \--log-dir=/opt/kubernetes/logs \--etcd-servers=https://10.1.34.69:2379,https://10.1.34.71:2379,https://10.1.34.73:2379 \--bind-address=10.1.34.68 \--secure-port=6443 \--advertise-address=10.1.34.68 \--allow-privileged=true \--service-cluster-ip-range=10.0.0.0/24 \--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \--authorization-mode=RBAC,Node \--enable-bootstrap-token-auth=true \--token-auth-file=/opt/kubernetes/cfg/token.csv \--service-node-port-range=30000-32767 \--kubelet-client-certificate=/opt/kubernetes/ssl/server.pem \--kubelet-client-key=/opt/kubernetes/ssl/server-key.pem \--tls-cert-file=/opt/kubernetes/ssl/server.pem  \--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \--client-ca-file=/opt/kubernetes/ssl/ca.pem \--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \--etcd-cafile=/opt/kubernetes/ssl/etcd/ca.pem \--etcd-certfile=/opt/kubernetes/ssl/etcd/server.pem \--etcd-keyfile=/opt/kubernetes/ssl/etcd/server-key.pem \--audit-log-maxage=30 \--audit-log-maxbackup=3 \--audit-log-maxsize=100 \--audit-log-path=/opt/kubernetes/logs/k8s-audit.log"</code></pre><p>如果需要修改k8s默认的端口范围，可以参考博客：<a href="https://blog.csdn.net/qq1445654576/article/details/104581296/" target="_blank" rel="noopener">https://blog.csdn.net/qq1445654576/article/details/104581296/</a> </p><h3 id="二、执行yaml文件"><a href="#二、执行yaml文件" class="headerlink" title="二、执行yaml文件"></a>二、执行yaml文件</h3><h4 id="1、创建存放yaml文件的文件夹"><a href="#1、创建存放yaml文件的文件夹" class="headerlink" title="1、创建存放yaml文件的文件夹"></a>1、创建存放yaml文件的文件夹</h4><pre><code class="hljs shell">[root@k8s-master1 ~]# mkdir -p deploy/eureka[root@k8s-master1 ~]# cd deploy/eureka/</code></pre><p>将上文中写好的eureka.yaml文件上传到eureka文件夹下。</p><h4 id="2、kubectl命令"><a href="#2、kubectl命令" class="headerlink" title="2、kubectl命令"></a>2、kubectl命令</h4><p>由于在windows电脑上编辑，空格与unix不一样，执行会导致校验yaml格式不通过。</p><pre><code class="hljs shell">[root@k8s-master1 deploy]# kubectl apply -f eureka/service/eureka unchangederror: error parsing eureka/eureka.yaml: error converting YAML to JSON: yaml: line 20: found character that cannot start any token</code></pre><p>检查到对应的位置，处理好空格，再次执行</p><pre><code class="hljs shell">[root@k8s-master1 deploy]# kubectl apply -f eureka/service/eureka createddeployment.apps/eureka created</code></pre><h3 id="三、验证k8s应用部署情况"><a href="#三、验证k8s应用部署情况" class="headerlink" title="三、验证k8s应用部署情况"></a>三、验证k8s应用部署情况</h3><h4 id="1、查看Services"><a href="#1、查看Services" class="headerlink" title="1、查看Services"></a>1、查看Services</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s_eureka_services.png" srcset="/img/loading.gif" alt=""></p><h4 id="2、查看Deployments"><a href="#2、查看Deployments" class="headerlink" title="2、查看Deployments"></a>2、查看Deployments</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s_eureka_deployments.png" srcset="/img/loading.gif" alt=""></p><h4 id="3、查看运行Pods"><a href="#3、查看运行Pods" class="headerlink" title="3、查看运行Pods"></a>3、查看运行Pods</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s_eureka_pods.png" srcset="/img/loading.gif" alt=""></p><h4 id="4、访问Eureka-Server地址验证应用部署"><a href="#4、访问Eureka-Server地址验证应用部署" class="headerlink" title="4、访问Eureka Server地址验证应用部署"></a>4、访问Eureka Server地址验证应用部署</h4><p>通过查看Pods，可以看到Eureka的容器运行的3个节点分别为：k8s-node2，k8s-master1和k8s-master3，固定端口在yaml中设置的是：30090。</p><p>k8s会根据集群中的机器做动态的调度，运行在节点上。如果需要指定容器运行所在的宿主机，也是在yaml中做相应的配置修改。</p><table><thead><tr><th align="center">节点名称</th><th align="center">IP</th></tr></thead><tbody><tr><td align="center">k8s-master1</td><td align="center">10.1.34.68</td></tr><tr><td align="center">k8s-master3</td><td align="center">10.1.34.70</td></tr><tr><td align="center">k8s-node2</td><td align="center">10.1.34.72</td></tr></tbody></table><p>浏览器访问：<a href="http://10.1.34.70:30090/" target="_blank" rel="noopener">http://10.1.34.70:30090/</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/70_30090_eureka.png" srcset="/img/loading.gif" alt=""></p><p>浏览器访问：<a href="http://10.1.34.72:30090/" target="_blank" rel="noopener">http://10.1.34.72:30090/</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/72_30090_eureka.png" srcset="/img/loading.gif" alt=""></p><p>浏览器访问：<a href="http://10.1.34.68:30090/" target="_blank" rel="noopener">http://10.1.34.68:30090/</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/68_30090_eureka.png" srcset="/img/loading.gif" alt=""></p><p>至此，通过k8s运行docker集群的示例已经验证，搭建成功。</p>]]></content>
    
    
    
    <tags>
      
      <tag>yaml</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于Ansible脚本部署k8s多Master集群</title>
    <link href="/2020/07/28/%E5%9F%BA%E4%BA%8EAnsible%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2k8s%E5%A4%9AMaster%E9%9B%86%E7%BE%A4/"/>
    <url>/2020/07/28/%E5%9F%BA%E4%BA%8EAnsible%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2k8s%E5%A4%9AMaster%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h3 id="一、安装要求"><a href="#一、安装要求" class="headerlink" title="一、安装要求"></a>一、安装要求</h3><h4 id="1、机器要求"><a href="#1、机器要求" class="headerlink" title="1、机器要求"></a>1、机器要求</h4><p>在开始之前，部署Kubernetes集群机器需要满足以下几个条件：</p><ul><li>一台或多台机器，操作系统 CentOS7.x-86_x64</li><li>硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多</li><li>集群中所有机器之间网络互通</li><li>可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点</li><li>禁止swap分区</li></ul><p>确保所有节点系统时间一致</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install ntpdate -y</span><span class="hljs-meta">#</span><span class="bash"> ntpdate time.windows.com</span></code></pre><h4 id="2、多Master架构图"><a href="#2、多Master架构图" class="headerlink" title="2、多Master架构图"></a>2、多Master架构图</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/multi-master.jpg" srcset="/img/loading.gif" alt=""></p><h4 id="3、多Master服务器规划"><a href="#3、多Master服务器规划" class="headerlink" title="3、多Master服务器规划"></a>3、多Master服务器规划</h4><table><thead><tr><th align="center">节点名称</th><th align="center">IP</th></tr></thead><tbody><tr><td align="center">ansible</td><td align="center">10.1.96.48</td></tr><tr><td align="center">lb-master</td><td align="center">10.1.34.74</td></tr><tr><td align="center">lb-backup</td><td align="center">10.1.34.75</td></tr><tr><td align="center">k8s-master1</td><td align="center">10.1.34.68</td></tr><tr><td align="center">k8s-master2</td><td align="center">10.1.34.69</td></tr><tr><td align="center">k8s-master3</td><td align="center">10.1.34.70</td></tr><tr><td align="center">k8s-node1</td><td align="center">10.1.34.71</td></tr><tr><td align="center">k8s-node2</td><td align="center">10.1.34.72</td></tr><tr><td align="center">k8s-node3</td><td align="center">10.1.34.73</td></tr></tbody></table><h3 id="二、下载所需文件并修改配置"><a href="#二、下载所需文件并修改配置" class="headerlink" title="二、下载所需文件并修改配置"></a>二、下载所需文件并修改配置</h3><h4 id="1、下载Ansible部署文件"><a href="#1、下载Ansible部署文件" class="headerlink" title="1、下载Ansible部署文件"></a>1、下载Ansible部署文件</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://github.com/calvinGithub/ansible-install-k8s</span><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span> ansible-install-k8s</span></code></pre><p>下载软件包并上传到<strong>上面列举的机器</strong>root目录下，并解压压缩包</p><p>链接:<a href="https://pan.baidu.com/s/11-c6ZEwwKS2YsnZqlcMIyw" target="_blank" rel="noopener">https://pan.baidu.com/s/11-c6ZEwwKS2YsnZqlcMIyw</a>  密码:gsep</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> tar zxf binary_pkg.tar.gz</span></code></pre><h4 id="2、修改hosts文件"><a href="#2、修改hosts文件" class="headerlink" title="2、修改hosts文件"></a>2、修改hosts文件</h4><p>修改hosts文件，根据规划修改对应IP和名称。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vi hosts</span>[master]10.1.34.68 node_name=k8s-master110.1.34.69 node_name=k8s-master210.1.34.70 node_name=k8s-master3[node]10.1.34.71 node_name=k8s-node110.1.34.72 node_name=k8s-node210.1.34.73 node_name=k8s-node3[etcd]10.1.34.69 etcd_name=etcd-110.1.34.71 etcd_name=etcd-210.1.34.73 etcd_name=etcd-3[lb]10.1.34.74 lb_name=lb-master10.1.34.75 lb_name=lb-backup[k8s:children]masternode[newnode]<span class="hljs-meta">#</span><span class="bash">192.168.31.91 node_name=k8s-node4</span></code></pre><h4 id="3、修改group-vars-all-yml文件"><a href="#3、修改group-vars-all-yml文件" class="headerlink" title="3、修改group_vars/all.yml文件"></a>3、修改group_vars/all.yml文件</h4><p>修改软件包目录和证书可信任IP。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim group_vars/all.yml</span><span class="hljs-meta">#</span><span class="bash"> 安装目录 </span>software_dir: '/root/binary_pkg'k8s_work_dir: '/opt/kubernetes'etcd_work_dir: '/opt/etcd'tmp_dir: '/tmp/k8s'<span class="hljs-meta">#</span><span class="bash"> 集群网络</span>service_cidr: '10.0.0.0/24'cluster_dns: '10.0.0.2'   # 与roles/addons/files/coredns.yaml中IP一致，并且是service_cidr中的IPpod_cidr: '10.244.0.0/16' # 与roles/addons/files/kube-flannel.yaml中网段一致service_nodeport_range: '30000-32767'cluster_domain: 'cluster.local'<span class="hljs-meta">#</span><span class="bash"> 高可用，如果部署单Master，该项忽略</span>vip: '10.1.34.74'nic: 'ens192' # ifconfig查看自己机器的网卡名称<span class="hljs-meta">#</span><span class="bash"> 自签证书可信任IP列表，为方便扩展，可添加多个预留IP</span>cert_hosts:<span class="hljs-meta">  #</span><span class="bash"> 包含所有LB、VIP、Master IP和service_cidr的第一个IP</span>  k8s:    - 10.0.0.1    - 10.1.96.48    - 10.1.34.74    - 10.1.34.75    - 10.1.34.68    - 10.1.34.69    - 10.1.34.70    - 10.1.34.71    - 10.1.34.72    - 10.1.34.73<span class="hljs-meta">  #</span><span class="bash"> 包含所有etcd节点IP</span>  etcd:    - 10.1.34.69    - 10.1.34.71    - 10.1.34.73</code></pre><h3 id="三、一键部署"><a href="#三、一键部署" class="headerlink" title="三、一键部署"></a>三、一键部署</h3><h4 id="1、centos7安装Ansible"><a href="#1、centos7安装Ansible" class="headerlink" title="1、centos7安装Ansible"></a>1、centos7安装Ansible</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install -y epel-release</span><span class="hljs-meta">#</span><span class="bash"> yum install ansible -y</span></code></pre><h4 id="2、多Master版启动命令"><a href="#2、多Master版启动命令" class="headerlink" title="2、多Master版启动命令"></a>2、多Master版启动命令</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ansible-playbook -i hosts multi-master-deploy.yml -uroot -k</span></code></pre><p>命令结束后，可以看到生成了访问令牌</p><pre><code class="hljs shell">TASK [addons : Kubernetes Dashboard登录信息] ******************************************************************************************************************************************************ok: [10.1.34.68] =&gt; &#123;    "ui.stdout_lines": [        "访问地址---&gt;https://NodeIP:30001",         "令牌内容---&gt;eyJhbGciOiJSUzI1NiIsImtpZCI6IjNqVXMyekhlT2Vha3QwS0kwWTktaUNFYXg1TS1pSnZMajNnRE0zQllKYm8ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tdndnOTkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiODY4YTlmYTMtYmY2Mi00ZjkxLWFmMDMtMzlhNGMzYmFhYjhiIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.fAvxmkRuevgQ3SyhCs3xGGaQdY_YnJUzXArA2-V3UdEZQT66u4BIGi_jJGuzFZ7IIFKIZtkNlFrJRukDSyxJtgXFimmz38Aq1P3Vys5ZwryKGfSZ3KvfkNxVHcamBRihIOU4ePrHQBKOVOg6F4uENt636ZlyUj6433JtMSLYqKLtcL1ctpPEtnmUujebr8uLFaWyfMpdiSLCXHcRuoFo9EMFgLKWvhMpUbhLUhOYT_kp8H5e35i2sK7rC6ty6r-F8imClQCyKKebXr_H5hYDghwjocFN6RhN6zyJzniGF8EYW2i7qa6QECIzvUaQbQ7yM0J4D-eMPCoNHGcQdgvFuA"    ]&#125;ok: [10.1.34.70] =&gt; &#123;    "ui.stdout_lines": [        "访问地址---&gt;https://NodeIP:30001",         "令牌内容---&gt;eyJhbGciOiJSUzI1NiIsImtpZCI6IjNqVXMyekhlT2Vha3QwS0kwWTktaUNFYXg1TS1pSnZMajNnRE0zQllKYm8ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tdndnOTkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiODY4YTlmYTMtYmY2Mi00ZjkxLWFmMDMtMzlhNGMzYmFhYjhiIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.fAvxmkRuevgQ3SyhCs3xGGaQdY_YnJUzXArA2-V3UdEZQT66u4BIGi_jJGuzFZ7IIFKIZtkNlFrJRukDSyxJtgXFimmz38Aq1P3Vys5ZwryKGfSZ3KvfkNxVHcamBRihIOU4ePrHQBKOVOg6F4uENt636ZlyUj6433JtMSLYqKLtcL1ctpPEtnmUujebr8uLFaWyfMpdiSLCXHcRuoFo9EMFgLKWvhMpUbhLUhOYT_kp8H5e35i2sK7rC6ty6r-F8imClQCyKKebXr_H5hYDghwjocFN6RhN6zyJzniGF8EYW2i7qa6QECIzvUaQbQ7yM0J4D-eMPCoNHGcQdgvFuA"    ]&#125;ok: [10.1.34.69] =&gt; &#123;    "ui.stdout_lines": [        "访问地址---&gt;https://NodeIP:30001",         "令牌内容---&gt;eyJhbGciOiJSUzI1NiIsImtpZCI6IjNqVXMyekhlT2Vha3QwS0kwWTktaUNFYXg1TS1pSnZMajNnRE0zQllKYm8ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tdndnOTkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiODY4YTlmYTMtYmY2Mi00ZjkxLWFmMDMtMzlhNGMzYmFhYjhiIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.fAvxmkRuevgQ3SyhCs3xGGaQdY_YnJUzXArA2-V3UdEZQT66u4BIGi_jJGuzFZ7IIFKIZtkNlFrJRukDSyxJtgXFimmz38Aq1P3Vys5ZwryKGfSZ3KvfkNxVHcamBRihIOU4ePrHQBKOVOg6F4uENt636ZlyUj6433JtMSLYqKLtcL1ctpPEtnmUujebr8uLFaWyfMpdiSLCXHcRuoFo9EMFgLKWvhMpUbhLUhOYT_kp8H5e35i2sK7rC6ty6r-F8imClQCyKKebXr_H5hYDghwjocFN6RhN6zyJzniGF8EYW2i7qa6QECIzvUaQbQ7yM0J4D-eMPCoNHGcQdgvFuA"    ]&#125;</code></pre><h3 id="四、测试部署情况"><a href="#四、测试部署情况" class="headerlink" title="四、测试部署情况"></a>四、测试部署情况</h3><h4 id="1、访问Dashboard"><a href="#1、访问Dashboard" class="headerlink" title="1、访问Dashboard"></a>1、访问Dashboard</h4><p>随便访问任何一个节点，<a href="https://Node:30001" target="_blank" rel="noopener">https://Node:30001</a> ，并输入上面输出的token。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s_login.png" srcset="/img/loading.gif" alt=""></p><p>登录后，点击Nodes，可以看到加入到k8s集群的所有节点，默认命名空间为default。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/%E5%A4%9AMaster%E9%9B%86%E7%BE%A4Node.png" srcset="/img/loading.gif" alt="image-20200718231233862"></p><p>进入页面后，点击左侧导航栏Nodes，可以看到目前加入k8s的服务器节点。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718231355987.png" srcset="/img/loading.gif" alt="image-20200718231355987"></p><h4 id="2、使用kubectl命令查看容器"><a href="#2、使用kubectl命令查看容器" class="headerlink" title="2、使用kubectl命令查看容器"></a>2、使用kubectl命令查看容器</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 在任意一个master节点使用命令查看nodes</span>[root@k8s-master1 ~]# kubectl get nodesNAME           STATUS   ROLES    AGE    VERSIONk8s-master1    Ready    &lt;none&gt;   29h    v1.18.6k8s-master2    Ready    &lt;none&gt;   29h    v1.18.6k8s-master3    Ready    &lt;none&gt;   29h    v1.18.6k8s-node1      Ready    &lt;none&gt;   29h    v1.18.6k8s-node2      Ready    &lt;none&gt;   29h    v1.18.6k8s-node3      Ready    &lt;none&gt;   29h    v1.18.6</code></pre><p>至此，多Master的k8s集群已经搭建成功。</p><h3 id="五、增加集群节点"><a href="#五、增加集群节点" class="headerlink" title="五、增加集群节点"></a>五、增加集群节点</h3><h4 id="1、修改hosts文件添加新节点ip"><a href="#1、修改hosts文件添加新节点ip" class="headerlink" title="1、修改hosts文件添加新节点ip"></a>1、修改hosts文件添加新节点ip</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vi hosts</span>[newnode]10.1.34.20 node_name=PaasNexus0110.1.34.7  node_name=PaasHarbor01</code></pre><h4 id="2、执行部署命令"><a href="#2、执行部署命令" class="headerlink" title="2、执行部署命令"></a>2、执行部署命令</h4><pre><code class="hljs shell">ansible-playbook -i hosts add-node.yml -uroot -k</code></pre><h4 id="3、在任一Master节点允许颁发证书并加入集群"><a href="#3、在任一Master节点允许颁发证书并加入集群" class="headerlink" title="3、在任一Master节点允许颁发证书并加入集群"></a>3、在任一Master节点允许颁发证书并加入集群</h4><pre><code class="hljs shell">[root@k8s-master1 ~]# kubectl get csrNAME                                                   AGE    SIGNERNAME               REQUESTOR           CONDITIONnode-csr-lGmQlYI-ByUQGy0F7Deox0vfkKJuWZX0gh3rPodClOA   117m   kubernetes.io/kube-apiserver-client-kubelet   kubelet-bootstrap   Pending[root@k8s-master1 ~]# kubectl certificate approve node-csr-lGmQlYI-ByUQGy0F7Deox0vfkKJuWZX0gh3rPodClOAcertificatesigningrequest.certificates.k8s.io/node-csr-lGmQlYI-ByUQGy0F7Deox0vfkKJuWZX0gh3rPodClOA approved</code></pre><p>接着用命令查看节点</p><pre><code class="hljs shell">[root@k8s-master1 ~]# kubectl get nodesNAME           STATUS   ROLES    AGE    VERSIONk8s-master1    Ready    &lt;none&gt;   29h    v1.18.6k8s-master2    Ready    &lt;none&gt;   29h    v1.18.6k8s-master3    Ready    &lt;none&gt;   29h    v1.18.6k8s-node1      Ready    &lt;none&gt;   29h    v1.18.6k8s-node2      Ready    &lt;none&gt;   29h    v1.18.6k8s-node3      Ready    &lt;none&gt;   29h    v1.18.6paasharbor01   Ready    &lt;none&gt;   152m   v1.18.6paasnexus01    Ready    &lt;none&gt;   152m   v1.18.6</code></pre><p>k8s Dashboard页面查看Nodes</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/%E6%96%B0%E5%A2%9Ek8s%E8%8A%82%E7%82%B9.png" srcset="/img/loading.gif" alt="image-20200728160021747"></p><p>最后还需要注意一点，为了新加入的节点能够彼此通信，还需要在master节点和新增加的机器下增加hosts映射。</p><p>其中10.1.34.20 PaasNexus01和10.1.34.7  PaasHarbor01是新增的机器，需要补充的。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vi /etc/hosts</span>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4::1         localhost localhost.localdomain localhost6 localhost6.localdomain610.1.34.68 k8s-master110.1.34.69 k8s-master210.1.34.70 k8s-master310.1.34.71 k8s-node110.1.34.72 k8s-node210.1.34.73 k8s-node310.1.34.20 PaasNexus0110.1.34.7  PaasHarbor01</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>k8s多Master</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于Ansible脚本部署k8s单Master集群</title>
    <link href="/2020/07/28/%E5%9F%BA%E4%BA%8EAnsible%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2k8s%E5%8D%95Master%E9%9B%86%E7%BE%A4/"/>
    <url>/2020/07/28/%E5%9F%BA%E4%BA%8EAnsible%E8%84%9A%E6%9C%AC%E9%83%A8%E7%BD%B2k8s%E5%8D%95Master%E9%9B%86%E7%BE%A4/</url>
    
    <content type="html"><![CDATA[<h3 id="一、安装要求"><a href="#一、安装要求" class="headerlink" title="一、安装要求"></a>一、安装要求</h3><h4 id="1、机器要求"><a href="#1、机器要求" class="headerlink" title="1、机器要求"></a>1、机器要求</h4><p>在开始之前，部署Kubernetes集群机器需要满足以下几个条件：</p><ul><li>一台或多台机器，操作系统 CentOS7.x-86_x64</li><li>硬件配置：2GB或更多RAM，2个CPU或更多CPU，硬盘30GB或更多</li><li>集群中所有机器之间网络互通</li><li>可以访问外网，需要拉取镜像，如果服务器不能上网，需要提前下载镜像并导入节点</li><li>禁止swap分区</li></ul><p>确保所有节点系统时间一致</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install ntpdate -y</span><span class="hljs-meta">#</span><span class="bash"> ntpdate time.windows.com</span></code></pre><h4 id="2、单Master架构图"><a href="#2、单Master架构图" class="headerlink" title="2、单Master架构图"></a>2、单Master架构图</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/single-master.jpg" srcset="/img/loading.gif" alt="single-master"></p><h4 id="3、单Master服务器规划"><a href="#3、单Master服务器规划" class="headerlink" title="3、单Master服务器规划"></a>3、单Master服务器规划</h4><table><thead><tr><th align="center">节点名称</th><th align="center">IP</th></tr></thead><tbody><tr><td align="center">etcd-1</td><td align="center">10.10.10.8</td></tr><tr><td align="center">k8s-node01</td><td align="center">10.10.10.9</td></tr><tr><td align="center">k8s-node02</td><td align="center">10.10.10.10</td></tr></tbody></table><h3 id="二、下载所需文件并修改配置"><a href="#二、下载所需文件并修改配置" class="headerlink" title="二、下载所需文件并修改配置"></a>二、下载所需文件并修改配置</h3><h4 id="1、下载Ansible部署文件"><a href="#1、下载Ansible部署文件" class="headerlink" title="1、下载Ansible部署文件"></a>1、下载Ansible部署文件</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> git <span class="hljs-built_in">clone</span> https://github.com/calvinGithub/ansible-install-k8s</span><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">cd</span> ansible-install-k8s</span></code></pre><p>下载软件包并上传到<strong>上面列举的机器</strong>root目录下，并解压压缩包</p><p>链接:<a href="https://pan.baidu.com/s/11-c6ZEwwKS2YsnZqlcMIyw" target="_blank" rel="noopener">https://pan.baidu.com/s/11-c6ZEwwKS2YsnZqlcMIyw</a>  密码:gsep</p><pre><code class="hljs css"># <span class="hljs-selector-tag">tar</span> <span class="hljs-selector-tag">zxf</span> <span class="hljs-selector-tag">binary_pkg</span><span class="hljs-selector-class">.tar</span><span class="hljs-selector-class">.gz</span></code></pre><h4 id="2、修改hosts文件"><a href="#2、修改hosts文件" class="headerlink" title="2、修改hosts文件"></a>2、修改hosts文件</h4><p>修改hosts文件，根据规划修改对应IP和名称。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vi hosts</span>[master]<span class="hljs-meta">#</span><span class="bash"> 如果部署单Master，只保留一个Master节点</span><span class="hljs-meta">#</span><span class="bash"> 默认Naster节点也部署Node组件</span>10.10.10.8 node_name=k8s-master01[node]10.10.10.9 node_name=k8s-node0110.10.10.10 node_name=k8s-node02[etcd]10.10.10.8 etcd_name=etcd-110.10.10.9 etcd_name=etcd-210.10.10.10 etcd_name=etcd-3[lb]<span class="hljs-meta">#</span><span class="bash"> 如果部署单Master，该项忽略</span>192.168.31.63 lb_name=lb-master192.168.31.71 lb_name=lb-backup[k8s:children]masternode[newnode]<span class="hljs-meta">#</span><span class="bash">192.168.31.91 node_name=k8s-node3</span></code></pre><h4 id="3、修改group-vars-all-yml文件"><a href="#3、修改group-vars-all-yml文件" class="headerlink" title="3、修改group_vars/all.yml文件"></a>3、修改group_vars/all.yml文件</h4><p>修改软件包目录和证书可信任IP。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> vim group_vars/all.yml</span><span class="hljs-meta">#</span><span class="bash"> 安装目录 </span>software_dir: '/root/binary_pkg'k8s_work_dir: '/opt/kubernetes'etcd_work_dir: '/opt/etcd'tmp_dir: '/tmp/k8s'<span class="hljs-meta">#</span><span class="bash"> 集群网络</span>service_cidr: '10.0.0.0/24'cluster_dns: '10.0.0.2'   # 与roles/addons/files/coredns.yaml中IP一致，并且是service_cidr中的IPpod_cidr: '10.244.0.0/16' # 与roles/addons/files/kube-flannel.yaml中网段一致service_nodeport_range: '30000-32767'cluster_domain: 'cluster.local'<span class="hljs-meta">#</span><span class="bash"> 高可用，如果部署单Master，该项忽略</span>vip: '192.168.31.88'nic: 'ens33' # ifconfig查看自己机器的网卡名称<span class="hljs-meta">#</span><span class="bash"> 自签证书可信任IP列表，为方便扩展，可添加多个预留IP</span>cert_hosts:<span class="hljs-meta">  #</span><span class="bash"> 包含所有LB、VIP、Master IP和service_cidr的第一个IP</span>  k8s:    - 10.0.0.1    - 10.10.10.8    - 10.10.10.9    - 10.10.10.10<span class="hljs-meta">  #</span><span class="bash"> 包含所有etcd节点IP</span>  etcd:    - 10.10.10.8    - 10.10.10.9    - 10.10.10.10</code></pre><h3 id="三、一键部署"><a href="#三、一键部署" class="headerlink" title="三、一键部署"></a>三、一键部署</h3><h4 id="1、centos7安装Ansible"><a href="#1、centos7安装Ansible" class="headerlink" title="1、centos7安装Ansible"></a>1、centos7安装Ansible</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> yum install -y epel-release</span><span class="hljs-meta">#</span><span class="bash"> yum install ansible -y</span></code></pre><h4 id="2、单Master版启动命令"><a href="#2、单Master版启动命令" class="headerlink" title="2、单Master版启动命令"></a>2、单Master版启动命令</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ansible-playbook -i hosts single-master-deploy.yml -uroot -k</span></code></pre><p>命令结束后，可以看到生成了访问令牌</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718230920066.png" srcset="/img/loading.gif" alt="image-20200718230920066"></p><pre><code class="hljs shell">eyJhbGciOiJSUzI1NiIsImtpZCI6IjFhMTN5LXdUVFN6TVVoNmZ1dUltVngyTmxvMXNKejFTYkZkbldaNWhWdXcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tcXRyNmwiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiYWY5YWIyNmMtN2MwZC00MTZkLWJhYmMtZTMwMWY5MDBmMjU0Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.4gRDjg8It_8h0LInLHXrchsUCcdH0UerDDkCdd2y_UnKwc6wAhReDQYc2NaYVda2704Hu7-n7_fXFATQ_bf6noBjoQNsCEJ1NwBl0DO4h3l_i80YIVT9R2yl1nEq6CY5Da7rR01pp6-Htih1mpEb-O_xsP_L6FrsVjKBubY63gK4O9LQLQSnoEE-24ggg2VgyS5rVHTTDlEpq3J0XeqXiBbaZJa8o65Yg08sKzeHAffgY2538p-9ZV5-tJAsL4wbud8TjSXCzWkxsVsmH7Au0ZwRrth1tVrVEqoPGOusDVu6sFxSO53lpwsvlcg1TpKIxNMEbDTQ6GSJbPA_dyeW9Q</code></pre><h3 id="四、测试部署情况"><a href="#四、测试部署情况" class="headerlink" title="四、测试部署情况"></a>四、测试部署情况</h3><h4 id="1、访问Dashboard"><a href="#1、访问Dashboard" class="headerlink" title="1、访问Dashboard"></a>1、访问Dashboard</h4><p>随便访问任何一个节点，<a href="https://Node:30001" target="_blank" rel="noopener">https://Node:30001</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718231233862.png" srcset="/img/loading.gif" alt="image-20200718231233862"></p><p>进入页面后，点击左侧导航栏Nodes，可以看到目前加入k8s的服务器节点。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718231355987.png" srcset="/img/loading.gif" alt="image-20200718231355987"></p><h4 id="2、使用kubectl命令查看容器"><a href="#2、使用kubectl命令查看容器" class="headerlink" title="2、使用kubectl命令查看容器"></a>2、使用kubectl命令查看容器</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 1.查看nodes</span>[root@k8s-master01 ansible-install-k8s]# kubectl get nodeNAME           STATUS   ROLES    AGE   VERSIONk8s-master01   Ready    &lt;none&gt;   15m   v1.18.6k8s-node01     Ready    &lt;none&gt;   15m   v1.18.6k8s-node02     Ready    &lt;none&gt;   15m   v1.18.6<span class="hljs-meta">#</span><span class="bash"> 2.创建应用nginx</span>[root@k8s-master01 ansible-install-k8s]# kubectl create deployment web --image=nginxdeployment.apps/web created<span class="hljs-meta">#</span><span class="bash"> 3.启动应用nginx</span>[root@k8s-master01 ansible-install-k8s]# kubectl expose deployment web --port=80 --target-port=80 --name=web --type=NodePortservice/web exposed<span class="hljs-meta">#</span><span class="bash"> 4.查看所有启动的应用(30794即为容器80端口对应的宿主机映射端口)</span>[root@k8s-master01 ansible-install-k8s]# kubectl get svcNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGEkubernetes   ClusterIP   10.0.0.1     &lt;none&gt;        443/TCP        32mweb          NodePort    10.0.0.95    &lt;none&gt;        80:30794/TCP   38s</code></pre><h4 id="3、Dashboard查看运行容器"><a href="#3、Dashboard查看运行容器" class="headerlink" title="3、Dashboard查看运行容器"></a>3、Dashboard查看运行容器</h4><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718232547965.png" srcset="/img/loading.gif" alt="image-20200718232547965"></p><p>可以看到测试用的web容器已经启动，查看容器日志</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718232658162.png" srcset="/img/loading.gif" alt="image-20200718232658162"></p><p>浏览器访问：<a href="http://10.10.10.9:30794/" target="_blank" rel="noopener">http://10.10.10.9:30794/</a></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/image-20200718232858065.png" srcset="/img/loading.gif" alt="image-20200718232858065"></p><p>至此，单Master的k8s集群及部署docker应用已经成功。</p>]]></content>
    
    
    
    <tags>
      
      <tag>k8s单Master</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Centos7安装Ansible异常问题排查</title>
    <link href="/2020/07/28/Centos7%E5%AE%89%E8%A3%85Ansible%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/"/>
    <url>/2020/07/28/Centos7%E5%AE%89%E8%A3%85Ansible%E5%BC%82%E5%B8%B8%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/</url>
    
    <content type="html"><![CDATA[<p>目标：Centos7系统的机器安装Ansible</p><p>yum源：阿里云</p><p>外网：虚拟机可以连接外网</p><p>出现的问题：直接用命令yum -y install ansible，发现安装不上。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/ansible.png" srcset="/img/loading.gif" alt="image-20200724112514837"></p><p><strong>问题排查</strong>：</p><p>（1）原来安装ansible，需要先安装epel源</p><p>（2）<strong>yum install epel-release</strong></p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/epel.png" srcset="/img/loading.gif" alt="image-20200724112635120"></p><p>然后<strong>yum repolis</strong>t查看源，发现并没有epel。</p><p>（3）排查机器上是否已经存在epel源：<strong>yum list installed|grep epel</strong></p><p>发现确实存在，先移除：<strong>yum remove epel-release.noarch</strong></p><p>（4）重新下载安装epel源</p><pre><code class="hljs shell">wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmrpm -ivh epel-release-latest-7.noarch.rpm</code></pre><p>接着查看源：<strong>yum repolist</strong>，发现epel源已经存在</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/repolist.png" srcset="/img/loading.gif" alt="image-20200724113312221"></p><p>（5）最后重新用命令：<strong>yum install ansible -y</strong> 安装Ansible即可。</p><p><img src="https://ecblog.oss-cn-hangzhou.aliyuncs.com/k8s/ansible_version.png" srcset="/img/loading.gif" alt="image-20200724113350777"></p>]]></content>
    
    
    
    <tags>
      
      <tag>ansible</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
